{
  "frontmatter": {
    "id": "aws-emr",
    "status": "published",
    "name": "EMR",
    "title": "Dagster & AWS EMR",
    "excerpt": "The AWS EMR integration allows you to seamlessly integrate AWS EMR into your Dagster pipelines for petabyte-scale data processing using open source tools like Apache Spark, Hive, Presto, and more.",
    "partnerlink": "https://aws.amazon.com/",
    "categories": ["Compute"],
    "enabledBy": [],
    "enables": [],
    "tags": ["dagster-supported", "compute"]
  },
  "logoFilename": "aws-emr.svg",
  "content": "The `dagster-aws` integration provides ways orchestrating data pipelines that leverage AWS services, including AWS EMR (Elastic MapReduce). This integration allows you to run and scale big data workloads using open source tools such as Apache Spark, Hive, Presto, and more.\n\nUsing this integration, you can:\n\n- Seamlessly integrate AWS EMR into your Dagster pipelines.\n- Utilize EMR for petabyte-scale data processing.\n- Easily manage and monitor EMR clusters and jobs from within Dagster.\n- Leverage Dagster's orchestration capabilities to handle complex data workflows involving EMR.\n\n### Installation\n\n```bash\npip install dagster-aws\n```\n\n### Examples\n\n\n```python\nfrom pathlib import Path\nfrom typing import Any\n\nfrom dagster_aws.emr import emr_pyspark_step_launcher\nfrom dagster_aws.s3 import S3Resource\nfrom dagster_pyspark import PySparkResource\nfrom pyspark.sql import DataFrame, Row\nfrom pyspark.sql.types import IntegerType, StringType, StructField, StructType\n\nimport dagster as dg\n\nemr_pyspark = PySparkResource(spark_config={\"spark.executor.memory\": \"2g\"})\n\n\n@dg.asset\ndef people(\n    pyspark: PySparkResource, pyspark_step_launcher: dg.ResourceParam[Any]\n) -> DataFrame:\n    schema = StructType(\n        [StructField(\"name\", StringType()), StructField(\"age\", IntegerType())]\n    )\n    rows = [\n        Row(name=\"Thom\", age=51),\n        Row(name=\"Jonny\", age=48),\n        Row(name=\"Nigel\", age=49),\n    ]\n    return pyspark.spark_session.createDataFrame(rows, schema)\n\n\n@dg.asset\ndef people_over_50(\n    pyspark_step_launcher: dg.ResourceParam[Any], people: DataFrame\n) -> DataFrame:\n    return people.filter(people[\"age\"] > 50)\n\n\ndefs = dg.Definitions(\n    assets=[people, people_over_50],\n    resources={\n        \"pyspark_step_launcher\": emr_pyspark_step_launcher.configured(\n            {\n                \"cluster_id\": {\"env\": \"EMR_CLUSTER_ID\"},\n                \"local_pipeline_package_path\": str(Path(__file__).parent),\n                \"deploy_local_pipeline_package\": True,\n                \"region_name\": \"us-west-1\",\n                \"staging_bucket\": \"my_staging_bucket\",\n                \"wait_for_logs\": True,\n            }\n        ),\n        \"pyspark\": emr_pyspark,\n        \"s3\": S3Resource(),\n    },\n)\n```\n        \n\n### About AWS EMR\n\n**AWS EMR** (Elastic MapReduce) is a cloud big data platform for processing vast amounts of data using open source tools such as Apache Spark, Apache Hive, Apache HBase, Apache Flink, Apache Hudi, and Presto. It simplifies running big data frameworks, allowing you to process and analyze large datasets quickly and cost-effectively. AWS EMR provides the scalability, flexibility, and reliability needed to handle complex data processing tasks, making it an ideal choice for data engineers and scientists."
}
