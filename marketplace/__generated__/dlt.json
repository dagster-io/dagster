{
  "frontmatter": {
    "id": "dlt",
    "status": "published",
    "name": "dlt",
    "title": "Dagster & dlt",
    "excerpt": "Easily ingest and replicate data between systems with dlt through Dagster.",
    "partnerlink": "https://dlthub.com/",
    "categories": [
      "ETL"
    ],
    "enabledBy": [],
    "enables": [],
    "tags": [
      "dagster-supported",
      "etl"
    ]
  },
  "logoFilename": "dlthub.jpeg",
  "content": "This integration allows you to use [dlt](https://dlthub.com/) to easily ingest and replicate data between systems through Dagster.\n\n### Installation\n\n```bash\npip install dagster-dlt\n```\n\n### Example\n\n\n```python\nimport dlt\nfrom dagster_dlt import DagsterDltResource, dlt_assets\nfrom dlt_sources.github import github_reactions\n\nimport dagster as dg\n\n\n@dlt_assets(\n    dlt_source=github_reactions(\"dagster-io\", \"dagster\"),\n    dlt_pipeline=dlt.pipeline(\n        pipeline_name=\"github_issues\",\n        dataset_name=\"github\",\n        destination=\"snowflake\",\n    ),\n    name=\"github\",\n    group_name=\"github\",\n)\ndef github_issues_to_snowflake_assets(\n    context: dg.AssetExecutionContext, dlt: DagsterDltResource\n):\n    yield from dlt.run(context=context)\n\n\ndefs = dg.Definitions(\n    assets=[\n        github_issues_to_snowflake_assets,\n    ],\n    resources={\n        \"dlt\": DagsterDltResource(),\n    },\n)\n```\n        \n\n:::note\n\nIf you are using the [sql_database](https://dlthub.com/docs/api_reference/dlt/sources/sql_database/__init__) source, consider setting `defer_table_reflect=True` to reduce database reads. By default, the Dagster daemon will refresh definitions roughly every minute, which will query the database for resource definitions.\n\n:::\n\n### About dlt\n\n[Data Load Tool (dlt)](https://dlthub.com/) is an open source library for creating efficient data pipelines. It offers features like secret management, data structure conversion, incremental updates, and pre-built sources and destinations, simplifying the process of loading messy data into well-structured datasets."
}