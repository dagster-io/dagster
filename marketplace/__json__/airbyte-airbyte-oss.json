{
  "frontmatter": {
    "id": "airbyte-airbyte-oss",
    "status": "published",
    "name": "Airbyte",
    "title": "Dagster & Airbyte",
    "excerpt": "Orchestrate Airbyte connections and schedule syncs alongside upstream or downstream dependencies.",
    "logoFilename": "airbyte.svg",
    "partnerlink": "https://airbyte.com/tutorials/orchestrate-data-ingestion-and-transformation-pipelines",
    "categories": ["ETL"],
    "enabledBy": [],
    "enables": [],
    "tags": ["dagster-supported", "etl"]
  },
  "content": "Using this integration, you can trigger Airbyte syncs and orchestrate your Airbyte connections from within Dagster, making it easy to chain an Airbyte sync with upstream or downstream steps in your workflow.\n\n### Installation\n\n```bash\npip install dagster-airbyte\n```\n\n### Example\n\n\n```python\nfrom dagster_airbyte import AirbyteResource, load_assets_from_airbyte_instance\n\nimport dagster as dg\n\n# Load all assets from your Airbyte instance\nairbyte_assets = load_assets_from_airbyte_instance(\n    # Connect to your OSS Airbyte instance\n    AirbyteResource(\n        host=\"localhost\",\n        port=\"8000\",\n        # If using basic auth, include username and password:\n        username=\"airbyte\",\n        password=dg.EnvVar(\"AIRBYTE_PASSWORD\"),\n    )\n)\n\ndefs = dg.Definitions(\n    assets=[airbyte_assets],\n)\n```\n        \n\n### About Airbyte\n\n**Airbyte** is an open source data integration engine that helps you consolidate your SaaS application and database data into your data warehouses, lakes and databases."
}
