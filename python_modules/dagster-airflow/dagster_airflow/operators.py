'''The dagster-airflow operators.'''
import ast
import json
import logging
import os

from abc import ABCMeta, abstractmethod
from contextlib import contextmanager

from six import with_metaclass

from airflow.exceptions import AirflowException
from airflow.operators.docker_operator import DockerOperator
from airflow.operators.python_operator import PythonOperator
from airflow.utils.file import TemporaryDirectory
from docker import APIClient, from_env

from dagster import check, seven
from dagster.seven.json import JSONDecodeError

from .query import QUERY
from .util import convert_airflow_datestr_to_epoch_ts


DOCKER_TEMPDIR = '/tmp'

DEFAULT_ENVIRONMENT = {
    'AWS_ACCESS_KEY_ID': os.getenv('AWS_ACCESS_KEY_ID'),
    'AWS_SECRET_ACCESS_KEY': os.getenv('AWS_SECRET_ACCESS_KEY'),
}

LINE_LENGTH = 100


def parse_raw_res(raw_res):
    res = None
    # FIXME
    # Unfortunately, log lines don't necessarily come back in order...
    # This is error-prone, if something else logs JSON
    lines = list(filter(None, reversed(raw_res.split('\n'))))

    for line in lines:
        try:
            res = json.loads(line)
            break
        # If we don't get a GraphQL response, check the next line
        except JSONDecodeError:
            continue

    return (res, raw_res)


def airflow_storage_exception(tmp_dir):
    return AirflowException(
        'No storage config found -- must configure either filesystem or s3 storage for '
        'the DagsterPythonOperator. Ex.: \n'
        'storage:\n'
        '  filesystem:\n'
        '    base_dir: \'{tmp_dir}\''
        '\n\n --or--\n\n'
        'storage:\n'
        '  s3:\n'
        '    s3_bucket: \'my-s3-bucket\'\n'.format(tmp_dir=tmp_dir)
    )


class DagsterOperator(with_metaclass(ABCMeta)):  # pylint:disable=no-init
    '''Abstract base class for Dagster operators.

    Implement operator_for_solid to support dynamic generation of Airflow operators corresponding to
    the execution plan steps generated by a Dagster solid.
    '''

    @classmethod
    @abstractmethod
    def operator_for_solid(
        cls,
        handle,
        pipeline_name,
        environment_dict,
        mode,
        solid_name,
        step_keys,
        dag,
        dag_id,
        op_kwargs,
    ):
        pass

    @classmethod
    def handle_errors(cls, res, raw_res):
        if res is None:
            raise AirflowException('Unhandled error type. Raw response: {}'.format(raw_res))

        if res.get('errors'):
            raise AirflowException('Internal error in GraphQL request. Response: {}'.format(res))

        if not res.get('data', {}).get('executePlan', {}).get('__typename'):
            raise AirflowException('Unexpected response type. Response: {}'.format(res))

    @classmethod
    def handle_result(cls, res):
        res_data = res['data']['executePlan']

        res_type = res_data['__typename']

        if res_type == 'PipelineConfigValidationInvalid':
            errors = [err['message'] for err in res_data['errors']]
            raise AirflowException(
                'Pipeline configuration invalid:\n{errors}'.format(errors='\n'.join(errors))
            )

        if res_type == 'PipelineNotFoundError':
            raise AirflowException(
                'Pipeline "{pipeline_name}" not found: {message}:'.format(
                    pipeline_name=res_data['pipelineName'], message=res_data['message']
                )
            )

        if res_type == 'ExecutePlanSuccess':
            if res_data['hasFailures']:
                errors = [
                    step['errorMessage'] for step in res_data['stepEvents'] if not step['success']
                ]
                raise AirflowException(
                    'Subplan execution failed:\n{errors}'.format(errors='\n'.join(errors))
                )

            return res

        if res_type == 'PythonError':
            raise AirflowException(
                'Subplan execution failed: {message}\n{stack}'.format(
                    message=res_data['message'], stack=res_data['stack']
                )
            )

        # Catchall
        return res


# pylint: disable=len-as-condition
class ModifiedDockerOperator(DockerOperator):
    """ModifiedDockerOperator supports host temporary directories on OSX.

    Incorporates https://github.com/apache/airflow/pull/4315/ and an implementation of
    https://issues.apache.org/jira/browse/AIRFLOW-3825.

    :param host_tmp_dir: Specify the location of the temporary directory on the host which will
        be mapped to tmp_dir. If not provided defaults to using the standard system temp directory.
    :type host_tmp_dir: str
    """

    def __init__(self, host_tmp_dir='/tmp', **kwargs):
        self.host_tmp_dir = host_tmp_dir
        kwargs['xcom_push'] = True
        super(ModifiedDockerOperator, self).__init__(**kwargs)

    @contextmanager
    def get_host_tmp_dir(self):
        '''Abstracts the tempdir context manager so that this can be overridden.'''
        with TemporaryDirectory(prefix='airflowtmp', dir=self.host_tmp_dir) as tmp_dir:
            yield tmp_dir

    def execute(self, context):
        '''Modified only to use the get_host_tmp_dir helper.'''
        self.log.info('Starting docker container from image %s', self.image)

        tls_config = self.__get_tls_config()
        if self.docker_conn_id:
            self.cli = self.get_hook().get_conn()
        else:
            self.cli = APIClient(base_url=self.docker_url, version=self.api_version, tls=tls_config)

        if self.force_pull or len(self.cli.images(name=self.image)) == 0:
            self.log.info('Pulling docker image %s', self.image)
            for l in self.cli.pull(self.image, stream=True):
                output = json.loads(l.decode('utf-8').strip())
                if 'status' in output:
                    self.log.info("%s", output['status'])

        with self.get_host_tmp_dir() as host_tmp_dir:
            self.environment['AIRFLOW_TMP_DIR'] = self.tmp_dir
            self.volumes.append('{0}:{1}'.format(host_tmp_dir, self.tmp_dir))

            self.container = self.cli.create_container(
                command=self.get_command(),
                environment=self.environment,
                host_config=self.cli.create_host_config(
                    auto_remove=self.auto_remove,
                    binds=self.volumes,
                    network_mode=self.network_mode,
                    shm_size=self.shm_size,
                    dns=self.dns,
                    dns_search=self.dns_search,
                    cpu_shares=int(round(self.cpus * 1024)),
                    mem_limit=self.mem_limit,
                ),
                image=self.image,
                user=self.user,
                working_dir=self.working_dir,
            )
            self.cli.start(self.container['Id'])

            res = []
            line = ''
            for new_line in self.cli.logs(container=self.container['Id'], stream=True):
                line = new_line.strip()
                if hasattr(line, 'decode'):
                    line = line.decode('utf-8')
                self.log.info(line)
                res.append(line)

            result = self.cli.wait(self.container['Id'])
            if result['StatusCode'] != 0:
                raise AirflowException('docker container failed: ' + repr(result))

            if self.xcom_push_flag:
                # Try to avoid any kind of race condition?
                return '\n'.join(res) + '\n' if self.xcom_all else str(line)

    # This is a class-private name on DockerOperator for no good reason --
    # all that the status quo does is inhibit extension of the class.
    # See https://issues.apache.org/jira/browse/AIRFLOW-3880
    def __get_tls_config(self):
        # pylint: disable=no-member
        return super(ModifiedDockerOperator, self)._DockerOperator__get_tls_config()


class DagsterDockerOperator(ModifiedDockerOperator, DagsterOperator):
    '''Dagster operator for Apache Airflow.

    Wraps a modified DockerOperator incorporating https://github.com/apache/airflow/pull/4315.

    Additionally, if a Docker client can be initialized using docker.from_env,
    Unlike the standard DockerOperator, this operator also supports config using docker.from_env,
    so it isn't necessary to explicitly set docker_url, tls_config, or api_version.

    '''

    # py2 compat
    # pylint: disable=keyword-arg-before-vararg
    def __init__(
        self,
        step=None,
        environment_dict=None,
        pipeline_name=None,
        mode=None,
        step_keys=None,
        s3_bucket_name=None,
        *args,
        **kwargs
    ):
        check.str_param(pipeline_name, 'pipeline_name')
        step_keys = check.opt_list_param(step_keys, 'step_keys', of_type=str)
        environment_dict = check.opt_dict_param(environment_dict, 'environment_dict', key_type=str)

        self.step = step
        self.environment_dict = environment_dict
        self.pipeline_name = pipeline_name
        self.mode = mode
        self.step_keys = step_keys
        self.docker_conn_id_set = kwargs.get('docker_conn_id') is not None
        self.s3_bucket_name = s3_bucket_name
        self._run_id = None

        # These shenanigans are so we can override DockerOperator.get_hook in order to configure
        # a docker client using docker.from_env, rather than messing with the logic of
        # DockerOperator.execute
        if not self.docker_conn_id_set:
            try:
                from_env().version()
            except Exception:  # pylint: disable=broad-except
                pass
            else:
                kwargs['docker_conn_id'] = True

        # We do this because log lines won't necessarily be emitted in order (!) -- so we can't
        # just check the last log line to see if it's JSON.
        kwargs['xcom_all'] = True

        # Store Airflow DAG run timestamp so that we can pass along via execution metadata
        self.airflow_ts = kwargs.get('ts')

        if 'environment' not in kwargs:
            kwargs['environment'] = DEFAULT_ENVIRONMENT

        super(DagsterDockerOperator, self).__init__(*args, **kwargs)

    @classmethod
    def operator_for_solid(
        cls,
        handle,
        pipeline_name,
        environment_dict,
        mode,
        solid_name,
        step_keys,
        dag,
        dag_id,
        op_kwargs,
    ):
        tmp_dir = op_kwargs.pop('tmp_dir', DOCKER_TEMPDIR)
        host_tmp_dir = op_kwargs.pop('host_tmp_dir', seven.get_system_temp_directory())

        if 'storage' not in environment_dict:
            raise airflow_storage_exception(tmp_dir)

        # black 18.9b0 doesn't support py27-compatible formatting of the below invocation (omitting
        # the trailing comma after **op_kwargs) -- black 19.3b0 supports multiple python versions,
        # but currently doesn't know what to do with from __future__ import print_function -- see
        # https://github.com/ambv/black/issues/768
        # fmt: off
        return DagsterDockerOperator(
            step=solid_name,
            environment_dict=environment_dict,
            dag=dag,
            tmp_dir=tmp_dir,
            pipeline_name=pipeline_name,
            mode=mode,
            step_keys=step_keys,
            task_id=solid_name,
            host_tmp_dir=host_tmp_dir,
            **op_kwargs
        )
        # fmt: on

    @property
    def run_id(self):
        if self._run_id is None:
            return ''
        else:
            return self._run_id

    @property
    def query(self):
        variables = {
            'executionParams': {
                'environmentConfigData': self.environment_dict,
                'mode': self.mode,
                'selector': {'name': self.pipeline_name},
                'executionMetadata': {'runId': self.run_id},
                'stepKeys': self.step_keys,
            }
        }

        if self.airflow_ts:
            variables['executionParams']['executionMetadata']['tags'] = [
                {'key': 'airflow_ts', 'value': self.airflow_ts},
                {
                    'key': 'execution_epoch_time',
                    'value': '%f' % convert_airflow_datestr_to_epoch_ts(self.airflow_ts),
                },
            ]

        return '-v \'{variables}\' {query}'.format(
            variables=seven.json.dumps(variables), query=QUERY
        )

    def get_command(self):
        if self.command is not None and self.command.strip().find('[') == 0:
            commands = ast.literal_eval(self.command)
        elif self.command is not None:
            commands = self.command
        else:
            commands = self.query
        return commands

    def get_hook(self):
        if self.docker_conn_id_set:
            return super(DagsterDockerOperator, self).get_hook()

        class _DummyHook(object):
            def get_conn(self):
                return from_env().api

        return _DummyHook()

    def execute(self, context):
        if 'run_id' in self.params:
            self._run_id = self.params['run_id']
        elif 'dag_run' in context and context['dag_run'] is not None:
            self._run_id = context['dag_run'].run_id

        try:
            self.log.debug('Executing with query: {query}'.format(query=self.query))

            raw_res = super(DagsterDockerOperator, self).execute(context)
            self.log.info('Finished executing container.')

            (res, raw_res) = parse_raw_res(raw_res)

            self.handle_errors(res, raw_res)

            return self.handle_result(res)

        finally:
            self._run_id = None

    # This is a class-private name on DockerOperator for no good reason --
    # all that the status quo does is inhibit extension of the class.
    # See https://issues.apache.org/jira/browse/AIRFLOW-3880
    def __get_tls_config(self):
        # pylint:disable=no-member
        return super(DagsterDockerOperator, self)._ModifiedDockerOperator__get_tls_config()

    @contextmanager
    def get_host_tmp_dir(self):
        yield self.host_tmp_dir


class DagsterPythonOperator(PythonOperator, DagsterOperator):
    @classmethod
    def make_python_callable(cls, handle, pipeline_name, mode, environment_dict, step_keys):
        try:
            from dagster_graphql.cli import execute_query_from_cli
        except ImportError:
            raise AirflowException(
                'To use the DagsterPythonOperator, dagster and dagster_graphql must be installed '
                'in your Airflow environment.'
            )

        def python_callable(ts, dag_run, **kwargs):  # pylint: disable=unused-argument
            run_id = dag_run.run_id

            def _construct_variables(redact=False):
                return {
                    'executionParams': {
                        'environmentConfigData': 'REDACTED' if redact else environment_dict,
                        'mode': mode,
                        'selector': {'name': pipeline_name},
                        'executionMetadata': {
                            'runId': run_id,
                            'tags': [
                                {'key': 'airflow_ts', 'value': ts},
                                {
                                    'key': 'execution_epoch_time',
                                    'value': '%f' % convert_airflow_datestr_to_epoch_ts(ts),
                                },
                            ],
                        },
                        'stepKeys': step_keys,
                    }
                }

            # TODO: This removes the environment, pending an effective way to scrub it for secrets:
            # it's very useful for debugging to understand the GraphQL query that's being executed.
            # We should update to include the sanitized environment_dict.
            logging.info(
                'Executing GraphQL query:\n'
                + QUERY
                + '\n'
                + 'with variables:\n'
                + seven.json.dumps(_construct_variables(redact=True), indent=2)
            )

            res = json.loads(
                execute_query_from_cli(
                    handle, QUERY, variables=seven.json.dumps(_construct_variables())
                )
            )
            cls.handle_errors(res, None)
            return cls.handle_result(res)

        return python_callable

    @classmethod
    def operator_for_solid(
        cls,
        handle,
        pipeline_name,
        environment_dict,
        mode,
        solid_name,
        step_keys,
        dag,
        dag_id,
        op_kwargs,
    ):
        if 'storage' not in environment_dict:
            raise airflow_storage_exception('/tmp/special_place')

        # black 18.9b0 doesn't support py27-compatible formatting of the below invocation (omitting
        # the trailing comma after **op_kwargs) -- black 19.3b0 supports multiple python versions,
        # but currently doesn't know what to do with from __future__ import print_function -- see
        # https://github.com/ambv/black/issues/768
        # fmt: off
        return PythonOperator(
            task_id=solid_name,
            provide_context=True,
            python_callable=cls.make_python_callable(
                handle,
                pipeline_name,
                mode,
                environment_dict,
                step_keys
            ),
            dag=dag,
            **op_kwargs
        )
        # fmt: on
