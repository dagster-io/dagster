# serializer version: 1
# name: TestDynamicCommandExecution.test_command_execution[agent_error_agent_not_found_json]
  '''
  {"error": "Agent with ID 'nonexistent-agent-uuid' not found"}
  {"error": "Agent not found: nonexistent-agent-uuid"}
  Error: Failed to get agent: Agent not found: nonexistent-agent-uuid
  
  '''
# ---
# name: TestDynamicCommandExecution.test_command_execution[agent_error_agent_not_found_text_text]
  '''
  Agent with ID 'nonexistent-agent-uuid' not found
  Error querying Dagster Plus API: Agent not found: nonexistent-agent-uuid
  Error: Failed to get agent: Agent not found: nonexistent-agent-uuid
  
  '''
# ---
# name: TestDynamicCommandExecution.test_command_execution[agent_error_malformed_agent_id_json]
  '''
  {"error": "Agent with ID '''' not found"}
  {"error": "Agent not found: ''"}
  Error: Failed to get agent: Agent not found: ''
  
  '''
# ---
# name: TestDynamicCommandExecution.test_command_execution[agent_error_malformed_agent_id_text_text]
  '''
  Agent with ID '''' not found
  Error querying Dagster Plus API: Agent not found: ''
  Error: Failed to get agent: Agent not found: ''
  
  '''
# ---
# name: TestDynamicCommandExecution.test_command_execution[agent_success_agent_version_metadata_json]
  dict({
    'agent_label': None,
    'id': 'da7da2bb-33eb-4ce1-93ef-1a92e6737236',
    'last_heartbeat_time': 1758118915.5946717,
    'metadata': list([
      dict({
        'key': 'version',
        'value': '"1.5.1"',
      }),
      dict({
        'key': 'type',
        'value': '"K8sUserCodeLauncher"',
      }),
      dict({
        'key': 'queues',
        'value': '["default queue"]',
      }),
    ]),
    'status': 'RUNNING',
  })
# ---
# name: TestDynamicCommandExecution.test_command_execution[agent_success_agent_with_metadata_json]
  dict({
    'agent_label': None,
    'id': '858fd654-d0b5-477c-ac1a-c63db56246b7',
    'last_heartbeat_time': 1758118924.6589644,
    'metadata': list([
      dict({
        'key': 'utilization_metrics',
        'value': '{}',
      }),
      dict({
        'key': 'version',
        'value': '"1.11.10"',
      }),
      dict({
        'key': 'type',
        'value': '"K8sUserCodeLauncher"',
      }),
      dict({
        'key': 'queues',
        'value': '["default queue"]',
      }),
    ]),
    'status': 'RUNNING',
  })
# ---
# name: TestDynamicCommandExecution.test_command_execution[agent_success_agent_without_label_text_text]
  '''
  Label: Agent da7da2bb
  ID: da7da2bb-33eb-4ce1-93ef-1a92e6737236
  Status: RUNNING
  Last Heartbeat: 2025-09-17 14:21:55
  
  Metadata:
    version: "1.5.1"
    type: "K8sUserCodeLauncher"
    queues: ["default queue"]
  
  '''
# ---
# name: TestDynamicCommandExecution.test_command_execution[agent_success_multiple_agents_json]
  dict({
    'items': list([
      dict({
        'agent_label': None,
        'id': '858fd654-d0b5-477c-ac1a-c63db56246b7',
        'last_heartbeat_time': 1758118924.6589644,
        'metadata': list([
          dict({
            'key': 'utilization_metrics',
            'value': '{}',
          }),
          dict({
            'key': 'version',
            'value': '"1.11.10"',
          }),
          dict({
            'key': 'type',
            'value': '"K8sUserCodeLauncher"',
          }),
          dict({
            'key': 'queues',
            'value': '["default queue"]',
          }),
        ]),
        'status': 'RUNNING',
      }),
      dict({
        'agent_label': None,
        'id': 'da7da2bb-33eb-4ce1-93ef-1a92e6737236',
        'last_heartbeat_time': 1758118915.5946717,
        'metadata': list([
          dict({
            'key': 'version',
            'value': '"1.5.1"',
          }),
          dict({
            'key': 'type',
            'value': '"K8sUserCodeLauncher"',
          }),
          dict({
            'key': 'queues',
            'value': '["default queue"]',
          }),
        ]),
        'status': 'RUNNING',
      }),
    ]),
    'total': 2,
  })
# ---
# name: TestDynamicCommandExecution.test_command_execution[agent_success_multiple_agents_text_text]
  '''
  Label: Agent 858fd654
  ID: 858fd654-d0b5-477c-ac1a-c63db56246b7
  Status: RUNNING
  Last Heartbeat: 2025-09-17 14:22:04
  
  Label: Agent da7da2bb
  ID: da7da2bb-33eb-4ce1-93ef-1a92e6737236
  Status: RUNNING
  Last Heartbeat: 2025-09-17 14:21:55
  
  '''
# ---
# name: TestDynamicCommandExecution.test_command_execution[agent_success_running_agent_json]
  dict({
    'agent_label': None,
    'id': '858fd654-d0b5-477c-ac1a-c63db56246b7',
    'last_heartbeat_time': 1758118924.6589644,
    'metadata': list([
      dict({
        'key': 'utilization_metrics',
        'value': '{}',
      }),
      dict({
        'key': 'version',
        'value': '"1.11.10"',
      }),
      dict({
        'key': 'type',
        'value': '"K8sUserCodeLauncher"',
      }),
      dict({
        'key': 'queues',
        'value': '["default queue"]',
      }),
    ]),
    'status': 'RUNNING',
  })
# ---
# name: TestDynamicCommandExecution.test_command_execution[agent_success_running_agent_text_text]
  '''
  Label: Agent 858fd654
  ID: 858fd654-d0b5-477c-ac1a-c63db56246b7
  Status: RUNNING
  Last Heartbeat: 2025-09-17 14:22:04
  
  Metadata:
    utilization_metrics: {}
    version: "1.11.10"
    type: "K8sUserCodeLauncher"
    queues: ["default queue"]
  
  '''
# ---
# name: TestDynamicCommandExecution.test_command_execution[agent_success_second_agent_json_json]
  dict({
    'agent_label': None,
    'id': 'da7da2bb-33eb-4ce1-93ef-1a92e6737236',
    'last_heartbeat_time': 1758118915.5946717,
    'metadata': list([
      dict({
        'key': 'version',
        'value': '"1.5.1"',
      }),
      dict({
        'key': 'type',
        'value': '"K8sUserCodeLauncher"',
      }),
      dict({
        'key': 'queues',
        'value': '["default queue"]',
      }),
    ]),
    'status': 'RUNNING',
  })
# ---
# name: TestDynamicCommandExecution.test_command_execution[agent_success_second_agent_text_text]
  '''
  Label: Agent da7da2bb
  ID: da7da2bb-33eb-4ce1-93ef-1a92e6737236
  Status: RUNNING
  Last Heartbeat: 2025-09-17 14:21:55
  
  Metadata:
    version: "1.5.1"
    type: "K8sUserCodeLauncher"
    queues: ["default queue"]
  
  '''
# ---
# name: TestDynamicCommandExecution.test_command_execution[agent_success_single_agent_json]
  dict({
    'agent_label': None,
    'id': '858fd654-d0b5-477c-ac1a-c63db56246b7',
    'last_heartbeat_time': 1758118924.6589644,
    'metadata': list([
      dict({
        'key': 'utilization_metrics',
        'value': '{}',
      }),
      dict({
        'key': 'version',
        'value': '"1.11.10"',
      }),
      dict({
        'key': 'type',
        'value': '"K8sUserCodeLauncher"',
      }),
      dict({
        'key': 'queues',
        'value': '["default queue"]',
      }),
    ]),
    'status': 'RUNNING',
  })
# ---
# name: TestDynamicCommandExecution.test_command_execution[agent_success_single_agent_text_text]
  '''
  Label: Agent 858fd654
  ID: 858fd654-d0b5-477c-ac1a-c63db56246b7
  Status: RUNNING
  Last Heartbeat: 2025-09-17 14:22:04
  
  Metadata:
    utilization_metrics: {}
    version: "1.11.10"
    type: "K8sUserCodeLauncher"
    queues: ["default queue"]
  
  '''
# ---
# name: TestDynamicCommandExecution.test_command_execution[asset_error_asset_not_found_json]
  '''
  {"error": "Asset not found: nonexistent-asset"}
  Error: Failed to get asset: Asset not found: nonexistent-asset
  
  '''
# ---
# name: TestDynamicCommandExecution.test_command_execution[asset_error_asset_not_found_status_view_json]
  '''
  {"error": "Asset not found: nonexistent-asset"}
  Error: Failed to get asset: Asset not found: nonexistent-asset
  
  '''
# ---
# name: TestDynamicCommandExecution.test_command_execution[asset_error_invalid_view_json]
  '''
  Usage: dg api asset get [OPTIONS] ASSET_KEY
  Try 'dg api asset get -h' for help.
  
  Error: Invalid value for '--view': 'invalid' is not 'status'.
  
  '''
# ---
# name: TestDynamicCommandExecution.test_command_execution[asset_error_malformed_asset_key_json]
  '''
  {"error": "Asset not found: ''"}
  Error: Failed to get asset: Asset not found: ''
  
  '''
# ---
# name: TestDynamicCommandExecution.test_command_execution[asset_success_assets_status_view_human_readable_text]
  '''
  Asset Key: asset_health_report_slack
  ID: ["asset_health_report_slack"]
  Description: Sends Dagster asset health report to #bot-purina Slack channel
  Group: monitoring
  Kinds: slack
  Deps: None
  Asset Health: HEALTHY
  Materialization Status: HEALTHY
  Freshness Status: NOT_APPLICABLE
  Asset Checks Status: NOT_APPLICABLE
  Latest Materialization: 2025-09-30 13:01:29
  Latest Run ID: 68fb6e21-5928-4da1-b9fe-3f6a565e3a39
  
  Asset Key: aws/cloud-prod/user_roles
  ID: ["aws", "cloud-prod", "user_roles"]
  Description: Snowflake stages for AWS data, creates new stages for new assets, refreses existing stages.
  Group: aws_stages
  Kinds: None
  Deps: None
  Asset Health: HEALTHY
  Materialization Status: HEALTHY
  Freshness Status: NOT_APPLICABLE
  Asset Checks Status: NOT_APPLICABLE
  Latest Materialization: 2025-10-01 03:02:40
  Latest Run ID: 74a2f116-1e7a-41d8-9d6d-5b47c05ebbe0
  
  Asset Key: aws/cloud-prod/workspace_staging_asset_checks
  ID: ["aws", "cloud-prod", "workspace_staging_asset_checks"]
  Description: Snowflake stages for AWS data, creates new stages for new assets, refreses existing stages.
  Group: aws_stages
  Kinds: None
  Deps: cloud-prod-workspace-replication/staging/asset_checks
  Asset Health: HEALTHY
  Materialization Status: HEALTHY
  Freshness Status: HEALTHY
  Asset Checks Status: NOT_APPLICABLE
  Latest Materialization: 2025-10-01 09:20:37
  Latest Run ID: 3fb15668-f967-45ec-ac66-7682a6097583
  
  Asset Key: aws/cloud-prod/workspace_staging_assets
  ID: ["aws", "cloud-prod", "workspace_staging_assets"]
  Description: Snowflake stages for AWS data, creates new stages for new assets, refreses existing stages.
  Group: aws_stages
  Kinds: None
  Deps: cloud-prod-workspace-replication/staging/assets
  Asset Health: HEALTHY
  Materialization Status: HEALTHY
  Freshness Status: HEALTHY
  Asset Checks Status: NOT_APPLICABLE
  Latest Materialization: 2025-10-01 09:20:37
  Latest Run ID: 3fb15668-f967-45ec-ac66-7682a6097583
  
  Asset Key: aws/cloud-prod/workspace_staging_external_repo_metadata
  ID: ["aws", "cloud-prod", "workspace_staging_external_repo_metadata"]
  Description: Snowflake stages for AWS data, creates new stages for new assets, refreses existing stages.
  Group: aws_stages
  Kinds: None
  Deps: cloud-prod-workspace-replication/staging/external_repo_metadata
  Asset Health: HEALTHY
  Materialization Status: HEALTHY
  Freshness Status: HEALTHY
  Asset Checks Status: NOT_APPLICABLE
  Latest Materialization: 2025-10-01 09:20:37
  Latest Run ID: 3fb15668-f967-45ec-ac66-7682a6097583
  
  Asset Key: aws/cloud-prod/workspace_staging_jobs
  ID: ["aws", "cloud-prod", "workspace_staging_jobs"]
  Description: Snowflake stages for AWS data, creates new stages for new assets, refreses existing stages.
  Group: aws_stages
  Kinds: None
  Deps: cloud-prod-workspace-replication/staging/jobs
  Asset Health: HEALTHY
  Materialization Status: HEALTHY
  Freshness Status: HEALTHY
  Asset Checks Status: NOT_APPLICABLE
  Latest Materialization: 2025-10-01 09:20:37
  Latest Run ID: 3fb15668-f967-45ec-ac66-7682a6097583
  
  Asset Key: aws/cloud-prod/workspace_staging_metadata
  ID: ["aws", "cloud-prod", "workspace_staging_metadata"]
  Description: Snowflake stages for AWS data, creates new stages for new assets, refreses existing stages.
  Group: aws_stages
  Kinds: None
  Deps: cloud-prod-workspace-replication/staging/metadata
  Asset Health: HEALTHY
  Materialization Status: HEALTHY
  Freshness Status: HEALTHY
  Asset Checks Status: NOT_APPLICABLE
  Latest Materialization: 2025-10-01 09:20:37
  Latest Run ID: 3fb15668-f967-45ec-ac66-7682a6097583
  
  Asset Key: aws/cloud-prod/workspace_staging_partitions
  ID: ["aws", "cloud-prod", "workspace_staging_partitions"]
  Description: Snowflake stages for AWS data, creates new stages for new assets, refreses existing stages.
  Group: aws_stages
  Kinds: None
  Deps: cloud-prod-workspace-replication/staging/partitions
  Asset Health: HEALTHY
  Materialization Status: HEALTHY
  Freshness Status: HEALTHY
  Asset Checks Status: NOT_APPLICABLE
  Latest Materialization: 2025-10-01 09:20:37
  Latest Run ID: 3fb15668-f967-45ec-ac66-7682a6097583
  
  '''
# ---
# name: TestDynamicCommandExecution.test_command_execution[asset_success_assets_status_view_json]
  dict({
    'cursor': '["aws", "cloud-prod", "workspace_staging_partitions_ext"]',
    'has_more': True,
    'items': list([
      dict({
        'asset_key': 'asset_health_report_slack',
        'asset_key_parts': list([
          'asset_health_report_slack',
        ]),
        'dependency_keys': list([
        ]),
        'description': 'Sends Dagster asset health report to #bot-purina Slack channel',
        'group_name': 'monitoring',
        'id': '["asset_health_report_slack"]',
        'kinds': list([
          'slack',
        ]),
        'metadata_entries': list([
        ]),
        'status': dict({
          'asset_checks_status': 'NOT_APPLICABLE',
          'asset_health': 'HEALTHY',
          'checks_status': dict({
            'num_failed_checks': None,
            'num_warning_checks': None,
            'status': 'NOT_APPLICABLE',
            'total_num_checks': None,
          }),
          'freshness_info': None,
          'freshness_status': 'NOT_APPLICABLE',
          'health_metadata': None,
          'latest_materialization': dict({
            'partition': None,
            'run_id': '68fb6e21-5928-4da1-b9fe-3f6a565e3a39',
            'timestamp': 1759237289874.0,
          }),
          'materialization_status': 'HEALTHY',
        }),
      }),
      dict({
        'asset_key': 'aws/cloud-prod/user_roles',
        'asset_key_parts': list([
          'aws',
          'cloud-prod',
          'user_roles',
        ]),
        'dependency_keys': list([
        ]),
        'description': 'Snowflake stages for AWS data, creates new stages for new assets, refreses existing stages.',
        'group_name': 'aws_stages',
        'id': '["aws", "cloud-prod", "user_roles"]',
        'kinds': list([
        ]),
        'metadata_entries': list([
        ]),
        'status': dict({
          'asset_checks_status': 'NOT_APPLICABLE',
          'asset_health': 'HEALTHY',
          'checks_status': dict({
            'num_failed_checks': None,
            'num_warning_checks': None,
            'status': 'NOT_APPLICABLE',
            'total_num_checks': None,
          }),
          'freshness_info': None,
          'freshness_status': 'NOT_APPLICABLE',
          'health_metadata': None,
          'latest_materialization': dict({
            'partition': None,
            'run_id': '74a2f116-1e7a-41d8-9d6d-5b47c05ebbe0',
            'timestamp': 1759287760784.0,
          }),
          'materialization_status': 'HEALTHY',
        }),
      }),
      dict({
        'asset_key': 'aws/cloud-prod/workspace_staging_asset_checks',
        'asset_key_parts': list([
          'aws',
          'cloud-prod',
          'workspace_staging_asset_checks',
        ]),
        'dependency_keys': list([
          'cloud-prod-workspace-replication/staging/asset_checks',
        ]),
        'description': 'Snowflake stages for AWS data, creates new stages for new assets, refreses existing stages.',
        'group_name': 'aws_stages',
        'id': '["aws", "cloud-prod", "workspace_staging_asset_checks"]',
        'kinds': list([
        ]),
        'metadata_entries': list([
        ]),
        'status': dict({
          'asset_checks_status': 'NOT_APPLICABLE',
          'asset_health': 'HEALTHY',
          'checks_status': dict({
            'num_failed_checks': None,
            'num_warning_checks': None,
            'status': 'NOT_APPLICABLE',
            'total_num_checks': None,
          }),
          'freshness_info': dict({
            'cron_schedule': None,
            'current_lag_minutes': None,
            'current_minutes_late': None,
            'latest_materialization_minutes_late': None,
            'maximum_lag_minutes': None,
          }),
          'freshness_status': 'HEALTHY',
          'health_metadata': None,
          'latest_materialization': dict({
            'partition': None,
            'run_id': '3fb15668-f967-45ec-ac66-7682a6097583',
            'timestamp': 1759310437939.0,
          }),
          'materialization_status': 'HEALTHY',
        }),
      }),
      dict({
        'asset_key': 'aws/cloud-prod/workspace_staging_assets',
        'asset_key_parts': list([
          'aws',
          'cloud-prod',
          'workspace_staging_assets',
        ]),
        'dependency_keys': list([
          'cloud-prod-workspace-replication/staging/assets',
        ]),
        'description': 'Snowflake stages for AWS data, creates new stages for new assets, refreses existing stages.',
        'group_name': 'aws_stages',
        'id': '["aws", "cloud-prod", "workspace_staging_assets"]',
        'kinds': list([
        ]),
        'metadata_entries': list([
        ]),
        'status': dict({
          'asset_checks_status': 'NOT_APPLICABLE',
          'asset_health': 'HEALTHY',
          'checks_status': dict({
            'num_failed_checks': None,
            'num_warning_checks': None,
            'status': 'NOT_APPLICABLE',
            'total_num_checks': None,
          }),
          'freshness_info': dict({
            'cron_schedule': None,
            'current_lag_minutes': None,
            'current_minutes_late': None,
            'latest_materialization_minutes_late': None,
            'maximum_lag_minutes': None,
          }),
          'freshness_status': 'HEALTHY',
          'health_metadata': None,
          'latest_materialization': dict({
            'partition': None,
            'run_id': '3fb15668-f967-45ec-ac66-7682a6097583',
            'timestamp': 1759310437219.0,
          }),
          'materialization_status': 'HEALTHY',
        }),
      }),
      dict({
        'asset_key': 'aws/cloud-prod/workspace_staging_external_repo_metadata',
        'asset_key_parts': list([
          'aws',
          'cloud-prod',
          'workspace_staging_external_repo_metadata',
        ]),
        'dependency_keys': list([
          'cloud-prod-workspace-replication/staging/external_repo_metadata',
        ]),
        'description': 'Snowflake stages for AWS data, creates new stages for new assets, refreses existing stages.',
        'group_name': 'aws_stages',
        'id': '["aws", "cloud-prod", "workspace_staging_external_repo_metadata"]',
        'kinds': list([
        ]),
        'metadata_entries': list([
        ]),
        'status': dict({
          'asset_checks_status': 'NOT_APPLICABLE',
          'asset_health': 'HEALTHY',
          'checks_status': dict({
            'num_failed_checks': None,
            'num_warning_checks': None,
            'status': 'NOT_APPLICABLE',
            'total_num_checks': None,
          }),
          'freshness_info': dict({
            'cron_schedule': None,
            'current_lag_minutes': None,
            'current_minutes_late': None,
            'latest_materialization_minutes_late': None,
            'maximum_lag_minutes': None,
          }),
          'freshness_status': 'HEALTHY',
          'health_metadata': None,
          'latest_materialization': dict({
            'partition': None,
            'run_id': '3fb15668-f967-45ec-ac66-7682a6097583',
            'timestamp': 1759310437858.0,
          }),
          'materialization_status': 'HEALTHY',
        }),
      }),
      dict({
        'asset_key': 'aws/cloud-prod/workspace_staging_jobs',
        'asset_key_parts': list([
          'aws',
          'cloud-prod',
          'workspace_staging_jobs',
        ]),
        'dependency_keys': list([
          'cloud-prod-workspace-replication/staging/jobs',
        ]),
        'description': 'Snowflake stages for AWS data, creates new stages for new assets, refreses existing stages.',
        'group_name': 'aws_stages',
        'id': '["aws", "cloud-prod", "workspace_staging_jobs"]',
        'kinds': list([
        ]),
        'metadata_entries': list([
        ]),
        'status': dict({
          'asset_checks_status': 'NOT_APPLICABLE',
          'asset_health': 'HEALTHY',
          'checks_status': dict({
            'num_failed_checks': None,
            'num_warning_checks': None,
            'status': 'NOT_APPLICABLE',
            'total_num_checks': None,
          }),
          'freshness_info': dict({
            'cron_schedule': None,
            'current_lag_minutes': None,
            'current_minutes_late': None,
            'latest_materialization_minutes_late': None,
            'maximum_lag_minutes': None,
          }),
          'freshness_status': 'HEALTHY',
          'health_metadata': None,
          'latest_materialization': dict({
            'partition': None,
            'run_id': '3fb15668-f967-45ec-ac66-7682a6097583',
            'timestamp': 1759310437770.0,
          }),
          'materialization_status': 'HEALTHY',
        }),
      }),
      dict({
        'asset_key': 'aws/cloud-prod/workspace_staging_metadata',
        'asset_key_parts': list([
          'aws',
          'cloud-prod',
          'workspace_staging_metadata',
        ]),
        'dependency_keys': list([
          'cloud-prod-workspace-replication/staging/metadata',
        ]),
        'description': 'Snowflake stages for AWS data, creates new stages for new assets, refreses existing stages.',
        'group_name': 'aws_stages',
        'id': '["aws", "cloud-prod", "workspace_staging_metadata"]',
        'kinds': list([
        ]),
        'metadata_entries': list([
        ]),
        'status': dict({
          'asset_checks_status': 'NOT_APPLICABLE',
          'asset_health': 'HEALTHY',
          'checks_status': dict({
            'num_failed_checks': None,
            'num_warning_checks': None,
            'status': 'NOT_APPLICABLE',
            'total_num_checks': None,
          }),
          'freshness_info': dict({
            'cron_schedule': None,
            'current_lag_minutes': None,
            'current_minutes_late': None,
            'latest_materialization_minutes_late': None,
            'maximum_lag_minutes': None,
          }),
          'freshness_status': 'HEALTHY',
          'health_metadata': None,
          'latest_materialization': dict({
            'partition': None,
            'run_id': '3fb15668-f967-45ec-ac66-7682a6097583',
            'timestamp': 1759310437135.0,
          }),
          'materialization_status': 'HEALTHY',
        }),
      }),
      dict({
        'asset_key': 'aws/cloud-prod/workspace_staging_partitions',
        'asset_key_parts': list([
          'aws',
          'cloud-prod',
          'workspace_staging_partitions',
        ]),
        'dependency_keys': list([
          'cloud-prod-workspace-replication/staging/partitions',
        ]),
        'description': 'Snowflake stages for AWS data, creates new stages for new assets, refreses existing stages.',
        'group_name': 'aws_stages',
        'id': '["aws", "cloud-prod", "workspace_staging_partitions"]',
        'kinds': list([
        ]),
        'metadata_entries': list([
        ]),
        'status': dict({
          'asset_checks_status': 'NOT_APPLICABLE',
          'asset_health': 'HEALTHY',
          'checks_status': dict({
            'num_failed_checks': None,
            'num_warning_checks': None,
            'status': 'NOT_APPLICABLE',
            'total_num_checks': None,
          }),
          'freshness_info': dict({
            'cron_schedule': None,
            'current_lag_minutes': None,
            'current_minutes_late': None,
            'latest_materialization_minutes_late': None,
            'maximum_lag_minutes': None,
          }),
          'freshness_status': 'HEALTHY',
          'health_metadata': None,
          'latest_materialization': dict({
            'partition': None,
            'run_id': '3fb15668-f967-45ec-ac66-7682a6097583',
            'timestamp': 1759310437442.0,
          }),
          'materialization_status': 'HEALTHY',
        }),
      }),
    ]),
  })
# ---
# name: TestDynamicCommandExecution.test_command_execution[asset_success_assets_status_view_with_cursor_json]
  dict({
    'cursor': None,
    'has_more': False,
    'items': list([
    ]),
  })
# ---
# name: TestDynamicCommandExecution.test_command_execution[asset_success_assets_with_cursor_json]
  dict({
    'cursor': None,
    'has_more': False,
    'items': list([
    ]),
  })
# ---
# name: TestDynamicCommandExecution.test_command_execution[asset_success_multiple_assets_json]
  dict({
    'cursor': '["aws", "cloud-prod", "workspace_staging_partitions_ext"]',
    'has_more': True,
    'items': list([
      dict({
        'asset_key': 'asset_health_report_slack',
        'asset_key_parts': list([
          'asset_health_report_slack',
        ]),
        'dependency_keys': list([
        ]),
        'description': 'Sends Dagster asset health report to #bot-purina Slack channel',
        'group_name': 'monitoring',
        'id': 'dagster_open_platform.__repository__.["asset_health_report_slack"]',
        'kinds': list([
          'slack',
        ]),
        'metadata_entries': list([
        ]),
        'status': None,
      }),
      dict({
        'asset_key': 'aws/cloud-prod/user_roles',
        'asset_key_parts': list([
          'aws',
          'cloud-prod',
          'user_roles',
        ]),
        'dependency_keys': list([
        ]),
        'description': 'Snowflake stages for AWS data, creates new stages for new assets, refreses existing stages.',
        'group_name': 'aws_stages',
        'id': 'dagster_open_platform.__repository__.["aws", "cloud-prod", "user_roles"]',
        'kinds': list([
        ]),
        'metadata_entries': list([
        ]),
        'status': None,
      }),
      dict({
        'asset_key': 'aws/cloud-prod/workspace_staging_asset_checks',
        'asset_key_parts': list([
          'aws',
          'cloud-prod',
          'workspace_staging_asset_checks',
        ]),
        'dependency_keys': list([
          'cloud-prod-workspace-replication/staging/asset_checks',
        ]),
        'description': 'Snowflake stages for AWS data, creates new stages for new assets, refreses existing stages.',
        'group_name': 'aws_stages',
        'id': 'dagster_open_platform.__repository__.["aws", "cloud-prod", "workspace_staging_asset_checks"]',
        'kinds': list([
        ]),
        'metadata_entries': list([
        ]),
        'status': None,
      }),
      dict({
        'asset_key': 'aws/cloud-prod/workspace_staging_assets',
        'asset_key_parts': list([
          'aws',
          'cloud-prod',
          'workspace_staging_assets',
        ]),
        'dependency_keys': list([
          'cloud-prod-workspace-replication/staging/assets',
        ]),
        'description': 'Snowflake stages for AWS data, creates new stages for new assets, refreses existing stages.',
        'group_name': 'aws_stages',
        'id': 'dagster_open_platform.__repository__.["aws", "cloud-prod", "workspace_staging_assets"]',
        'kinds': list([
        ]),
        'metadata_entries': list([
        ]),
        'status': None,
      }),
      dict({
        'asset_key': 'aws/cloud-prod/workspace_staging_external_repo_metadata',
        'asset_key_parts': list([
          'aws',
          'cloud-prod',
          'workspace_staging_external_repo_metadata',
        ]),
        'dependency_keys': list([
          'cloud-prod-workspace-replication/staging/external_repo_metadata',
        ]),
        'description': 'Snowflake stages for AWS data, creates new stages for new assets, refreses existing stages.',
        'group_name': 'aws_stages',
        'id': 'dagster_open_platform.__repository__.["aws", "cloud-prod", "workspace_staging_external_repo_metadata"]',
        'kinds': list([
        ]),
        'metadata_entries': list([
        ]),
        'status': None,
      }),
      dict({
        'asset_key': 'aws/cloud-prod/workspace_staging_jobs',
        'asset_key_parts': list([
          'aws',
          'cloud-prod',
          'workspace_staging_jobs',
        ]),
        'dependency_keys': list([
          'cloud-prod-workspace-replication/staging/jobs',
        ]),
        'description': 'Snowflake stages for AWS data, creates new stages for new assets, refreses existing stages.',
        'group_name': 'aws_stages',
        'id': 'dagster_open_platform.__repository__.["aws", "cloud-prod", "workspace_staging_jobs"]',
        'kinds': list([
        ]),
        'metadata_entries': list([
        ]),
        'status': None,
      }),
      dict({
        'asset_key': 'aws/cloud-prod/workspace_staging_metadata',
        'asset_key_parts': list([
          'aws',
          'cloud-prod',
          'workspace_staging_metadata',
        ]),
        'dependency_keys': list([
          'cloud-prod-workspace-replication/staging/metadata',
        ]),
        'description': 'Snowflake stages for AWS data, creates new stages for new assets, refreses existing stages.',
        'group_name': 'aws_stages',
        'id': 'dagster_open_platform.__repository__.["aws", "cloud-prod", "workspace_staging_metadata"]',
        'kinds': list([
        ]),
        'metadata_entries': list([
        ]),
        'status': None,
      }),
      dict({
        'asset_key': 'aws/cloud-prod/workspace_staging_partitions',
        'asset_key_parts': list([
          'aws',
          'cloud-prod',
          'workspace_staging_partitions',
        ]),
        'dependency_keys': list([
          'cloud-prod-workspace-replication/staging/partitions',
        ]),
        'description': 'Snowflake stages for AWS data, creates new stages for new assets, refreses existing stages.',
        'group_name': 'aws_stages',
        'id': 'dagster_open_platform.__repository__.["aws", "cloud-prod", "workspace_staging_partitions"]',
        'kinds': list([
        ]),
        'metadata_entries': list([
        ]),
        'status': None,
      }),
    ]),
  })
# ---
# name: TestDynamicCommandExecution.test_command_execution[asset_success_nested_asset_json]
  dict({
    'asset_key': 'aws/cloud-prod/workspace_staging_assets',
    'asset_key_parts': list([
      'aws',
      'cloud-prod',
      'workspace_staging_assets',
    ]),
    'dependency_keys': list([
      'cloud-prod-workspace-replication/staging/assets',
    ]),
    'description': 'Snowflake stages for AWS data, creates new stages for new assets, refreses existing stages.',
    'group_name': 'aws_stages',
    'id': 'dagster_open_platform.__repository__.["aws", "cloud-prod", "workspace_staging_assets"]',
    'kinds': list([
    ]),
    'metadata_entries': list([
    ]),
    'status': None,
  })
# ---
# name: TestDynamicCommandExecution.test_command_execution[asset_success_paginated_assets_json]
  dict({
    'cursor': '["__snowflake_query_metadata_self_serve_customer_attribution_check_all_partitions_job"]',
    'has_more': True,
    'items': list([
    ]),
  })
# ---
# name: TestDynamicCommandExecution.test_command_execution[asset_success_paginated_assets_status_view_json]
  dict({
    'cursor': '["__snowflake_query_metadata_self_serve_customer_attribution_check_all_partitions_job"]',
    'has_more': True,
    'items': list([
    ]),
  })
# ---
# name: TestDynamicCommandExecution.test_command_execution[asset_success_single_asset_json]
  dict({
    'asset_key': 'asset_health_report_slack',
    'asset_key_parts': list([
      'asset_health_report_slack',
    ]),
    'dependency_keys': list([
    ]),
    'description': 'Sends Dagster asset health report to #bot-purina Slack channel',
    'group_name': 'monitoring',
    'id': 'dagster_open_platform.__repository__.["asset_health_report_slack"]',
    'kinds': list([
      'slack',
    ]),
    'metadata_entries': list([
    ]),
    'status': None,
  })
# ---
# name: TestDynamicCommandExecution.test_command_execution[asset_success_single_asset_status_view_human_readable_text]
  '''
  Asset Key: aws/cloud-prod/workspace_staging_metadata
  ID: ["aws", "cloud-prod", "workspace_staging_metadata"]
  Description: Snowflake stages for AWS data, creates new stages for new assets, refreses existing stages.
  Group: aws_stages
  Kinds: None
  Deps: cloud-prod-workspace-replication/staging/metadata
  
  Status Information:
    Asset Health: HEALTHY
    Materialization Status: HEALTHY
    Freshness Status: HEALTHY
    Asset Checks Status: NOT_APPLICABLE
    Latest Materialization: 2025-10-01 09:20:37
    Latest Run ID: 3fb15668-f967-45ec-ac66-7682a6097583
  
  '''
# ---
# name: TestDynamicCommandExecution.test_command_execution[asset_success_single_asset_status_view_json]
  dict({
    'asset_key': 'aws/cloud-prod/workspace_staging_jobs',
    'asset_key_parts': list([
      'aws',
      'cloud-prod',
      'workspace_staging_jobs',
    ]),
    'dependency_keys': list([
      'cloud-prod-workspace-replication/staging/jobs',
    ]),
    'description': 'Snowflake stages for AWS data, creates new stages for new assets, refreses existing stages.',
    'group_name': 'aws_stages',
    'id': '["aws", "cloud-prod", "workspace_staging_jobs"]',
    'kinds': list([
    ]),
    'metadata_entries': list([
    ]),
    'status': dict({
      'asset_checks_status': 'NOT_APPLICABLE',
      'asset_health': 'HEALTHY',
      'checks_status': dict({
        'num_failed_checks': None,
        'num_warning_checks': None,
        'status': 'NOT_APPLICABLE',
        'total_num_checks': None,
      }),
      'freshness_info': dict({
        'cron_schedule': None,
        'current_lag_minutes': None,
        'current_minutes_late': None,
        'latest_materialization_minutes_late': None,
        'maximum_lag_minutes': None,
      }),
      'freshness_status': 'HEALTHY',
      'health_metadata': None,
      'latest_materialization': dict({
        'partition': None,
        'run_id': '3fb15668-f967-45ec-ac66-7682a6097583',
        'timestamp': 1759310437770.0,
      }),
      'materialization_status': 'HEALTHY',
    }),
  })
# ---
# name: TestDynamicCommandExecution.test_command_execution[deployment_empty_deployments_json]
  dict({
    'items': list([
      dict({
        'id': 87,
        'name': 'data-eng-prod',
        'type': 'PRODUCTION',
      }),
      dict({
        'id': 190248,
        'name': 'gtm20',
        'type': 'PRODUCTION',
      }),
      dict({
        'id': 213933,
        'name': 'data-eng-staging',
        'type': 'PRODUCTION',
      }),
      dict({
        'id': 217021,
        'name': 'data-eng-dev',
        'type': 'PRODUCTION',
      }),
    ]),
    'total': 4,
  })
# ---
# name: TestDynamicCommandExecution.test_command_execution[deployment_success_multiple_deployments_json]
  dict({
    'items': list([
      dict({
        'id': 87,
        'name': 'data-eng-prod',
        'type': 'PRODUCTION',
      }),
      dict({
        'id': 190248,
        'name': 'gtm20',
        'type': 'PRODUCTION',
      }),
      dict({
        'id': 213933,
        'name': 'data-eng-staging',
        'type': 'PRODUCTION',
      }),
      dict({
        'id': 217021,
        'name': 'data-eng-dev',
        'type': 'PRODUCTION',
      }),
    ]),
    'total': 4,
  })
# ---
# name: TestDynamicCommandExecution.test_command_execution[deployment_success_multiple_deployments_text_text]
  '''
  Name: data-eng-prod
  ID: 87
  Type: PRODUCTION
  
  Name: gtm20
  ID: 190248
  Type: PRODUCTION
  
  Name: data-eng-staging
  ID: 213933
  Type: PRODUCTION
  
  Name: data-eng-dev
  ID: 217021
  Type: PRODUCTION
  
  '''
# ---
# name: TestDynamicCommandExecution.test_command_execution[log_error_cursor_no_recording_json_json]
  '''
  {"error": "Exhausted 0 responses"}
  Error: Failed to get logs for run 85156191-170d-4229-8eba-14450b6ca9e3: Exhausted 0 responses
  
  '''
# ---
# name: TestDynamicCommandExecution.test_command_execution[log_error_cursor_no_recording_text_text]
  '''
  Error querying Dagster Plus API: Exhausted 0 responses
  Error: Failed to get logs for run 85156191-170d-4229-8eba-14450b6ca9e3: Exhausted 0 responses
  
  '''
# ---
# name: TestDynamicCommandExecution.test_command_execution[log_error_empty_logs_no_recording_json_json]
  '''
  {"error": "Exhausted 0 responses"}
  Error: Failed to get logs for run empty-run-uuid-0000-0000-000000000000: Exhausted 0 responses
  
  '''
# ---
# name: TestDynamicCommandExecution.test_command_execution[log_error_empty_logs_no_recording_text_text]
  '''
  Error querying Dagster Plus API: Exhausted 0 responses
  Error: Failed to get logs for run empty-run-uuid-0000-0000-000000000000: Exhausted 0 responses
  
  '''
# ---
# name: TestDynamicCommandExecution.test_command_execution[log_error_empty_run_id_json_json]
  '''
  {"error": "Exhausted 0 responses"}
  Error: Failed to get logs for run '': Exhausted 0 responses
  
  '''
# ---
# name: TestDynamicCommandExecution.test_command_execution[log_error_empty_run_id_text_text]
  '''
  Error querying Dagster Plus API: Exhausted 0 responses
  Error: Failed to get logs for run '': Exhausted 0 responses
  
  '''
# ---
# name: TestDynamicCommandExecution.test_command_execution[log_error_invalid_level_json_json]
  dict({
    'count': 0,
    'cursor': 'eyJ0eXBlIjogIlNUT1JBR0VfSUQiLCAidmFsdWUiOiA0ODgwODU1MzE0M30=',
    'hasMore': False,
    'logs': list([
    ]),
    'run_id': '85156191-170d-4229-8eba-14450b6ca9e3',
  })
# ---
# name: TestDynamicCommandExecution.test_command_execution[log_error_invalid_level_text_text]
  '''
  No logs found for run 85156191-170d-4229-8eba-14450b6ca9e3
  
  '''
# ---
# name: TestDynamicCommandExecution.test_command_execution[log_error_malformed_run_id_json_json]
  '''
  {"error": "Exhausted 0 responses"}
  Error: Failed to get logs for run invalid-uuid-format: Exhausted 0 responses
  
  '''
# ---
# name: TestDynamicCommandExecution.test_command_execution[log_error_malformed_run_id_text_text]
  '''
  Error querying Dagster Plus API: Exhausted 0 responses
  Error: Failed to get logs for run invalid-uuid-format: Exhausted 0 responses
  
  '''
# ---
# name: TestDynamicCommandExecution.test_command_execution[log_error_run_not_found_json_json]
  '''
  {"error": "Exhausted 0 responses"}
  Error: Failed to get logs for run nonexistent-run-uuid-1234-5678-900000000000: Exhausted 0 responses
  
  '''
# ---
# name: TestDynamicCommandExecution.test_command_execution[log_error_run_not_found_text_text]
  '''
  Error querying Dagster Plus API: Exhausted 0 responses
  Error: Failed to get logs for run nonexistent-run-uuid-1234-5678-900000000000: Exhausted 0 responses
  
  '''
# ---
# name: TestDynamicCommandExecution.test_command_execution[log_success_logs_basic_json_json]
  dict({
    'count': 20,
    'cursor': 'eyJ0eXBlIjogIlNUT1JBR0VfSUQiLCAidmFsdWUiOiA0ODgyMTg1Nzg2MH0=',
    'hasMore': True,
    'logs': list([
      dict({
        'error': None,
        'eventType': 'ASSET_CHECK_EVALUATION_PLANNED',
        'level': 'DEBUG',
        'message': 'check_avg_orders_freshness_job intends to execute asset check anomaly_detection_freshness_check on asset ["MARKETING", "avg_orders"]',
        'runId': 'b45fff8a-7def-43b8-805d-1cf5f435c997',
        'stepKey': 'anomaly_detection_freshness_check_458e9b8f',
        'timestamp': '1758817801771',
      }),
      dict({
        'error': None,
        'eventType': 'RUN_ENQUEUED',
        'level': 'INFO',
        'message': '',
        'runId': 'b45fff8a-7def-43b8-805d-1cf5f435c997',
        'stepKey': None,
        'timestamp': '1758817801798',
      }),
      dict({
        'error': None,
        'eventType': 'RUN_STARTING',
        'level': 'INFO',
        'message': '',
        'runId': 'b45fff8a-7def-43b8-805d-1cf5f435c997',
        'stepKey': None,
        'timestamp': '1758817804629',
      }),
      dict({
        'error': None,
        'eventType': 'ENGINE_EVENT',
        'level': 'INFO',
        'message': 'Sending a request to the agent to execute steps in the user cloud',
        'runId': 'b45fff8a-7def-43b8-805d-1cf5f435c997',
        'stepKey': None,
        'timestamp': '1758817804660',
      }),
      dict({
        'error': None,
        'eventType': 'ENGINE_EVENT',
        'level': 'INFO',
        'message': '[DagsterCloudAgent] Agent d0a5374c is launching run b45fff8a-7def-43b8-805d-1cf5f435c997',
        'runId': 'b45fff8a-7def-43b8-805d-1cf5f435c997',
        'stepKey': None,
        'timestamp': '1758817804845',
      }),
      dict({
        'error': None,
        'eventType': 'ENGINE_EVENT',
        'level': 'INFO',
        'message': '[CloudK8sRunLauncher] Creating Kubernetes run worker job',
        'runId': 'b45fff8a-7def-43b8-805d-1cf5f435c997',
        'stepKey': None,
        'timestamp': '1758817804996',
      }),
      dict({
        'error': None,
        'eventType': 'ENGINE_EVENT',
        'level': 'INFO',
        'message': '[CloudK8sRunLauncher] Kubernetes run worker job created',
        'runId': 'b45fff8a-7def-43b8-805d-1cf5f435c997',
        'stepKey': None,
        'timestamp': '1758817805113',
      }),
      dict({
        'error': None,
        'eventType': 'ENGINE_EVENT',
        'level': 'INFO',
        'message': 'Starting run metrics thread with container_metrics_enabled=True and python_metrics_enabled=False',
        'runId': 'b45fff8a-7def-43b8-805d-1cf5f435c997',
        'stepKey': None,
        'timestamp': '1758817813386',
      }),
      dict({
        'error': None,
        'eventType': 'ENGINE_EVENT',
        'level': 'INFO',
        'message': 'Started process for run (pid: 1).',
        'runId': 'b45fff8a-7def-43b8-805d-1cf5f435c997',
        'stepKey': None,
        'timestamp': '1758817813451',
      }),
      dict({
        'error': None,
        'eventType': 'RUN_START',
        'level': 'DEBUG',
        'message': 'Started execution of run for "check_avg_orders_freshness_job".',
        'runId': 'b45fff8a-7def-43b8-805d-1cf5f435c997',
        'stepKey': None,
        'timestamp': '1758817827876',
      }),
      dict({
        'error': None,
        'eventType': 'ENGINE_EVENT',
        'level': 'DEBUG',
        'message': 'Executing steps using multiprocess executor: parent process (pid: 1)',
        'runId': 'b45fff8a-7def-43b8-805d-1cf5f435c997',
        'stepKey': None,
        'timestamp': '1758817827919',
      }),
      dict({
        'error': None,
        'eventType': 'STEP_WORKER_STARTING',
        'level': 'DEBUG',
        'message': 'Launching subprocess for "anomaly_detection_freshness_check_458e9b8f".',
        'runId': 'b45fff8a-7def-43b8-805d-1cf5f435c997',
        'stepKey': 'anomaly_detection_freshness_check_458e9b8f',
        'timestamp': '1758817828047',
      }),
      dict({
        'error': None,
        'eventType': 'STEP_WORKER_STARTED',
        'level': 'DEBUG',
        'message': 'Executing step "anomaly_detection_freshness_check_458e9b8f" in subprocess.',
        'runId': 'b45fff8a-7def-43b8-805d-1cf5f435c997',
        'stepKey': 'anomaly_detection_freshness_check_458e9b8f',
        'timestamp': '1758817829513',
      }),
      dict({
        'error': None,
        'eventType': 'RESOURCE_INIT_STARTED',
        'level': 'DEBUG',
        'message': 'Starting initialization of resources [io_manager].',
        'runId': 'b45fff8a-7def-43b8-805d-1cf5f435c997',
        'stepKey': 'anomaly_detection_freshness_check_458e9b8f',
        'timestamp': '1758817833373',
      }),
      dict({
        'error': None,
        'eventType': 'RESOURCE_INIT_SUCCESS',
        'level': 'DEBUG',
        'message': 'Finished initialization of resources [io_manager].',
        'runId': 'b45fff8a-7def-43b8-805d-1cf5f435c997',
        'stepKey': 'anomaly_detection_freshness_check_458e9b8f',
        'timestamp': '1758817833453',
      }),
      dict({
        'error': None,
        'eventType': 'LOGS_CAPTURED',
        'level': 'DEBUG',
        'message': 'Started capturing logs in process (pid: 19).',
        'runId': 'b45fff8a-7def-43b8-805d-1cf5f435c997',
        'stepKey': None,
        'timestamp': '1758817833507',
      }),
      dict({
        'error': None,
        'eventType': 'STEP_START',
        'level': 'DEBUG',
        'message': 'Started execution of step "anomaly_detection_freshness_check_458e9b8f".',
        'runId': 'b45fff8a-7def-43b8-805d-1cf5f435c997',
        'stepKey': 'anomaly_detection_freshness_check_458e9b8f',
        'timestamp': '1758817833533',
      }),
      dict({
        'error': None,
        'eventType': 'STEP_OUTPUT',
        'level': 'DEBUG',
        'message': 'Yielded output "MARKETING__avg_orders_anomaly_detection_freshness_check" of type "Any". (Type check passed).',
        'runId': 'b45fff8a-7def-43b8-805d-1cf5f435c997',
        'stepKey': 'anomaly_detection_freshness_check_458e9b8f',
        'timestamp': '1758817833714',
      }),
      dict({
        'error': None,
        'eventType': 'ASSET_CHECK_EVALUATION',
        'level': 'DEBUG',
        'message': "Asset check 'anomaly_detection_freshness_check' on 'MARKETING/avg_orders' did not pass. Description: 'Asset is overdue for an update. The last update was 2 days, 16 hours, 25 minutes, 24 seconds ago, which is past the allowed distance of 2 days, 5 minutes, 11 seconds ago.'",
        'runId': 'b45fff8a-7def-43b8-805d-1cf5f435c997',
        'stepKey': 'anomaly_detection_freshness_check_458e9b8f',
        'timestamp': '1758817833766',
      }),
      dict({
        'error': None,
        'eventType': 'STEP_SUCCESS',
        'level': 'DEBUG',
        'message': 'Finished execution of step "anomaly_detection_freshness_check_458e9b8f" in 246ms.',
        'runId': 'b45fff8a-7def-43b8-805d-1cf5f435c997',
        'stepKey': 'anomaly_detection_freshness_check_458e9b8f',
        'timestamp': '1758817833807',
      }),
    ]),
    'run_id': 'b45fff8a-7def-43b8-805d-1cf5f435c997',
  })
# ---
# name: TestDynamicCommandExecution.test_command_execution[log_success_logs_basic_text_text]
  '''
  Logs for run b45fff8a-7def-43b8-805d-1cf5f435c997:
  
  TIMESTAMP            LEVEL    STEP_KEY                  MESSAGE
  --------------------------------------------------------------------------------
  2025-09-25 16:30:01  DEBUG    anomaly_detection_freshn  check_avg_orders_freshness_job intends to execute asset check anomaly_detection_freshness_check on asset ["MARKETING", "avg_orders"]
  2025-09-25 16:30:01  INFO                               
  2025-09-25 16:30:04  INFO                               
  2025-09-25 16:30:04  INFO                               Sending a request to the agent to execute steps in the user cloud
  2025-09-25 16:30:04  INFO                               [DagsterCloudAgent] Agent d0a5374c is launching run b45fff8a-7def-43b8-805d-1cf5f435c997
  2025-09-25 16:30:04  INFO                               [CloudK8sRunLauncher] Creating Kubernetes run worker job
  2025-09-25 16:30:05  INFO                               [CloudK8sRunLauncher] Kubernetes run worker job created
  2025-09-25 16:30:13  INFO                               Starting run metrics thread with container_metrics_enabled=True and python_metrics_enabled=False
  2025-09-25 16:30:13  INFO                               Started process for run (pid: 1).
  2025-09-25 16:30:27  DEBUG                              Started execution of run for "check_avg_orders_freshness_job".
  2025-09-25 16:30:27  DEBUG                              Executing steps using multiprocess executor: parent process (pid: 1)
  2025-09-25 16:30:28  DEBUG    anomaly_detection_freshn  Launching subprocess for "anomaly_detection_freshness_check_458e9b8f".
  2025-09-25 16:30:29  DEBUG    anomaly_detection_freshn  Executing step "anomaly_detection_freshness_check_458e9b8f" in subprocess.
  2025-09-25 16:30:33  DEBUG    anomaly_detection_freshn  Starting initialization of resources [io_manager].
  2025-09-25 16:30:33  DEBUG    anomaly_detection_freshn  Finished initialization of resources [io_manager].
  2025-09-25 16:30:33  DEBUG                              Started capturing logs in process (pid: 19).
  2025-09-25 16:30:33  DEBUG    anomaly_detection_freshn  Started execution of step "anomaly_detection_freshness_check_458e9b8f".
  2025-09-25 16:30:33  DEBUG    anomaly_detection_freshn  Yielded output "MARKETING__avg_orders_anomaly_detection_freshness_check" of type "Any". (Type check passed).
  2025-09-25 16:30:33  DEBUG    anomaly_detection_freshn  Asset check 'anomaly_detection_freshness_check' on 'MARKETING/avg_orders' did not pass. Description: 'Asset is overdue for an update. The last update was 2 days, 16 hours, 25 minutes, 24 seconds ago, which is past the allowed distance of 2 days, 5 minutes, 11 seconds ago.'
  2025-09-25 16:30:33  DEBUG    anomaly_detection_freshn  Finished execution of step "anomaly_detection_freshness_check_458e9b8f" in 246ms.
  
  Total log entries: 20
  Note: More logs available (use --limit to increase or --cursor to paginate)
  
  '''
# ---
# name: TestDynamicCommandExecution.test_command_execution[log_success_logs_debug_level_json_json]
  dict({
    'count': 0,
    'cursor': 'eyJ0eXBlIjogIlNUT1JBR0VfSUQiLCAidmFsdWUiOiA0ODgwODUzNTQzOH0=',
    'hasMore': True,
    'logs': list([
    ]),
    'run_id': '85156191-170d-4229-8eba-14450b6ca9e3',
  })
# ---
# name: TestDynamicCommandExecution.test_command_execution[log_success_logs_debug_level_text_text]
  '''
  No logs found for run 85156191-170d-4229-8eba-14450b6ca9e3
  
  '''
# ---
# name: TestDynamicCommandExecution.test_command_execution[log_success_logs_error_level_only_json_json]
  dict({
    'count': 0,
    'cursor': 'eyJ0eXBlIjogIlNUT1JBR0VfSUQiLCAidmFsdWUiOiA0ODgwODU1MzE0M30=',
    'hasMore': False,
    'logs': list([
    ]),
    'run_id': '85156191-170d-4229-8eba-14450b6ca9e3',
  })
# ---
# name: TestDynamicCommandExecution.test_command_execution[log_success_logs_error_level_only_text_text]
  '''
  No logs found for run 85156191-170d-4229-8eba-14450b6ca9e3
  
  '''
# ---
# name: TestDynamicCommandExecution.test_command_execution[log_success_logs_small_limit_json_json]
  dict({
    'count': 5,
    'cursor': 'eyJ0eXBlIjogIlNUT1JBR0VfSUQiLCAidmFsdWUiOiA0ODgwODUyODM1MX0=',
    'hasMore': True,
    'logs': list([
      dict({
        'error': None,
        'eventType': 'ASSET_MATERIALIZATION_PLANNED',
        'level': 'DEBUG',
        'message': 'run_etl_pipeline intends to materialize asset ["enriched_data"]',
        'runId': '85156191-170d-4229-8eba-14450b6ca9e3',
        'stepKey': 'enriched_data.concat_chunk_list',
        'timestamp': '1758808865047',
      }),
      dict({
        'error': None,
        'eventType': 'ENGINE_EVENT',
        'level': 'INFO',
        'message': 'Launched as an automatic retry',
        'runId': '85156191-170d-4229-8eba-14450b6ca9e3',
        'stepKey': None,
        'timestamp': '1758808865075',
      }),
      dict({
        'error': None,
        'eventType': 'RUN_ENQUEUED',
        'level': 'INFO',
        'message': '',
        'runId': '85156191-170d-4229-8eba-14450b6ca9e3',
        'stepKey': None,
        'timestamp': '1758808865098',
      }),
      dict({
        'error': None,
        'eventType': 'RUN_STARTING',
        'level': 'INFO',
        'message': '',
        'runId': '85156191-170d-4229-8eba-14450b6ca9e3',
        'stepKey': None,
        'timestamp': '1758808867161',
      }),
      dict({
        'error': None,
        'eventType': 'ENGINE_EVENT',
        'level': 'INFO',
        'message': 'Sending a request to the agent to execute steps in the user cloud',
        'runId': '85156191-170d-4229-8eba-14450b6ca9e3',
        'stepKey': None,
        'timestamp': '1758808867189',
      }),
    ]),
    'run_id': '85156191-170d-4229-8eba-14450b6ca9e3',
  })
# ---
# name: TestDynamicCommandExecution.test_command_execution[log_success_logs_small_limit_text_text]
  '''
  Logs for run 85156191-170d-4229-8eba-14450b6ca9e3:
  
  TIMESTAMP            LEVEL    STEP_KEY                  MESSAGE
  --------------------------------------------------------------------------------
  2025-09-25 14:01:05  DEBUG    enriched_data.concat_chu  run_etl_pipeline intends to materialize asset ["enriched_data"]
  2025-09-25 14:01:05  INFO                               Launched as an automatic retry
  2025-09-25 14:01:05  INFO                               
  2025-09-25 14:01:07  INFO                               
  2025-09-25 14:01:07  INFO                               Sending a request to the agent to execute steps in the user cloud
  
  Total log entries: 5
  Note: More logs available (use --limit to increase or --cursor to paginate)
  
  '''
# ---
# name: TestDynamicCommandExecution.test_command_execution[log_success_logs_step_filter_json_json]
  dict({
    'count': 3,
    'cursor': 'eyJ0eXBlIjogIlNUT1JBR0VfSUQiLCAidmFsdWUiOiA0ODgwODU0MDMyNn0=',
    'hasMore': True,
    'logs': list([
      dict({
        'error': None,
        'eventType': 'STEP_WORKER_STARTING',
        'level': 'DEBUG',
        'message': 'Launching subprocess for "enriched_data.process_chunk[11]".',
        'runId': '85156191-170d-4229-8eba-14450b6ca9e3',
        'stepKey': 'enriched_data.process_chunk[11]',
        'timestamp': '1758808871633',
      }),
      dict({
        'error': None,
        'eventType': 'STEP_WORKER_STARTED',
        'level': 'DEBUG',
        'message': 'Executing step "enriched_data.process_chunk[11]" in subprocess.',
        'runId': '85156191-170d-4229-8eba-14450b6ca9e3',
        'stepKey': 'enriched_data.process_chunk[11]',
        'timestamp': '1758808872870',
      }),
      dict({
        'error': None,
        'eventType': 'RESOURCE_INIT_STARTED',
        'level': 'DEBUG',
        'message': 'Starting initialization of resources [api, io_manager].',
        'runId': '85156191-170d-4229-8eba-14450b6ca9e3',
        'stepKey': 'enriched_data.process_chunk[11]',
        'timestamp': '1758808873361',
      }),
    ]),
    'run_id': '85156191-170d-4229-8eba-14450b6ca9e3',
  })
# ---
# name: TestDynamicCommandExecution.test_command_execution[log_success_logs_step_filter_text_text]
  '''
  Logs for run 85156191-170d-4229-8eba-14450b6ca9e3:
  
  TIMESTAMP            LEVEL    STEP_KEY                  MESSAGE
  --------------------------------------------------------------------------------
  2025-09-25 14:01:11  DEBUG    enriched_data.process_ch  Launching subprocess for "enriched_data.process_chunk[11]".
  2025-09-25 14:01:12  DEBUG    enriched_data.process_ch  Executing step "enriched_data.process_chunk[11]" in subprocess.
  2025-09-25 14:01:13  DEBUG    enriched_data.process_ch  Starting initialization of resources [api, io_manager].
  
  Total log entries: 3
  Note: More logs available (use --limit to increase or --cursor to paginate)
  
  '''
# ---
# name: TestDynamicCommandExecution.test_command_execution[log_success_logs_with_errors_json_json]
  dict({
    'count': 41,
    'cursor': 'eyJ0eXBlIjogIlNUT1JBR0VfSUQiLCAidmFsdWUiOiA0ODgwODU1MzE0M30=',
    'hasMore': False,
    'logs': list([
      dict({
        'error': None,
        'eventType': 'ASSET_MATERIALIZATION_PLANNED',
        'level': 'DEBUG',
        'message': 'run_etl_pipeline intends to materialize asset ["enriched_data"]',
        'runId': '85156191-170d-4229-8eba-14450b6ca9e3',
        'stepKey': 'enriched_data.concat_chunk_list',
        'timestamp': '1758808865047',
      }),
      dict({
        'error': None,
        'eventType': 'ENGINE_EVENT',
        'level': 'INFO',
        'message': 'Launched as an automatic retry',
        'runId': '85156191-170d-4229-8eba-14450b6ca9e3',
        'stepKey': None,
        'timestamp': '1758808865075',
      }),
      dict({
        'error': None,
        'eventType': 'RUN_ENQUEUED',
        'level': 'INFO',
        'message': '',
        'runId': '85156191-170d-4229-8eba-14450b6ca9e3',
        'stepKey': None,
        'timestamp': '1758808865098',
      }),
      dict({
        'error': None,
        'eventType': 'RUN_STARTING',
        'level': 'INFO',
        'message': '',
        'runId': '85156191-170d-4229-8eba-14450b6ca9e3',
        'stepKey': None,
        'timestamp': '1758808867161',
      }),
      dict({
        'error': None,
        'eventType': 'ENGINE_EVENT',
        'level': 'INFO',
        'message': 'Sending a request to the agent to execute steps in the user cloud',
        'runId': '85156191-170d-4229-8eba-14450b6ca9e3',
        'stepKey': None,
        'timestamp': '1758808867189',
      }),
      dict({
        'error': None,
        'eventType': 'ENGINE_EVENT',
        'level': 'INFO',
        'message': '[DagsterCloudAgent] Agent 3a659214 is launching run 85156191-170d-4229-8eba-14450b6ca9e3',
        'runId': '85156191-170d-4229-8eba-14450b6ca9e3',
        'stepKey': None,
        'timestamp': '1758808867555',
      }),
      dict({
        'error': None,
        'eventType': 'ENGINE_EVENT',
        'level': 'INFO',
        'message': '[CloudK8sRunLauncher] Creating Kubernetes run worker job',
        'runId': '85156191-170d-4229-8eba-14450b6ca9e3',
        'stepKey': None,
        'timestamp': '1758808867738',
      }),
      dict({
        'error': None,
        'eventType': 'ENGINE_EVENT',
        'level': 'INFO',
        'message': '[CloudK8sRunLauncher] Kubernetes run worker job created',
        'runId': '85156191-170d-4229-8eba-14450b6ca9e3',
        'stepKey': None,
        'timestamp': '1758808867910',
      }),
      dict({
        'error': None,
        'eventType': 'ENGINE_EVENT',
        'level': 'INFO',
        'message': 'Starting run metrics thread with container_metrics_enabled=True and python_metrics_enabled=False',
        'runId': '85156191-170d-4229-8eba-14450b6ca9e3',
        'stepKey': None,
        'timestamp': '1758808870636',
      }),
      dict({
        'error': None,
        'eventType': 'ENGINE_EVENT',
        'level': 'INFO',
        'message': 'Started process for run (pid: 1).',
        'runId': '85156191-170d-4229-8eba-14450b6ca9e3',
        'stepKey': None,
        'timestamp': '1758808870686',
      }),
      dict({
        'error': None,
        'eventType': 'RUN_START',
        'level': 'DEBUG',
        'message': 'Started execution of run for "run_etl_pipeline".',
        'runId': '85156191-170d-4229-8eba-14450b6ca9e3',
        'stepKey': None,
        'timestamp': '1758808871488',
      }),
      dict({
        'error': None,
        'eventType': 'ENGINE_EVENT',
        'level': 'DEBUG',
        'message': 'Executing steps using multiprocess executor: parent process (pid: 1)',
        'runId': '85156191-170d-4229-8eba-14450b6ca9e3',
        'stepKey': None,
        'timestamp': '1758808871524',
      }),
      dict({
        'error': None,
        'eventType': 'STEP_WORKER_STARTING',
        'level': 'DEBUG',
        'message': 'Launching subprocess for "enriched_data.process_chunk[11]".',
        'runId': '85156191-170d-4229-8eba-14450b6ca9e3',
        'stepKey': 'enriched_data.process_chunk[11]',
        'timestamp': '1758808871633',
      }),
      dict({
        'error': None,
        'eventType': 'STEP_WORKER_STARTED',
        'level': 'DEBUG',
        'message': 'Executing step "enriched_data.process_chunk[11]" in subprocess.',
        'runId': '85156191-170d-4229-8eba-14450b6ca9e3',
        'stepKey': 'enriched_data.process_chunk[11]',
        'timestamp': '1758808872870',
      }),
      dict({
        'error': None,
        'eventType': 'RESOURCE_INIT_STARTED',
        'level': 'DEBUG',
        'message': 'Starting initialization of resources [api, io_manager].',
        'runId': '85156191-170d-4229-8eba-14450b6ca9e3',
        'stepKey': 'enriched_data.process_chunk[11]',
        'timestamp': '1758808873361',
      }),
      dict({
        'error': None,
        'eventType': 'RESOURCE_INIT_SUCCESS',
        'level': 'DEBUG',
        'message': 'Finished initialization of resources [api, io_manager].',
        'runId': '85156191-170d-4229-8eba-14450b6ca9e3',
        'stepKey': 'enriched_data.process_chunk[11]',
        'timestamp': '1758808873521',
      }),
      dict({
        'error': None,
        'eventType': 'LOGS_CAPTURED',
        'level': 'DEBUG',
        'message': 'Started capturing logs in process (pid: 16).',
        'runId': '85156191-170d-4229-8eba-14450b6ca9e3',
        'stepKey': None,
        'timestamp': '1758808873570',
      }),
      dict({
        'error': None,
        'eventType': 'STEP_START',
        'level': 'DEBUG',
        'message': 'Started execution of step "enriched_data.process_chunk[11]".',
        'runId': '85156191-170d-4229-8eba-14450b6ca9e3',
        'stepKey': 'enriched_data.process_chunk[11]',
        'timestamp': '1758808873607',
      }),
      dict({
        'error': None,
        'eventType': None,
        'level': 'DEBUG',
        'message': 'Loading file from: /opt/dagster/dagster_home/storage/385defa2-7ac6-4d18-8a88-585a42393d9c/enriched_data.split_rows/result/11 using PickledObjectFilesystemIOManager...',
        'runId': '85156191-170d-4229-8eba-14450b6ca9e3',
        'stepKey': 'enriched_data.process_chunk[11]',
        'timestamp': '1758808873644',
      }),
      dict({
        'error': None,
        'eventType': 'STEP_UP_FOR_RETRY',
        'level': 'DEBUG',
        'message': 'Execution of step "enriched_data.process_chunk[11]" failed and has requested a retry.',
        'runId': '85156191-170d-4229-8eba-14450b6ca9e3',
        'stepKey': 'enriched_data.process_chunk[11]',
        'timestamp': '1758808873697',
      }),
      dict({
        'error': None,
        'eventType': 'STEP_WORKER_STARTING',
        'level': 'DEBUG',
        'message': 'Launching subprocess for "enriched_data.process_chunk[11]".',
        'runId': '85156191-170d-4229-8eba-14450b6ca9e3',
        'stepKey': 'enriched_data.process_chunk[11]',
        'timestamp': '1758808873742',
      }),
      dict({
        'error': None,
        'eventType': 'STEP_WORKER_STARTED',
        'level': 'DEBUG',
        'message': 'Executing step "enriched_data.process_chunk[11]" in subprocess.',
        'runId': '85156191-170d-4229-8eba-14450b6ca9e3',
        'stepKey': 'enriched_data.process_chunk[11]',
        'timestamp': '1758808874924',
      }),
      dict({
        'error': None,
        'eventType': 'RESOURCE_INIT_STARTED',
        'level': 'DEBUG',
        'message': 'Starting initialization of resources [api, io_manager].',
        'runId': '85156191-170d-4229-8eba-14450b6ca9e3',
        'stepKey': 'enriched_data.process_chunk[11]',
        'timestamp': '1758808875309',
      }),
      dict({
        'error': None,
        'eventType': 'RESOURCE_INIT_SUCCESS',
        'level': 'DEBUG',
        'message': 'Finished initialization of resources [api, io_manager].',
        'runId': '85156191-170d-4229-8eba-14450b6ca9e3',
        'stepKey': 'enriched_data.process_chunk[11]',
        'timestamp': '1758808875444',
      }),
      dict({
        'error': None,
        'eventType': 'LOGS_CAPTURED',
        'level': 'DEBUG',
        'message': 'Started capturing logs in process (pid: 26).',
        'runId': '85156191-170d-4229-8eba-14450b6ca9e3',
        'stepKey': None,
        'timestamp': '1758808875526',
      }),
      dict({
        'error': None,
        'eventType': 'STEP_RESTARTED',
        'level': 'DEBUG',
        'message': 'Started re-execution (attempt # 2) of step "enriched_data.process_chunk[11]".',
        'runId': '85156191-170d-4229-8eba-14450b6ca9e3',
        'stepKey': 'enriched_data.process_chunk[11]',
        'timestamp': '1758808875655',
      }),
      dict({
        'error': None,
        'eventType': None,
        'level': 'DEBUG',
        'message': 'Loading file from: /opt/dagster/dagster_home/storage/385defa2-7ac6-4d18-8a88-585a42393d9c/enriched_data.split_rows/result/11 using PickledObjectFilesystemIOManager...',
        'runId': '85156191-170d-4229-8eba-14450b6ca9e3',
        'stepKey': 'enriched_data.process_chunk[11]',
        'timestamp': '1758808875737',
      }),
      dict({
        'error': None,
        'eventType': 'STEP_UP_FOR_RETRY',
        'level': 'DEBUG',
        'message': 'Execution of step "enriched_data.process_chunk[11]" failed and has requested a retry.',
        'runId': '85156191-170d-4229-8eba-14450b6ca9e3',
        'stepKey': 'enriched_data.process_chunk[11]',
        'timestamp': '1758808875801',
      }),
      dict({
        'error': None,
        'eventType': 'STEP_WORKER_STARTING',
        'level': 'DEBUG',
        'message': 'Launching subprocess for "enriched_data.process_chunk[11]".',
        'runId': '85156191-170d-4229-8eba-14450b6ca9e3',
        'stepKey': 'enriched_data.process_chunk[11]',
        'timestamp': '1758808875835',
      }),
      dict({
        'error': None,
        'eventType': 'STEP_WORKER_STARTED',
        'level': 'DEBUG',
        'message': 'Executing step "enriched_data.process_chunk[11]" in subprocess.',
        'runId': '85156191-170d-4229-8eba-14450b6ca9e3',
        'stepKey': 'enriched_data.process_chunk[11]',
        'timestamp': '1758808876893',
      }),
      dict({
        'error': None,
        'eventType': 'RESOURCE_INIT_STARTED',
        'level': 'DEBUG',
        'message': 'Starting initialization of resources [api, io_manager].',
        'runId': '85156191-170d-4229-8eba-14450b6ca9e3',
        'stepKey': 'enriched_data.process_chunk[11]',
        'timestamp': '1758808877224',
      }),
      dict({
        'error': None,
        'eventType': 'RESOURCE_INIT_SUCCESS',
        'level': 'DEBUG',
        'message': 'Finished initialization of resources [api, io_manager].',
        'runId': '85156191-170d-4229-8eba-14450b6ca9e3',
        'stepKey': 'enriched_data.process_chunk[11]',
        'timestamp': '1758808877380',
      }),
      dict({
        'error': None,
        'eventType': 'LOGS_CAPTURED',
        'level': 'DEBUG',
        'message': 'Started capturing logs in process (pid: 36).',
        'runId': '85156191-170d-4229-8eba-14450b6ca9e3',
        'stepKey': None,
        'timestamp': '1758808877439',
      }),
      dict({
        'error': None,
        'eventType': 'STEP_RESTARTED',
        'level': 'DEBUG',
        'message': 'Started re-execution (attempt # 3) of step "enriched_data.process_chunk[11]".',
        'runId': '85156191-170d-4229-8eba-14450b6ca9e3',
        'stepKey': 'enriched_data.process_chunk[11]',
        'timestamp': '1758808877601',
      }),
      dict({
        'error': None,
        'eventType': None,
        'level': 'DEBUG',
        'message': 'Loading file from: /opt/dagster/dagster_home/storage/385defa2-7ac6-4d18-8a88-585a42393d9c/enriched_data.split_rows/result/11 using PickledObjectFilesystemIOManager...',
        'runId': '85156191-170d-4229-8eba-14450b6ca9e3',
        'stepKey': 'enriched_data.process_chunk[11]',
        'timestamp': '1758808877680',
      }),
      dict({
        'error': dict({
          'cause': dict({
            'cause': None,
            'className': 'FileNotFoundError',
            'message': '''
              FileNotFoundError: [Errno 2] No such file or directory: '/opt/dagster/dagster_home/storage/385defa2-7ac6-4d18-8a88-585a42393d9c/enriched_data.split_rows/result/11'
  
            ''',
            'stack': list([
              '''
                  File "/usr/local/lib/python3.12/site-packages/dagster/_core/execution/plan/utils.py", line 57, in op_execution_error_boundary
                    yield
  
              ''',
              '''
                  File "/usr/local/lib/python3.12/site-packages/dagster/_core/execution/plan/inputs.py", line 619, in _load_input_with_input_manager
                    value = input_manager.load_input(context)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  
              ''',
              '''
                  File "/usr/local/lib/python3.12/site-packages/dagster/_core/storage/upath_io_manager.py", line 406, in load_input
                    return self._load_single_input(path, context)
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  
              ''',
              '''
                  File "/usr/local/lib/python3.12/site-packages/dagster/_core/storage/upath_io_manager.py", line 273, in _load_single_input
                    obj = self.load_from_path(context=context, path=path)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  
              ''',
              '''
                  File "/usr/local/lib/python3.12/site-packages/dagster/_core/storage/fs_io_manager.py", line 284, in load_from_path
                    with path.open("rb") as file:
                         ^^^^^^^^^^^^^^^
  
              ''',
              '''
                  File "/usr/local/lib/python3.12/site-packages/upath/implementations/local.py", line 134, in open
                    return PosixPath.open(self, mode, buffering, encoding, errors, newline)
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  
              ''',
              '''
                  File "/usr/local/lib/python3.12/pathlib.py", line 1013, in open
                    return io.open(self, mode, buffering, encoding, errors, newline)
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  
              ''',
            ]),
            'stackTrace': '''
                File "/usr/local/lib/python3.12/site-packages/dagster/_core/execution/plan/utils.py", line 57, in op_execution_error_boundary
                  yield
              
                File "/usr/local/lib/python3.12/site-packages/dagster/_core/execution/plan/inputs.py", line 619, in _load_input_with_input_manager
                  value = input_manager.load_input(context)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
              
                File "/usr/local/lib/python3.12/site-packages/dagster/_core/storage/upath_io_manager.py", line 406, in load_input
                  return self._load_single_input(path, context)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
              
                File "/usr/local/lib/python3.12/site-packages/dagster/_core/storage/upath_io_manager.py", line 273, in _load_single_input
                  obj = self.load_from_path(context=context, path=path)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
              
                File "/usr/local/lib/python3.12/site-packages/dagster/_core/storage/fs_io_manager.py", line 284, in load_from_path
                  with path.open("rb") as file:
                       ^^^^^^^^^^^^^^^
              
                File "/usr/local/lib/python3.12/site-packages/upath/implementations/local.py", line 134, in open
                  return PosixPath.open(self, mode, buffering, encoding, errors, newline)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
              
                File "/usr/local/lib/python3.12/pathlib.py", line 1013, in open
                  return io.open(self, mode, buffering, encoding, errors, newline)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  
            ''',
          }),
          'className': 'RetryRequestedFromPolicy',
          'message': '''
            Exceeded max_retries of 2
  
          ''',
          'stack': list([
            '''
                File "/usr/local/lib/python3.12/site-packages/dagster/_core/execution/plan/execute_plan.py", line 246, in dagster_event_sequence_for_step
                  yield from check.generator(step_events)
  
            ''',
            '''
                File "/usr/local/lib/python3.12/site-packages/dagster/_core/execution/plan/execute_step.py", line 468, in core_dagster_event_sequence_for_step
                  for event_or_input_value in step_input.source.load_input_object(
                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  
            ''',
            '''
                File "/usr/local/lib/python3.12/site-packages/dagster/_core/execution/plan/inputs.py", line 364, in load_input_object
                  yield from _load_input_with_input_manager(input_manager, load_input_context)
  
            ''',
            '''
                File "/usr/local/lib/python3.12/site-packages/dagster/_core/execution/plan/inputs.py", line 612, in _load_input_with_input_manager
                  with op_execution_error_boundary(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  
            ''',
            '''
                File "/usr/local/lib/python3.12/contextlib.py", line 158, in __exit__
                  self.gen.throw(value)
  
            ''',
            '''
                File "/usr/local/lib/python3.12/site-packages/dagster/_core/execution/plan/utils.py", line 75, in op_execution_error_boundary
                  raise RetryRequestedFromPolicy(
  
            ''',
          ]),
          'stackTrace': '''
              File "/usr/local/lib/python3.12/site-packages/dagster/_core/execution/plan/execute_plan.py", line 246, in dagster_event_sequence_for_step
                yield from check.generator(step_events)
            
              File "/usr/local/lib/python3.12/site-packages/dagster/_core/execution/plan/execute_step.py", line 468, in core_dagster_event_sequence_for_step
                for event_or_input_value in step_input.source.load_input_object(
                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
            
              File "/usr/local/lib/python3.12/site-packages/dagster/_core/execution/plan/inputs.py", line 364, in load_input_object
                yield from _load_input_with_input_manager(input_manager, load_input_context)
            
              File "/usr/local/lib/python3.12/site-packages/dagster/_core/execution/plan/inputs.py", line 612, in _load_input_with_input_manager
                with op_execution_error_boundary(
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
            
              File "/usr/local/lib/python3.12/contextlib.py", line 158, in __exit__
                self.gen.throw(value)
            
              File "/usr/local/lib/python3.12/site-packages/dagster/_core/execution/plan/utils.py", line 75, in op_execution_error_boundary
                raise RetryRequestedFromPolicy(
  
          ''',
        }),
        'eventType': 'STEP_FAILURE',
        'level': 'ERROR',
        'message': 'Execution of step "enriched_data.process_chunk[11]" failed.',
        'runId': '85156191-170d-4229-8eba-14450b6ca9e3',
        'stepKey': 'enriched_data.process_chunk[11]',
        'timestamp': '1758808877726',
      }),
      dict({
        'error': None,
        'eventType': None,
        'level': 'ERROR',
        'message': "Dependencies for step enriched_data.concat_chunk_list failed: ['enriched_data.process_chunk[11]']. Not executing.",
        'runId': '85156191-170d-4229-8eba-14450b6ca9e3',
        'stepKey': 'enriched_data.concat_chunk_list',
        'timestamp': '1758808877786',
      }),
      dict({
        'error': None,
        'eventType': 'ENGINE_EVENT',
        'level': 'DEBUG',
        'message': 'Multiprocess executor: parent process exiting after 6.79s (pid: 1)',
        'runId': '85156191-170d-4229-8eba-14450b6ca9e3',
        'stepKey': None,
        'timestamp': '1758808878387',
      }),
      dict({
        'error': None,
        'eventType': 'RUN_FAILURE',
        'level': 'ERROR',
        'message': 'Execution of run for "run_etl_pipeline" failed. Steps failed: [\'enriched_data.process_chunk[11]\'].',
        'runId': '85156191-170d-4229-8eba-14450b6ca9e3',
        'stepKey': None,
        'timestamp': '1758808878519',
      }),
      dict({
        'error': None,
        'eventType': 'ENGINE_EVENT',
        'level': 'INFO',
        'message': 'Process for run exited (pid: 1).',
        'runId': '85156191-170d-4229-8eba-14450b6ca9e3',
        'stepKey': None,
        'timestamp': '1758808878602',
      }),
      dict({
        'error': None,
        'eventType': 'ASSET_FAILED_TO_MATERIALIZE',
        'level': 'INFO',
        'message': 'Asset ["enriched_data"] failed to materialize',
        'runId': '85156191-170d-4229-8eba-14450b6ca9e3',
        'stepKey': 'enriched_data.concat_chunk_list',
        'timestamp': '1758808880071',
      }),
    ]),
    'run_id': '85156191-170d-4229-8eba-14450b6ca9e3',
  })
# ---
# name: TestDynamicCommandExecution.test_command_execution[log_success_logs_with_errors_text_text]
  '''
  Logs for run 85156191-170d-4229-8eba-14450b6ca9e3:
  
  TIMESTAMP            LEVEL    STEP_KEY                  MESSAGE
  --------------------------------------------------------------------------------
  2025-09-25 14:01:05  DEBUG    enriched_data.concat_chu  run_etl_pipeline intends to materialize asset ["enriched_data"]
  2025-09-25 14:01:05  INFO                               Launched as an automatic retry
  2025-09-25 14:01:05  INFO                               
  2025-09-25 14:01:07  INFO                               
  2025-09-25 14:01:07  INFO                               Sending a request to the agent to execute steps in the user cloud
  2025-09-25 14:01:07  INFO                               [DagsterCloudAgent] Agent 3a659214 is launching run 85156191-170d-4229-8eba-14450b6ca9e3
  2025-09-25 14:01:07  INFO                               [CloudK8sRunLauncher] Creating Kubernetes run worker job
  2025-09-25 14:01:07  INFO                               [CloudK8sRunLauncher] Kubernetes run worker job created
  2025-09-25 14:01:10  INFO                               Starting run metrics thread with container_metrics_enabled=True and python_metrics_enabled=False
  2025-09-25 14:01:10  INFO                               Started process for run (pid: 1).
  2025-09-25 14:01:11  DEBUG                              Started execution of run for "run_etl_pipeline".
  2025-09-25 14:01:11  DEBUG                              Executing steps using multiprocess executor: parent process (pid: 1)
  2025-09-25 14:01:11  DEBUG    enriched_data.process_ch  Launching subprocess for "enriched_data.process_chunk[11]".
  2025-09-25 14:01:12  DEBUG    enriched_data.process_ch  Executing step "enriched_data.process_chunk[11]" in subprocess.
  2025-09-25 14:01:13  DEBUG    enriched_data.process_ch  Starting initialization of resources [api, io_manager].
  2025-09-25 14:01:13  DEBUG    enriched_data.process_ch  Finished initialization of resources [api, io_manager].
  2025-09-25 14:01:13  DEBUG                              Started capturing logs in process (pid: 16).
  2025-09-25 14:01:13  DEBUG    enriched_data.process_ch  Started execution of step "enriched_data.process_chunk[11]".
  2025-09-25 14:01:13  DEBUG    enriched_data.process_ch  Loading file from: /opt/dagster/dagster_home/storage/385defa2-7ac6-4d18-8a88-585a42393d9c/enriched_data.split_rows/result/11 using PickledObjectFilesystemIOManager...
  2025-09-25 14:01:13  DEBUG    enriched_data.process_ch  Execution of step "enriched_data.process_chunk[11]" failed and has requested a retry.
  2025-09-25 14:01:13  DEBUG    enriched_data.process_ch  Launching subprocess for "enriched_data.process_chunk[11]".
  2025-09-25 14:01:14  DEBUG    enriched_data.process_ch  Executing step "enriched_data.process_chunk[11]" in subprocess.
  2025-09-25 14:01:15  DEBUG    enriched_data.process_ch  Starting initialization of resources [api, io_manager].
  2025-09-25 14:01:15  DEBUG    enriched_data.process_ch  Finished initialization of resources [api, io_manager].
  2025-09-25 14:01:15  DEBUG                              Started capturing logs in process (pid: 26).
  2025-09-25 14:01:15  DEBUG    enriched_data.process_ch  Started re-execution (attempt # 2) of step "enriched_data.process_chunk[11]".
  2025-09-25 14:01:15  DEBUG    enriched_data.process_ch  Loading file from: /opt/dagster/dagster_home/storage/385defa2-7ac6-4d18-8a88-585a42393d9c/enriched_data.split_rows/result/11 using PickledObjectFilesystemIOManager...
  2025-09-25 14:01:15  DEBUG    enriched_data.process_ch  Execution of step "enriched_data.process_chunk[11]" failed and has requested a retry.
  2025-09-25 14:01:15  DEBUG    enriched_data.process_ch  Launching subprocess for "enriched_data.process_chunk[11]".
  2025-09-25 14:01:16  DEBUG    enriched_data.process_ch  Executing step "enriched_data.process_chunk[11]" in subprocess.
  2025-09-25 14:01:17  DEBUG    enriched_data.process_ch  Starting initialization of resources [api, io_manager].
  2025-09-25 14:01:17  DEBUG    enriched_data.process_ch  Finished initialization of resources [api, io_manager].
  2025-09-25 14:01:17  DEBUG                              Started capturing logs in process (pid: 36).
  2025-09-25 14:01:17  DEBUG    enriched_data.process_ch  Started re-execution (attempt # 3) of step "enriched_data.process_chunk[11]".
  2025-09-25 14:01:17  DEBUG    enriched_data.process_ch  Loading file from: /opt/dagster/dagster_home/storage/385defa2-7ac6-4d18-8a88-585a42393d9c/enriched_data.split_rows/result/11 using PickledObjectFilesystemIOManager...
  2025-09-25 14:01:17  ERROR    enriched_data.process_ch  Execution of step "enriched_data.process_chunk[11]" failed.
  
  Stack Trace:
      File "/usr/local/lib/python3.12/site-packages/dagster/_core/execution/plan/execute_plan.py", line 246, in dagster_event_sequence_for_step
        yield from check.generator(step_events)
      File "/usr/local/lib/python3.12/site-packages/dagster/_core/execution/plan/execute_step.py", line 468, in core_dagster_event_sequence_for_step
        for event_or_input_value in step_input.source.load_input_object(
                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
      File "/usr/local/lib/python3.12/site-packages/dagster/_core/execution/plan/inputs.py", line 364, in load_input_object
        yield from _load_input_with_input_manager(input_manager, load_input_context)
      File "/usr/local/lib/python3.12/site-packages/dagster/_core/execution/plan/inputs.py", line 612, in _load_input_with_input_manager
        with op_execution_error_boundary(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
      File "/usr/local/lib/python3.12/contextlib.py", line 158, in __exit__
        self.gen.throw(value)
      File "/usr/local/lib/python3.12/site-packages/dagster/_core/execution/plan/utils.py", line 75, in op_execution_error_boundary
        raise RetryRequestedFromPolicy(
  
  2025-09-25 14:01:17  ERROR    enriched_data.concat_chu  Dependencies for step enriched_data.concat_chunk_list failed: ['enriched_data.process_chunk[11]']. Not executing.
  2025-09-25 14:01:18  DEBUG                              Multiprocess executor: parent process exiting after 6.79s (pid: 1)
  2025-09-25 14:01:18  ERROR                              Execution of run for "run_etl_pipeline" failed. Steps failed: ['enriched_data.process_chunk[11]'].
  2025-09-25 14:01:18  INFO                               Process for run exited (pid: 1).
  2025-09-25 14:01:20  INFO     enriched_data.concat_chu  Asset ["enriched_data"] failed to materialize
  
  Total log entries: 41
  
  '''
# ---
# name: TestDynamicCommandExecution.test_command_execution[run_error_invalid_run_id_json]
  dict({
    'error': "Run not found: ''",
  })
# ---
# name: TestDynamicCommandExecution.test_command_execution[run_error_run_not_found_text]
  '''
  Error querying Dagster Plus API: Run not found: nonexistent-run-id
  
  '''
# ---
# name: TestDynamicCommandExecution.test_command_execution[run_success_canceled_run_json]
  dict({
    'created_at': 1751036896.306532,
    'ended_at': 1751036909.142719,
    'id': 'e0248005-fc38-4e55-a9fb-472c8694303b',
    'job_name': 'run_etl_pipeline',
    'started_at': 1751036903.35325,
    'status': 'CANCELED',
  })
# ---
# name: TestDynamicCommandExecution.test_command_execution[run_success_failed_run_json]
  dict({
    'created_at': 1757703542.251137,
    'ended_at': 1757703554.673739,
    'id': '4db16a22-4124-4d43-bd4c-ea0aeaa78172',
    'job_name': 'snowflake_insights_import',
    'started_at': 1757703550.780887,
    'status': 'FAILURE',
  })
# ---
# name: TestDynamicCommandExecution.test_command_execution[run_success_single_run_json]
  dict({
    'created_at': 1757703683.741368,
    'ended_at': 1757703732.670455,
    'id': '2e562bd3-7e72-4448-9f05-32bad2ea8046',
    'job_name': '__ASSET_JOB',
    'started_at': 1757703690.474944,
    'status': 'SUCCESS',
  })
# ---
# name: TestDynamicCommandExecution.test_command_execution[run_success_single_run_text_text]
  '''
  Run ID: 2e562bd3-7e72-4448-9f05-32bad2ea8046
  Status: SUCCESS
  Created: 1757703683.741368
  Started: 1757703690.474944
  Ended: 1757703732.670455
  Pipeline: __ASSET_JOB
  
  '''
# ---
# name: TestDynamicCommandExecution.test_command_execution[secret_success_multiple_secrets_json]
  dict({
    'items': list([
      dict({
        'all_branch_deployments_scope': True,
        'can_edit_secret': True,
        'can_view_secret_value': True,
        'full_deployment_scope': True,
        'id': '9138919ca1b44065b1911ef4c4e92cfa',
        'local_deployment_scope': True,
        'location_names': list([
        ]),
        'name': 'SNOWFLAKE_USER',
        'specific_branch_deployment_scope': None,
        'update_timestamp': '2023-05-04T19:14:15.542825Z',
        'updated_by': dict({
          'email': 'lopp@elementl.com',
        }),
        'value': None,
      }),
      dict({
        'all_branch_deployments_scope': True,
        'can_edit_secret': True,
        'can_view_secret_value': True,
        'full_deployment_scope': True,
        'id': '87198b0f905d4a90809595b310388e2d',
        'local_deployment_scope': True,
        'location_names': list([
        ]),
        'name': 'SNOWFLAKE_ROLE',
        'specific_branch_deployment_scope': None,
        'update_timestamp': '2023-11-29T21:58:48.844959Z',
        'updated_by': dict({
          'email': 'izzy@dagsterlabs.com',
        }),
        'value': None,
      }),
      dict({
        'all_branch_deployments_scope': True,
        'can_edit_secret': True,
        'can_view_secret_value': True,
        'full_deployment_scope': True,
        'id': '6d627d934f3445a59cc80a0700af6d65',
        'local_deployment_scope': True,
        'location_names': list([
        ]),
        'name': 'SNOWFLAKE_USERNAME',
        'specific_branch_deployment_scope': None,
        'update_timestamp': '2023-02-28T21:57:07.192470Z',
        'updated_by': dict({
          'email': 'lopp@elementl.com',
        }),
        'value': None,
      }),
      dict({
        'all_branch_deployments_scope': True,
        'can_edit_secret': True,
        'can_view_secret_value': True,
        'full_deployment_scope': True,
        'id': '4fdd87f4356d44dbbda1ee2f79936a59',
        'local_deployment_scope': True,
        'location_names': list([
        ]),
        'name': 'SNOWFLAKE_ACCOUNT',
        'specific_branch_deployment_scope': None,
        'update_timestamp': '2023-05-04T19:12:08.067031Z',
        'updated_by': dict({
          'email': 'lopp@elementl.com',
        }),
        'value': None,
      }),
      dict({
        'all_branch_deployments_scope': True,
        'can_edit_secret': True,
        'can_view_secret_value': True,
        'full_deployment_scope': True,
        'id': 'd09927cee20d4e599e766eb765c09519',
        'local_deployment_scope': True,
        'location_names': list([
        ]),
        'name': 'AWS_REGION',
        'specific_branch_deployment_scope': None,
        'update_timestamp': '2023-11-30T15:18:08.570713Z',
        'updated_by': dict({
          'email': 'izzy@dagsterlabs.com',
        }),
        'value': None,
      }),
      dict({
        'all_branch_deployments_scope': True,
        'can_edit_secret': True,
        'can_view_secret_value': True,
        'full_deployment_scope': True,
        'id': 'ef00b8a20b484620a261a50519afd644',
        'local_deployment_scope': True,
        'location_names': list([
        ]),
        'name': 'SNOWFLAKE_HOST',
        'specific_branch_deployment_scope': None,
        'update_timestamp': '2023-12-11T23:27:02.879755Z',
        'updated_by': dict({
          'email': 'izzy@dagsterlabs.com',
        }),
        'value': None,
      }),
      dict({
        'all_branch_deployments_scope': True,
        'can_edit_secret': True,
        'can_view_secret_value': True,
        'full_deployment_scope': False,
        'id': 'c607dcb8d19c4f80a2caf0bc1409fdd3',
        'local_deployment_scope': True,
        'location_names': list([
        ]),
        'name': 'DAGSTER_DBT_PARSE_PROJECT_ON_LOAD',
        'specific_branch_deployment_scope': None,
        'update_timestamp': '2024-04-05T17:52:55.019563Z',
        'updated_by': dict({
          'email': 'christian@dagsterlabs.com',
        }),
        'value': None,
      }),
      dict({
        'all_branch_deployments_scope': True,
        'can_edit_secret': True,
        'can_view_secret_value': True,
        'full_deployment_scope': True,
        'id': '0db2f8eec4e1434982b435227f050783',
        'local_deployment_scope': True,
        'location_names': list([
        ]),
        'name': 'AWS_S3_BUCKET',
        'specific_branch_deployment_scope': None,
        'update_timestamp': '2024-04-09T20:03:38.240532Z',
        'updated_by': dict({
          'email': 'christian@dagsterlabs.com',
        }),
        'value': None,
      }),
      dict({
        'all_branch_deployments_scope': True,
        'can_edit_secret': True,
        'can_view_secret_value': True,
        'full_deployment_scope': True,
        'id': 'b1467177bc8e4235b655edcfba6ca340',
        'local_deployment_scope': True,
        'location_names': list([
        ]),
        'name': 'AZURE_POWERBI_CLIENT_SECRET_ID',
        'specific_branch_deployment_scope': None,
        'update_timestamp': '2024-10-09T13:43:43.483920Z',
        'updated_by': dict({
          'email': 'christian@dagsterlabs.com',
        }),
        'value': None,
      }),
      dict({
        'all_branch_deployments_scope': True,
        'can_edit_secret': True,
        'can_view_secret_value': True,
        'full_deployment_scope': True,
        'id': 'e7b8bc919e4c491e985d6f34cd155604',
        'local_deployment_scope': True,
        'location_names': list([
        ]),
        'name': 'AZURE_POWERBI_CLIENT_ID',
        'specific_branch_deployment_scope': None,
        'update_timestamp': '2024-10-09T01:13:45.360461Z',
        'updated_by': dict({
          'email': 'christian@dagsterlabs.com',
        }),
        'value': None,
      }),
      dict({
        'all_branch_deployments_scope': True,
        'can_edit_secret': True,
        'can_view_secret_value': True,
        'full_deployment_scope': True,
        'id': '7981b0db240a4c02b7ff4663a696efcc',
        'local_deployment_scope': True,
        'location_names': list([
        ]),
        'name': 'AZURE_POWERBI_CLIENT_SECRET',
        'specific_branch_deployment_scope': None,
        'update_timestamp': '2024-10-09T01:13:56.007341Z',
        'updated_by': dict({
          'email': 'christian@dagsterlabs.com',
        }),
        'value': None,
      }),
      dict({
        'all_branch_deployments_scope': True,
        'can_edit_secret': True,
        'can_view_secret_value': True,
        'full_deployment_scope': True,
        'id': '48b8c1ad099545cdbbf44a8b0c520922',
        'local_deployment_scope': True,
        'location_names': list([
        ]),
        'name': 'AZURE_POWERBI_TENANT_ID',
        'specific_branch_deployment_scope': None,
        'update_timestamp': '2024-10-09T01:14:04.180889Z',
        'updated_by': dict({
          'email': 'christian@dagsterlabs.com',
        }),
        'value': None,
      }),
      dict({
        'all_branch_deployments_scope': True,
        'can_edit_secret': True,
        'can_view_secret_value': True,
        'full_deployment_scope': True,
        'id': '58fe4b5791b74556b394167a504876cc',
        'local_deployment_scope': True,
        'location_names': list([
        ]),
        'name': 'AZURE_POWERBI_WORKSPACE_ID',
        'specific_branch_deployment_scope': None,
        'update_timestamp': '2024-10-09T01:14:12.436403Z',
        'updated_by': dict({
          'email': 'christian@dagsterlabs.com',
        }),
        'value': None,
      }),
      dict({
        'all_branch_deployments_scope': True,
        'can_edit_secret': True,
        'can_view_secret_value': True,
        'full_deployment_scope': True,
        'id': '0eb6b13ae2fd4ac2b3d2e6f44688401c',
        'local_deployment_scope': True,
        'location_names': list([
        ]),
        'name': 'SNOWFLAKE_SCHEMA',
        'specific_branch_deployment_scope': None,
        'update_timestamp': '2025-06-27T19:20:59.521047Z',
        'updated_by': dict({
          'email': 'christian@dagsterlabs.com',
        }),
        'value': None,
      }),
      dict({
        'all_branch_deployments_scope': True,
        'can_edit_secret': True,
        'can_view_secret_value': True,
        'full_deployment_scope': True,
        'id': 'a6c8104f3ede4c41935bdd9895acd805',
        'local_deployment_scope': True,
        'location_names': list([
        ]),
        'name': 'EXAMPLE_ENVIRONMENT_VARIABLE',
        'specific_branch_deployment_scope': None,
        'update_timestamp': '2025-09-22T20:18:39.564529Z',
        'updated_by': dict({
          'email': 'colton@dagsterlabs.com',
        }),
        'value': None,
      }),
      dict({
        'all_branch_deployments_scope': True,
        'can_edit_secret': True,
        'can_view_secret_value': True,
        'full_deployment_scope': True,
        'id': 'c192d89000034a259062630a7fa58aad',
        'local_deployment_scope': True,
        'location_names': list([
        ]),
        'name': 'MWAA_AWS_ACCESS_KEY_ID',
        'specific_branch_deployment_scope': None,
        'update_timestamp': '2025-03-08T03:26:30.092809Z',
        'updated_by': dict({
          'email': 'colton@dagsterlabs.com',
        }),
        'value': None,
      }),
      dict({
        'all_branch_deployments_scope': True,
        'can_edit_secret': True,
        'can_view_secret_value': True,
        'full_deployment_scope': True,
        'id': 'bb813ff1bba04e179540839173faf9ed',
        'local_deployment_scope': True,
        'location_names': list([
        ]),
        'name': 'MWAA_AWS_SECRET_ACCESS_KEY',
        'specific_branch_deployment_scope': None,
        'update_timestamp': '2025-03-08T03:26:41.762188Z',
        'updated_by': dict({
          'email': 'colton@dagsterlabs.com',
        }),
        'value': None,
      }),
      dict({
        'all_branch_deployments_scope': False,
        'can_edit_secret': True,
        'can_view_secret_value': True,
        'full_deployment_scope': True,
        'id': '05ed2d9c9ac246adac68389d48ab448a',
        'local_deployment_scope': True,
        'location_names': list([
        ]),
        'name': 'DATABRICKS_TOKEN',
        'specific_branch_deployment_scope': None,
        'update_timestamp': '2025-07-26T00:32:00.876120Z',
        'updated_by': dict({
          'email': 'christian@dagsterlabs.com',
        }),
        'value': None,
      }),
      dict({
        'all_branch_deployments_scope': False,
        'can_edit_secret': True,
        'can_view_secret_value': True,
        'full_deployment_scope': True,
        'id': '18d411d2f8e143159fc348cfe2b9f578',
        'local_deployment_scope': True,
        'location_names': list([
        ]),
        'name': 'DATABRICKS_HOST',
        'specific_branch_deployment_scope': None,
        'update_timestamp': '2025-07-26T00:32:11.161514Z',
        'updated_by': dict({
          'email': 'christian@dagsterlabs.com',
        }),
        'value': None,
      }),
      dict({
        'all_branch_deployments_scope': True,
        'can_edit_secret': True,
        'can_view_secret_value': True,
        'full_deployment_scope': False,
        'id': '5e55512ef0164d6e92b050e54b0a2082',
        'local_deployment_scope': False,
        'location_names': list([
        ]),
        'name': 'DATABRICKS_HOST',
        'specific_branch_deployment_scope': None,
        'update_timestamp': '2025-07-26T00:32:44.377634Z',
        'updated_by': dict({
          'email': 'christian@dagsterlabs.com',
        }),
        'value': None,
      }),
      dict({
        'all_branch_deployments_scope': True,
        'can_edit_secret': True,
        'can_view_secret_value': True,
        'full_deployment_scope': False,
        'id': '65645109569c4acfa4b9f74de40329ce',
        'local_deployment_scope': False,
        'location_names': list([
        ]),
        'name': 'DATABRICKS_TOKEN',
        'specific_branch_deployment_scope': None,
        'update_timestamp': '2025-07-26T00:33:33.657806Z',
        'updated_by': dict({
          'email': 'christian@dagsterlabs.com',
        }),
        'value': None,
      }),
      dict({
        'all_branch_deployments_scope': True,
        'can_edit_secret': True,
        'can_view_secret_value': True,
        'full_deployment_scope': True,
        'id': '2d34f85706f44d77a33620ae5c3ef96b',
        'local_deployment_scope': True,
        'location_names': list([
        ]),
        'name': 'GITHUB_TOKEN',
        'specific_branch_deployment_scope': None,
        'update_timestamp': '2025-08-11T19:39:12.247125Z',
        'updated_by': dict({
          'email': 'christian@dagsterlabs.com',
        }),
        'value': None,
      }),
      dict({
        'all_branch_deployments_scope': True,
        'can_edit_secret': True,
        'can_view_secret_value': True,
        'full_deployment_scope': True,
        'id': 'f1a0dee193ac4f3d9a5c88e7f5169a26',
        'local_deployment_scope': True,
        'location_names': list([
        ]),
        'name': 'SNOWFLAKE_KEY',
        'specific_branch_deployment_scope': None,
        'update_timestamp': '2025-09-10T17:02:38.934973Z',
        'updated_by': dict({
          'email': 'christian@dagsterlabs.com',
        }),
        'value': None,
      }),
    ]),
    'total': 23,
  })
# ---
# name: TestDynamicCommandExecution.test_command_execution[secret_success_multiple_secrets_text_text]
  '''
  Name: SNOWFLAKE_USER
  Locations: All code locations
  Scopes: Full Deployment, All Branch Deployments, Local Deployment
  Can Edit: Yes
  Can View Value: Yes
  Updated By: lopp@elementl.com
  Updated: 2023-05-04 19:14:15
  
  Name: SNOWFLAKE_ROLE
  Locations: All code locations
  Scopes: Full Deployment, All Branch Deployments, Local Deployment
  Can Edit: Yes
  Can View Value: Yes
  Updated By: izzy@dagsterlabs.com
  Updated: 2023-11-29 21:58:48
  
  Name: SNOWFLAKE_USERNAME
  Locations: All code locations
  Scopes: Full Deployment, All Branch Deployments, Local Deployment
  Can Edit: Yes
  Can View Value: Yes
  Updated By: lopp@elementl.com
  Updated: 2023-02-28 21:57:07
  
  Name: SNOWFLAKE_ACCOUNT
  Locations: All code locations
  Scopes: Full Deployment, All Branch Deployments, Local Deployment
  Can Edit: Yes
  Can View Value: Yes
  Updated By: lopp@elementl.com
  Updated: 2023-05-04 19:12:08
  
  Name: AWS_REGION
  Locations: All code locations
  Scopes: Full Deployment, All Branch Deployments, Local Deployment
  Can Edit: Yes
  Can View Value: Yes
  Updated By: izzy@dagsterlabs.com
  Updated: 2023-11-30 15:18:08
  
  Name: SNOWFLAKE_HOST
  Locations: All code locations
  Scopes: Full Deployment, All Branch Deployments, Local Deployment
  Can Edit: Yes
  Can View Value: Yes
  Updated By: izzy@dagsterlabs.com
  Updated: 2023-12-11 23:27:02
  
  Name: DAGSTER_DBT_PARSE_PROJECT_ON_LOAD
  Locations: All code locations
  Scopes: All Branch Deployments, Local Deployment
  Can Edit: Yes
  Can View Value: Yes
  Updated By: christian@dagsterlabs.com
  Updated: 2024-04-05 17:52:55
  
  Name: AWS_S3_BUCKET
  Locations: All code locations
  Scopes: Full Deployment, All Branch Deployments, Local Deployment
  Can Edit: Yes
  Can View Value: Yes
  Updated By: christian@dagsterlabs.com
  Updated: 2024-04-09 20:03:38
  
  Name: AZURE_POWERBI_CLIENT_SECRET_ID
  Locations: All code locations
  Scopes: Full Deployment, All Branch Deployments, Local Deployment
  Can Edit: Yes
  Can View Value: Yes
  Updated By: christian@dagsterlabs.com
  Updated: 2024-10-09 13:43:43
  
  Name: AZURE_POWERBI_CLIENT_ID
  Locations: All code locations
  Scopes: Full Deployment, All Branch Deployments, Local Deployment
  Can Edit: Yes
  Can View Value: Yes
  Updated By: christian@dagsterlabs.com
  Updated: 2024-10-09 01:13:45
  
  Name: AZURE_POWERBI_CLIENT_SECRET
  Locations: All code locations
  Scopes: Full Deployment, All Branch Deployments, Local Deployment
  Can Edit: Yes
  Can View Value: Yes
  Updated By: christian@dagsterlabs.com
  Updated: 2024-10-09 01:13:56
  
  Name: AZURE_POWERBI_TENANT_ID
  Locations: All code locations
  Scopes: Full Deployment, All Branch Deployments, Local Deployment
  Can Edit: Yes
  Can View Value: Yes
  Updated By: christian@dagsterlabs.com
  Updated: 2024-10-09 01:14:04
  
  Name: AZURE_POWERBI_WORKSPACE_ID
  Locations: All code locations
  Scopes: Full Deployment, All Branch Deployments, Local Deployment
  Can Edit: Yes
  Can View Value: Yes
  Updated By: christian@dagsterlabs.com
  Updated: 2024-10-09 01:14:12
  
  Name: SNOWFLAKE_SCHEMA
  Locations: All code locations
  Scopes: Full Deployment, All Branch Deployments, Local Deployment
  Can Edit: Yes
  Can View Value: Yes
  Updated By: christian@dagsterlabs.com
  Updated: 2025-06-27 19:20:59
  
  Name: EXAMPLE_ENVIRONMENT_VARIABLE
  Locations: All code locations
  Scopes: Full Deployment, All Branch Deployments, Local Deployment
  Can Edit: Yes
  Can View Value: Yes
  Updated By: colton@dagsterlabs.com
  Updated: 2025-09-22 20:18:39
  
  Name: MWAA_AWS_ACCESS_KEY_ID
  Locations: All code locations
  Scopes: Full Deployment, All Branch Deployments, Local Deployment
  Can Edit: Yes
  Can View Value: Yes
  Updated By: colton@dagsterlabs.com
  Updated: 2025-03-08 03:26:30
  
  Name: MWAA_AWS_SECRET_ACCESS_KEY
  Locations: All code locations
  Scopes: Full Deployment, All Branch Deployments, Local Deployment
  Can Edit: Yes
  Can View Value: Yes
  Updated By: colton@dagsterlabs.com
  Updated: 2025-03-08 03:26:41
  
  Name: DATABRICKS_TOKEN
  Locations: All code locations
  Scopes: Full Deployment, Local Deployment
  Can Edit: Yes
  Can View Value: Yes
  Updated By: christian@dagsterlabs.com
  Updated: 2025-07-26 00:32:00
  
  Name: DATABRICKS_HOST
  Locations: All code locations
  Scopes: Full Deployment, Local Deployment
  Can Edit: Yes
  Can View Value: Yes
  Updated By: christian@dagsterlabs.com
  Updated: 2025-07-26 00:32:11
  
  Name: DATABRICKS_HOST
  Locations: All code locations
  Scopes: All Branch Deployments
  Can Edit: Yes
  Can View Value: Yes
  Updated By: christian@dagsterlabs.com
  Updated: 2025-07-26 00:32:44
  
  Name: DATABRICKS_TOKEN
  Locations: All code locations
  Scopes: All Branch Deployments
  Can Edit: Yes
  Can View Value: Yes
  Updated By: christian@dagsterlabs.com
  Updated: 2025-07-26 00:33:33
  
  Name: GITHUB_TOKEN
  Locations: All code locations
  Scopes: Full Deployment, All Branch Deployments, Local Deployment
  Can Edit: Yes
  Can View Value: Yes
  Updated By: christian@dagsterlabs.com
  Updated: 2025-08-11 19:39:12
  
  Name: SNOWFLAKE_KEY
  Locations: All code locations
  Scopes: Full Deployment, All Branch Deployments, Local Deployment
  Can Edit: Yes
  Can View Value: Yes
  Updated By: christian@dagsterlabs.com
  Updated: 2025-09-10 17:02:38
  
  '''
# ---
# name: TestDynamicCommandExecution.test_command_execution[secret_success_single_secret_json]
  dict({
    'all_branch_deployments_scope': True,
    'can_edit_secret': True,
    'can_view_secret_value': True,
    'full_deployment_scope': True,
    'id': 'a6c8104f3ede4c41935bdd9895acd805',
    'local_deployment_scope': True,
    'location_names': list([
    ]),
    'name': 'EXAMPLE_ENVIRONMENT_VARIABLE',
    'specific_branch_deployment_scope': None,
    'update_timestamp': '2025-09-22T20:18:39.564529Z',
    'updated_by': dict({
      'email': 'colton@dagsterlabs.com',
    }),
    'value': None,
  })
# ---
# name: TestDynamicCommandExecution.test_command_execution[secret_success_single_secret_text_text]
  '''
  Name: EXAMPLE_ENVIRONMENT_VARIABLE
  Locations: All code locations
  Scopes: Full Deployment, All Branch Deployments, Local Deployment
  Permissions:
    Can Edit: Yes
    Can View Value: Yes
  
  Value: <hidden - use --show-value to display>
  
  Updated By: colton@dagsterlabs.com
  Updated: 2025-09-22 20:18:39
  
  '''
# ---
# name: TestDynamicCommandExecution.test_command_execution[secret_success_single_secret_with_value_json]
  dict({
    'all_branch_deployments_scope': True,
    'can_edit_secret': True,
    'can_view_secret_value': True,
    'full_deployment_scope': True,
    'id': 'a6c8104f3ede4c41935bdd9895acd805',
    'local_deployment_scope': True,
    'location_names': list([
    ]),
    'name': 'EXAMPLE_ENVIRONMENT_VARIABLE',
    'specific_branch_deployment_scope': None,
    'update_timestamp': '2025-09-22T20:18:39.564529Z',
    'updated_by': dict({
      'email': 'colton@dagsterlabs.com',
    }),
    'value': 'Hello, World!',
  })
# ---
# name: TestDynamicCommandExecution.test_command_execution[secret_success_single_secret_with_value_text_text]
  '''
  Name: EXAMPLE_ENVIRONMENT_VARIABLE
  Locations: All code locations
  Scopes: Full Deployment, All Branch Deployments, Local Deployment
  Permissions:
    Can Edit: Yes
    Can View Value: Yes
  
  Value:
    Hello, World!
  
  Updated By: colton@dagsterlabs.com
  Updated: 2025-09-22 20:18:39
  
  '''
# ---
# name: TestDynamicCommandExecution.test_command_execution[sensor_empty_filtered_sensors_json]
  dict({
    'items': list([
      dict({
        'description': None,
        'name': 'default_automation_condition_sensor',
        'next_tick_timestamp': None,
        'sensor_type': 'AUTO_MATERIALIZE',
        'status': 'RUNNING',
      }),
      dict({
        'description': None,
        'name': 'orders_sensor',
        'next_tick_timestamp': None,
        'sensor_type': 'ASSET',
        'status': 'RUNNING',
      }),
      dict({
        'description': None,
        'name': 'watch_s3_sensor',
        'next_tick_timestamp': None,
        'sensor_type': 'STANDARD',
        'status': 'RUNNING',
      }),
      dict({
        'description': '''
          
              This sensor launches execution of freshness checks for the provided assets. The sensor will
              only launch a new execution of a freshness check if the check previously passed, but enough
              time has passed that the check could be overdue again. Once a check has failed, the sensor
              will not launch a new execution until the asset has been updated (which should automatically
              execute the check).
              
        ''',
        'name': 'weekly_freshness_check_sensor',
        'next_tick_timestamp': None,
        'sensor_type': 'STANDARD',
        'status': 'RUNNING',
      }),
      dict({
        'description': None,
        'name': 'default_automation_condition_sensor',
        'next_tick_timestamp': None,
        'sensor_type': 'AUTO_MATERIALIZE',
        'status': 'RUNNING',
      }),
      dict({
        'description': None,
        'name': 'mwaa_hooli_airflow_01__airflow_dag_status_sensor',
        'next_tick_timestamp': None,
        'sensor_type': 'STANDARD',
        'status': 'RUNNING',
      }),
    ]),
    'total': 6,
  })
# ---
# name: TestDynamicCommandExecution.test_command_execution[sensor_empty_sensors_json]
  dict({
    'items': list([
      dict({
        'description': None,
        'name': 'dbt_code_version_sensor',
        'next_tick_timestamp': None,
        'sensor_type': 'STANDARD',
        'status': 'STOPPED',
      }),
      dict({
        'description': None,
        'name': 'default_automation_condition_sensor',
        'next_tick_timestamp': None,
        'sensor_type': 'AUTO_MATERIALIZE',
        'status': 'RUNNING',
      }),
      dict({
        'description': None,
        'name': 'orders_sensor',
        'next_tick_timestamp': None,
        'sensor_type': 'ASSET',
        'status': 'RUNNING',
      }),
      dict({
        'description': None,
        'name': 'watch_s3_sensor',
        'next_tick_timestamp': None,
        'sensor_type': 'STANDARD',
        'status': 'RUNNING',
      }),
      dict({
        'description': '''
          
              This sensor launches execution of freshness checks for the provided assets. The sensor will
              only launch a new execution of a freshness check if the check previously passed, but enough
              time has passed that the check could be overdue again. Once a check has failed, the sensor
              will not launch a new execution until the asset has been updated (which should automatically
              execute the check).
              
        ''',
        'name': 'weekly_freshness_check_sensor',
        'next_tick_timestamp': None,
        'sensor_type': 'STANDARD',
        'status': 'RUNNING',
      }),
      dict({
        'description': None,
        'name': 'default_automation_condition_sensor',
        'next_tick_timestamp': None,
        'sensor_type': 'AUTO_MATERIALIZE',
        'status': 'RUNNING',
      }),
      dict({
        'description': None,
        'name': 'mwaa_hooli_airflow_01__airflow_dag_status_sensor',
        'next_tick_timestamp': None,
        'sensor_type': 'STANDARD',
        'status': 'RUNNING',
      }),
    ]),
    'total': 7,
  })
# ---
# name: TestDynamicCommandExecution.test_command_execution[sensor_empty_sensors_text_text]
  '''
  Name: dbt_code_version_sensor
  Status: STOPPED
  Type: STANDARD
  Description: None
  
  Name: default_automation_condition_sensor
  Status: RUNNING
  Type: AUTO_MATERIALIZE
  Description: None
  
  Name: orders_sensor
  Status: RUNNING
  Type: ASSET
  Description: None
  
  Name: watch_s3_sensor
  Status: RUNNING
  Type: STANDARD
  Description: None
  
  Name: weekly_freshness_check_sensor
  Status: RUNNING
  Type: STANDARD
  Description: 
      This sensor launches execution of freshness checks for the provided assets. The sensor will
      only launch a new execution of a freshness check if the check previously passed, but enough
      time has passed that the check could be overdue again. Once a check has failed, the sensor
      will not launch a new execution until the asset has been updated (which should automatically
      execute the check).
      
  
  Name: default_automation_condition_sensor
  Status: RUNNING
  Type: AUTO_MATERIALIZE
  Description: None
  
  Name: mwaa_hooli_airflow_01__airflow_dag_status_sensor
  Status: RUNNING
  Type: STANDARD
  Description: None
  
  '''
# ---
# name: TestDynamicCommandExecution.test_command_execution[sensor_error_empty_sensor_name_json]
  dict({
    'error': 'Exhausted 0 responses',
  })
# ---
# name: TestDynamicCommandExecution.test_command_execution[sensor_error_invalid_status_filter_json]
  '''
  Usage: dg api sensor list [OPTIONS]
  Try 'dg api sensor list -h' for help.
  
  Error: Invalid value for '--status': 'INVALID_STATUS' is not one of 'RUNNING', 'STOPPED', 'PAUSED'.
  
  '''
# ---
# name: TestDynamicCommandExecution.test_command_execution[sensor_error_multiple_sensors_same_name_json]
  dict({
    'error': 'Exhausted 0 responses',
  })
# ---
# name: TestDynamicCommandExecution.test_command_execution[sensor_error_sensor_not_found_json]
  dict({
    'error': 'Exhausted 0 responses',
  })
# ---
# name: TestDynamicCommandExecution.test_command_execution[sensor_error_sensor_not_found_text_text]
  '''
  Error: Failed to get sensor: Exhausted 0 responses
  
  '''
# ---
# name: TestDynamicCommandExecution.test_command_execution[sensor_success_filtered_sensors_paused_json]
  dict({
    'items': list([
    ]),
    'total': 0,
  })
# ---
# name: TestDynamicCommandExecution.test_command_execution[sensor_success_filtered_sensors_paused_text_text]
  '''
  
  
  '''
# ---
# name: TestDynamicCommandExecution.test_command_execution[sensor_success_filtered_sensors_running_json]
  dict({
    'items': list([
      dict({
        'description': None,
        'name': 'default_automation_condition_sensor',
        'next_tick_timestamp': None,
        'sensor_type': 'AUTO_MATERIALIZE',
        'status': 'RUNNING',
      }),
      dict({
        'description': None,
        'name': 'orders_sensor',
        'next_tick_timestamp': None,
        'sensor_type': 'ASSET',
        'status': 'RUNNING',
      }),
      dict({
        'description': None,
        'name': 'watch_s3_sensor',
        'next_tick_timestamp': None,
        'sensor_type': 'STANDARD',
        'status': 'RUNNING',
      }),
      dict({
        'description': '''
          
              This sensor launches execution of freshness checks for the provided assets. The sensor will
              only launch a new execution of a freshness check if the check previously passed, but enough
              time has passed that the check could be overdue again. Once a check has failed, the sensor
              will not launch a new execution until the asset has been updated (which should automatically
              execute the check).
              
        ''',
        'name': 'weekly_freshness_check_sensor',
        'next_tick_timestamp': None,
        'sensor_type': 'STANDARD',
        'status': 'RUNNING',
      }),
      dict({
        'description': None,
        'name': 'default_automation_condition_sensor',
        'next_tick_timestamp': None,
        'sensor_type': 'AUTO_MATERIALIZE',
        'status': 'RUNNING',
      }),
      dict({
        'description': None,
        'name': 'mwaa_hooli_airflow_01__airflow_dag_status_sensor',
        'next_tick_timestamp': None,
        'sensor_type': 'STANDARD',
        'status': 'RUNNING',
      }),
    ]),
    'total': 6,
  })
# ---
# name: TestDynamicCommandExecution.test_command_execution[sensor_success_filtered_sensors_running_text_text]
  '''
  Name: default_automation_condition_sensor
  Status: RUNNING
  Type: AUTO_MATERIALIZE
  Description: None
  
  Name: orders_sensor
  Status: RUNNING
  Type: ASSET
  Description: None
  
  Name: watch_s3_sensor
  Status: RUNNING
  Type: STANDARD
  Description: None
  
  Name: weekly_freshness_check_sensor
  Status: RUNNING
  Type: STANDARD
  Description: 
      This sensor launches execution of freshness checks for the provided assets. The sensor will
      only launch a new execution of a freshness check if the check previously passed, but enough
      time has passed that the check could be overdue again. Once a check has failed, the sensor
      will not launch a new execution until the asset has been updated (which should automatically
      execute the check).
      
  
  Name: default_automation_condition_sensor
  Status: RUNNING
  Type: AUTO_MATERIALIZE
  Description: None
  
  Name: mwaa_hooli_airflow_01__airflow_dag_status_sensor
  Status: RUNNING
  Type: STANDARD
  Description: None
  
  '''
# ---
# name: TestDynamicCommandExecution.test_command_execution[sensor_success_filtered_sensors_stopped_json]
  dict({
    'items': list([
      dict({
        'description': None,
        'name': 'dbt_code_version_sensor',
        'next_tick_timestamp': None,
        'sensor_type': 'STANDARD',
        'status': 'STOPPED',
      }),
    ]),
    'total': 1,
  })
# ---
# name: TestDynamicCommandExecution.test_command_execution[sensor_success_filtered_sensors_stopped_text_text]
  '''
  Name: dbt_code_version_sensor
  Status: STOPPED
  Type: STANDARD
  Description: None
  
  '''
# ---
# name: TestDynamicCommandExecution.test_command_execution[sensor_success_multiple_sensors_json]
  dict({
    'items': list([
      dict({
        'description': None,
        'name': 'dbt_code_version_sensor',
        'next_tick_timestamp': None,
        'sensor_type': 'STANDARD',
        'status': 'STOPPED',
      }),
      dict({
        'description': None,
        'name': 'default_automation_condition_sensor',
        'next_tick_timestamp': None,
        'sensor_type': 'AUTO_MATERIALIZE',
        'status': 'RUNNING',
      }),
      dict({
        'description': None,
        'name': 'orders_sensor',
        'next_tick_timestamp': None,
        'sensor_type': 'ASSET',
        'status': 'RUNNING',
      }),
      dict({
        'description': None,
        'name': 'watch_s3_sensor',
        'next_tick_timestamp': None,
        'sensor_type': 'STANDARD',
        'status': 'RUNNING',
      }),
      dict({
        'description': '''
          
              This sensor launches execution of freshness checks for the provided assets. The sensor will
              only launch a new execution of a freshness check if the check previously passed, but enough
              time has passed that the check could be overdue again. Once a check has failed, the sensor
              will not launch a new execution until the asset has been updated (which should automatically
              execute the check).
              
        ''',
        'name': 'weekly_freshness_check_sensor',
        'next_tick_timestamp': None,
        'sensor_type': 'STANDARD',
        'status': 'RUNNING',
      }),
      dict({
        'description': None,
        'name': 'default_automation_condition_sensor',
        'next_tick_timestamp': None,
        'sensor_type': 'AUTO_MATERIALIZE',
        'status': 'RUNNING',
      }),
      dict({
        'description': None,
        'name': 'mwaa_hooli_airflow_01__airflow_dag_status_sensor',
        'next_tick_timestamp': None,
        'sensor_type': 'STANDARD',
        'status': 'RUNNING',
      }),
    ]),
    'total': 7,
  })
# ---
# name: TestDynamicCommandExecution.test_command_execution[sensor_success_multiple_sensors_text_text]
  '''
  Name: dbt_code_version_sensor
  Status: STOPPED
  Type: STANDARD
  Description: None
  
  Name: default_automation_condition_sensor
  Status: RUNNING
  Type: AUTO_MATERIALIZE
  Description: None
  
  Name: orders_sensor
  Status: RUNNING
  Type: ASSET
  Description: None
  
  Name: watch_s3_sensor
  Status: RUNNING
  Type: STANDARD
  Description: None
  
  Name: weekly_freshness_check_sensor
  Status: RUNNING
  Type: STANDARD
  Description: 
      This sensor launches execution of freshness checks for the provided assets. The sensor will
      only launch a new execution of a freshness check if the check previously passed, but enough
      time has passed that the check could be overdue again. Once a check has failed, the sensor
      will not launch a new execution until the asset has been updated (which should automatically
      execute the check).
      
  
  Name: default_automation_condition_sensor
  Status: RUNNING
  Type: AUTO_MATERIALIZE
  Description: None
  
  Name: mwaa_hooli_airflow_01__airflow_dag_status_sensor
  Status: RUNNING
  Type: STANDARD
  Description: None
  
  '''
# ---
# name: TestDynamicCommandExecution.test_command_execution[sensor_success_single_sensor_running_json]
  dict({
    'description': None,
    'name': 'orders_sensor',
    'next_tick_timestamp': None,
    'sensor_type': 'ASSET',
    'status': 'RUNNING',
  })
# ---
# name: TestDynamicCommandExecution.test_command_execution[sensor_success_single_sensor_running_text_text]
  '''
  Name: orders_sensor
  Status: RUNNING
  Type: ASSET
  Description: None
  
  '''
# ---
# name: TestDynamicCommandExecution.test_command_execution[schedule_error_empty_schedule_name_json]
  dict({
    'code': 'INTERNAL_ERROR',
    'error': "Schedule not found: ''",
    'statusCode': 500,
    'type': 'server_error',
  })
# ---
# name: TestDynamicCommandExecution.test_command_execution[schedule_error_schedule_not_found_json]
  dict({
    'code': 'INTERNAL_ERROR',
    'error': 'Schedule not found: nonexistent-schedule',
    'statusCode': 500,
    'type': 'server_error',
  })
# ---
# name: TestDynamicCommandExecution.test_command_execution[schedule_success_multiple_schedules_json]
  dict({
    'items': list([
      dict({
        'cron_schedule': '0 0 * * *',
        'description': None,
        'execution_timezone': 'UTC',
        'id': '7a1ff4e6d334c36613cf2668b32c9bc4740b4751::19b110ce646fe80a563955a7d064ece4e5772ff0',
        'metadata_entries': list([
        ]),
        'name': 'check_linear_issues_schedule',
        'pipeline_name': 'check_linear_issues',
        'tags': list([
        ]),
      }),
      dict({
        'cron_schedule': '0 0 * * *',
        'description': None,
        'execution_timezone': 'UTC',
        'id': '24dca488bba47309385a8ebbf23d6c897388079d::381c62a7cf10fba230a975fc3aea0007d8bde9d6',
        'metadata_entries': list([
        ]),
        'name': 'check_ssl_expiration_schedule',
        'pipeline_name': 'check_ssl_expiration',
        'tags': list([
        ]),
      }),
      dict({
        'cron_schedule': '0 3 * * *',
        'description': None,
        'execution_timezone': 'UTC',
        'id': 'a204edd99883ef3fee1c0cd54c6192bf2b19269a::2b72065e9cbcd43a372a831bd4817b8da771214b',
        'metadata_entries': list([
        ]),
        'name': 'aws_replication_schedule',
        'pipeline_name': 'aws_replication_job',
        'tags': list([
        ]),
      }),
      dict({
        'cron_schedule': '0 3 * * *',
        'description': None,
        'execution_timezone': 'UTC',
        'id': 'b39f43b63a5d1c1308ce7aaa038da3df9d1fa902::aa094302d889abc0cfe8411f1048d39f3e9ad220',
        'metadata_entries': list([
        ]),
        'name': 'database_clone_cleanup_job_schedule',
        'pipeline_name': 'database_clone_cleanup_job',
        'tags': list([
        ]),
      }),
      dict({
        'cron_schedule': '0 3 * * *',
        'description': None,
        'execution_timezone': 'UTC',
        'id': '023097b9da352672eb18f8857fa81248dd686c6d::3e3240e79cdf4769eede4cac4ae9e952eda41bc0',
        'metadata_entries': list([
        ]),
        'name': 'dbt_analytics_core_schedule',
        'pipeline_name': 'dbt_analytics_core_job',
        'tags': list([
        ]),
      }),
      dict({
        'cron_schedule': '0 7 * * *',
        'description': None,
        'execution_timezone': 'UTC',
        'id': 'e88b09f1b1cac4bc7dd3370b73afd347c59548ce::72d9862a71d8703a11871a472c2b500d1adaaa62',
        'metadata_entries': list([
        ]),
        'name': 'dbt_analytics_snapshot_job_schedule',
        'pipeline_name': 'dbt_analytics_snapshot_job',
        'tags': list([
        ]),
      }),
      dict({
        'cron_schedule': '0 0 * * *',
        'description': None,
        'execution_timezone': 'UTC',
        'id': '2eec9bb0d1df64072ee3ebf6975372d695e7d2f6::25d2344f52ffd6484d6088361f6ca9ecb7849dc8',
        'metadata_entries': list([
        ]),
        'name': 'fivetran_connection_setup_tests_schedule',
        'pipeline_name': 'fivetran_connection_setup_tests',
        'tags': list([
        ]),
      }),
      dict({
        'cron_schedule': '0 0 * * *',
        'description': None,
        'execution_timezone': 'UTC',
        'id': '7bc30bd776c47813e4b98dc4c9d8f0e34fa30679::a381eb6a96f7ed463bcc69b501a04c9e8724b175',
        'metadata_entries': list([
        ]),
        'name': 'gong_calls_transcript_job_schedule',
        'pipeline_name': 'gong_calls_transcript_job',
        'tags': list([
        ]),
      }),
      dict({
        'cron_schedule': '0 2 * * *',
        'description': 'Daily schedule that processes the last 4 days of Google Search Console data',
        'execution_timezone': 'UTC',
        'id': '89a73b044bc53cc90f8fa988cf5536d5c3f5668b::8217cf1250bedd3eebf25c3c2f372566f3932126',
        'metadata_entries': list([
        ]),
        'name': 'google_search_console_daily_schedule',
        'pipeline_name': '__anonymous_asset_job_google_search_console_daily_schedule',
        'tags': list([
        ]),
      }),
      dict({
        'cron_schedule': '0 3 * * *',
        'description': None,
        'execution_timezone': 'UTC',
        'id': '8b27f90dbfe40bea2df6258eea2c473fdf14ce65::0205c824b71f6ff8fa187361befd6deca72c6fd3',
        'metadata_entries': list([
        ]),
        'name': 'hightouch_syncs_schedule',
        'pipeline_name': '__anonymous_asset_job_hightouch_syncs_schedule',
        'tags': list([
          dict({
            'key': 'team',
            'value': 'devrel',
          }),
        ]),
      }),
      dict({
        'cron_schedule': '0 * * * *',
        'description': None,
        'execution_timezone': 'UTC',
        'id': '7c695e2d3bd06de9bb8a87d17bb8f9fcbd0537f7::e9bbccfeabd17761009bca101d6eb5faae3ab9e5',
        'metadata_entries': list([
        ]),
        'name': 'hourly_hightouch_syncs_schedule',
        'pipeline_name': 'hourly_hightouch_syncs_job',
        'tags': list([
        ]),
      }),
      dict({
        'cron_schedule': '0 1 * * *',
        'description': 'Schedule that processes Sequel events data once daily at 1 AM EST',
        'execution_timezone': 'UTC',
        'id': '32716fe3fcaee9f96f80dab09162a35bf6db55f7::286d498370076140e605c70b8dd26f62140b6461',
        'metadata_entries': list([
        ]),
        'name': 'sequel_events_schedule',
        'pipeline_name': '__anonymous_asset_job_sequel_events_schedule',
        'tags': list([
        ]),
      }),
      dict({
        'cron_schedule': '15 1 * * *',
        'description': 'Schedule that processes Sequel registrants data once daily at 1:15 AM EST',
        'execution_timezone': 'UTC',
        'id': '8cf783a4f30d0eee13ad6d1aff783f3a98cb642d::88f3ef19f5e86fe76699e11e1ec45db2262134c1',
        'metadata_entries': list([
        ]),
        'name': 'sequel_registrants_schedule',
        'pipeline_name': '__anonymous_asset_job_sequel_registrants_schedule',
        'tags': list([
        ]),
      }),
      dict({
        'cron_schedule': '30 1 * * *',
        'description': 'Daily processing of Sequel event user activity logs at 1:30 AM EST',
        'execution_timezone': 'UTC',
        'id': 'c17696894dbe4559d5a493b5205f1e0b131019c9::238a6eb980181273843e5cd629c6b0006e808dd9',
        'metadata_entries': list([
        ]),
        'name': 'sequel_user_activity_logs_schedule',
        'pipeline_name': '__anonymous_asset_job_sequel_user_activity_logs_schedule',
        'tags': list([
        ]),
      }),
      dict({
        'cron_schedule': '0 3 * * *',
        'description': None,
        'execution_timezone': 'UTC',
        'id': 'b80992548a60f960c3b5edcd3ee05afa319b4507::d49bf91e4a6bc2534bc70cd53e80332e1aed77c6',
        'metadata_entries': list([
        ]),
        'name': 'sling_egress_job_schedule',
        'pipeline_name': 'sling_egress_job',
        'tags': list([
        ]),
      }),
      dict({
        'cron_schedule': '0 1 * * *',
        'description': None,
        'execution_timezone': 'UTC',
        'id': '90aab752d0482041196a3c8dc4adf10d3865baa4::23eb4db9d37a464de9403a50249e2781e13e7e64',
        'metadata_entries': list([
        ]),
        'name': 'statsig_upload_schedule',
        'pipeline_name': 'statsig_upload_job',
        'tags': list([
        ]),
      }),
      dict({
        'cron_schedule': '0 0 * * *',
        'description': None,
        'execution_timezone': 'UTC',
        'id': 'a1d227c3eea93d614f4b1f01b00b01232752583c::bdadc269f1087529f0d4f1c3d88692dd5dff5b0a',
        'metadata_entries': list([
        ]),
        'name': 'stripe_data_sync_observe_job_schedule',
        'pipeline_name': 'stripe_data_sync_observe_job',
        'tags': list([
        ]),
      }),
      dict({
        'cron_schedule': '@daily',
        'description': None,
        'execution_timezone': 'UTC',
        'id': '9117375126742ae12158a8740faf5d3df2f9fd1f::aba6b3993379f4a23c2813179050474cc8a10ccb',
        'metadata_entries': list([
        ]),
        'name': 'support_bot_schedule',
        'pipeline_name': 'support_bot_job',
        'tags': list([
        ]),
      }),
      dict({
        'cron_schedule': '0 3 * * *',
        'description': None,
        'execution_timezone': 'UTC',
        'id': '16bd4ed3fdf2ed4c1b2516094b94e1a47124330d::f94c28b35ff090169a603c0ef27fc6122b6382ad',
        'metadata_entries': list([
        ]),
        'name': 'product_operations_clone_cleanup_job_schedule',
        'pipeline_name': 'product_operations_clone_cleanup_job',
        'tags': list([
        ]),
      }),
      dict({
        'cron_schedule': '0 0 * * *',
        'description': None,
        'execution_timezone': 'UTC',
        'id': '1512b6ea06f79a49fc196b81592b7865bfa1cd13::4a45a9a1ab5d9c036aa7366582bacf6f26d67864',
        'metadata_entries': list([
        ]),
        'name': 'sync_insights_to_pg_schedule',
        'pipeline_name': 'sync_insights_to_pg_job',
        'tags': list([
        ]),
      }),
      dict({
        'cron_schedule': '0 8 * * 1-5',
        'description': None,
        'execution_timezone': 'US/Pacific',
        'id': '56b81eb0b636a2d99b910815b1ca13666311e8de::e30aff83f4bad40b9a600828f4b7f044ef3fc453',
        'metadata_entries': list([
        ]),
        'name': 'billing_issues_rollup_schedule',
        'pipeline_name': 'billing_issues_rollup',
        'tags': list([
        ]),
      }),
      dict({
        'cron_schedule': '0 0 * * *',
        'description': None,
        'execution_timezone': 'UTC',
        'id': '0c8047eb8097dd3786442e0b55a6c55782b3ef24::125ecdd4b7d75b7afd0a027d4060ff2563d5cf8c',
        'metadata_entries': list([
        ]),
        'name': 'daily_check_user_pipeline_status_job',
        'pipeline_name': 'check_user_pipeline_status_job',
        'tags': list([
        ]),
      }),
      dict({
        'cron_schedule': '30 * * * *',
        'description': None,
        'execution_timezone': 'UTC',
        'id': '33d43d1e40096781894ad677034dd6ab434d737f::33e4f97daac8ab0ca10be544ac0c57522bbe7e00',
        'metadata_entries': list([
        ]),
        'name': 'detect_self_serve_customers',
        'pipeline_name': 'detect_self_serve_customers',
        'tags': list([
        ]),
      }),
      dict({
        'cron_schedule': '0 7 * * *',
        'description': None,
        'execution_timezone': 'UTC',
        'id': '7a101df50e98f61f8fe35bcae8f88a3c29d1807e::02511114a0d3918cb1fe6c77d57864e3d3be5a3a',
        'metadata_entries': list([
        ]),
        'name': 'self_serve_customer_attribution_check_all_partitions_job_schedule',
        'pipeline_name': 'self_serve_customer_attribution_check_all_partitions_job',
        'tags': list([
        ]),
      }),
      dict({
        'cron_schedule': '59 * * * *',
        'description': None,
        'execution_timezone': 'UTC',
        'id': '0766e565572648c59472a4d2426655371f628a9f::65aad9b6c8fbddf8dc092e0f2e21bfd682f02d81',
        'metadata_entries': list([
        ]),
        'name': 'snowflake_insights_import_schedule',
        'pipeline_name': 'snowflake_insights_import',
        'tags': list([
        ]),
      }),
      dict({
        'cron_schedule': '0 4 * * *',
        'description': None,
        'execution_timezone': 'UTC',
        'id': '7480bf436140cb9452293058647b5a0107a32ecb::df691704a58e36d10658aca06674e2d2714fc280',
        'metadata_entries': list([
        ]),
        'name': 'sync_enterprise_contract_metadata_to_pg_schedule',
        'pipeline_name': 'sync_enterprise_contract_metadata_to_pg_job',
        'tags': list([
        ]),
      }),
      dict({
        'cron_schedule': '0 8 * * *',
        'description': None,
        'execution_timezone': 'UTC',
        'id': 'f010499ad97db681455121ee87f8310a3d61e7fb::0dc13a1576dfa974ecf66ec0df49ae7c366c971b',
        'metadata_entries': list([
        ]),
        'name': 'telemetry_events_to_snowflake_schedule',
        'pipeline_name': 'telemetry_events_to_snowflake',
        'tags': list([
        ]),
      }),
      dict({
        'cron_schedule': '0 */3 * * *',
        'description': None,
        'execution_timezone': 'UTC',
        'id': 'fd843abef5f0c30f0eb014f4d85c9e4f06a7c233::034edb748ec2915d90234111ac003fe1495db474',
        'metadata_entries': list([
        ]),
        'name': 'usage_attribution_schedule',
        'pipeline_name': 'self_serve_attribution_job',
        'tags': list([
        ]),
      }),
      dict({
        'cron_schedule': '*/10 * * * *',
        'description': None,
        'execution_timezone': 'UTC',
        'id': '070acb7bb19e457c407432bd8c4009c250eb1ad8::56c7ec2fa124bf4d70c9cd507d96cdba72286746',
        'metadata_entries': list([
        ]),
        'name': 'dogfood_cypress_job_schedule',
        'pipeline_name': 'dogfood_cypress_job',
        'tags': list([
        ]),
      }),
      dict({
        'cron_schedule': '*/10 * * * *',
        'description': None,
        'execution_timezone': 'UTC',
        'id': 'de73598d4ba48d6845bbbc3102cb8605b59ec96b::ab19554e7200c8f6d3e34585dec547a6a025b5fc',
        'metadata_entries': list([
        ]),
        'name': 'production_cypress_job_schedule',
        'pipeline_name': 'production_cypress_job',
        'tags': list([
        ]),
      }),
    ]),
  })
# ---
# name: TestDynamicCommandExecution.test_command_execution[schedule_success_schedules_human_readable_text]
  '''
  Name: check_linear_issues_schedule
  Job Name: check_linear_issues
  Cron Schedule: 0 0 * * *
  
  Name: check_ssl_expiration_schedule
  Job Name: check_ssl_expiration
  Cron Schedule: 0 0 * * *
  
  Name: aws_replication_schedule
  Job Name: aws_replication_job
  Cron Schedule: 0 3 * * *
  
  Name: database_clone_cleanup_job_schedule
  Job Name: database_clone_cleanup_job
  Cron Schedule: 0 3 * * *
  
  Name: dbt_analytics_core_schedule
  Job Name: dbt_analytics_core_job
  Cron Schedule: 0 3 * * *
  
  Name: dbt_analytics_snapshot_job_schedule
  Job Name: dbt_analytics_snapshot_job
  Cron Schedule: 0 7 * * *
  
  Name: fivetran_connection_setup_tests_schedule
  Job Name: fivetran_connection_setup_tests
  Cron Schedule: 0 0 * * *
  
  Name: gong_calls_transcript_job_schedule
  Job Name: gong_calls_transcript_job
  Cron Schedule: 0 0 * * *
  
  Name: google_search_console_daily_schedule
  Job Name: __anonymous_asset_job_google_search_console_daily_schedule
  Cron Schedule: 0 2 * * *
  
  Name: hightouch_syncs_schedule
  Job Name: __anonymous_asset_job_hightouch_syncs_schedule
  Cron Schedule: 0 3 * * *
  
  Name: hourly_hightouch_syncs_schedule
  Job Name: hourly_hightouch_syncs_job
  Cron Schedule: 0 * * * *
  
  Name: sequel_events_schedule
  Job Name: __anonymous_asset_job_sequel_events_schedule
  Cron Schedule: 0 1 * * *
  
  Name: sequel_registrants_schedule
  Job Name: __anonymous_asset_job_sequel_registrants_schedule
  Cron Schedule: 15 1 * * *
  
  Name: sequel_user_activity_logs_schedule
  Job Name: __anonymous_asset_job_sequel_user_activity_logs_schedule
  Cron Schedule: 30 1 * * *
  
  Name: sling_egress_job_schedule
  Job Name: sling_egress_job
  Cron Schedule: 0 3 * * *
  
  Name: statsig_upload_schedule
  Job Name: statsig_upload_job
  Cron Schedule: 0 1 * * *
  
  Name: stripe_data_sync_observe_job_schedule
  Job Name: stripe_data_sync_observe_job
  Cron Schedule: 0 0 * * *
  
  Name: support_bot_schedule
  Job Name: support_bot_job
  Cron Schedule: @daily
  
  Name: product_operations_clone_cleanup_job_schedule
  Job Name: product_operations_clone_cleanup_job
  Cron Schedule: 0 3 * * *
  
  Name: sync_insights_to_pg_schedule
  Job Name: sync_insights_to_pg_job
  Cron Schedule: 0 0 * * *
  
  Name: billing_issues_rollup_schedule
  Job Name: billing_issues_rollup
  Cron Schedule: 0 8 * * 1-5
  
  Name: daily_check_user_pipeline_status_job
  Job Name: check_user_pipeline_status_job
  Cron Schedule: 0 0 * * *
  
  Name: detect_self_serve_customers
  Job Name: detect_self_serve_customers
  Cron Schedule: 30 * * * *
  
  Name: self_serve_customer_attribution_check_all_partitions_job_schedule
  Job Name: self_serve_customer_attribution_check_all_partitions_job
  Cron Schedule: 0 7 * * *
  
  Name: snowflake_insights_import_schedule
  Job Name: snowflake_insights_import
  Cron Schedule: 59 * * * *
  
  Name: sync_enterprise_contract_metadata_to_pg_schedule
  Job Name: sync_enterprise_contract_metadata_to_pg_job
  Cron Schedule: 0 4 * * *
  
  Name: telemetry_events_to_snowflake_schedule
  Job Name: telemetry_events_to_snowflake
  Cron Schedule: 0 8 * * *
  
  Name: usage_attribution_schedule
  Job Name: self_serve_attribution_job
  Cron Schedule: 0 */3 * * *
  
  Name: dogfood_cypress_job_schedule
  Job Name: dogfood_cypress_job
  Cron Schedule: */10 * * * *
  
  Name: production_cypress_job_schedule
  Job Name: production_cypress_job
  Cron Schedule: */10 * * * *
  
  '''
# ---
# name: TestDynamicCommandExecution.test_command_execution[schedule_success_single_schedule_human_readable_text]
  '''
  Name: check_linear_issues_schedule
  Job Name: check_linear_issues
  Cron Schedule: 0 0 * * *
  Description: None
  Execution Timezone: UTC
  
  '''
# ---
# name: TestDynamicCommandExecution.test_command_execution[schedule_success_single_schedule_json]
  dict({
    'cron_schedule': '0 0 * * *',
    'description': None,
    'execution_timezone': 'UTC',
    'id': '7a1ff4e6d334c36613cf2668b32c9bc4740b4751::19b110ce646fe80a563955a7d064ece4e5772ff0',
    'metadata_entries': list([
    ]),
    'name': 'check_linear_issues_schedule',
    'pipeline_name': 'check_linear_issues',
    'tags': list([
    ]),
  })
# ---
