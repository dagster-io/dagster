# Example: Jobs with job-level parameters and job parameter references
# This demonstrates how the scaffolder extracts job-level parameters and preserves references

resources:
  jobs:
    databricks_pipeline_job:
      name: "Databricks Pipeline Job with Job Parameters"

      # Job-level parameters that will be extracted by the scaffolder and component
      parameters:
        environment: "{{ env }}"
        orchestrator_job_run_id: "{{ run_id }}"
        output_bucket_name: "{{ var.output_bucket_name }}"
        profiler_enabled: false
        experiment_settings: "mlflow://databricks"
        catalog_map: "{{ env }}.catalog"
        log_level: "INFO"
        llm_calls: "enabled"
        git_sha: "{{ git_sha }}"
        batch_size: 1000
        timeout_minutes: 30

      tasks:
        # Example 1: Notebook task with job parameter references
        - task_key: data_processing_notebook
          notebook_task:
            notebook_path: "/Users/{{ workspace_user }}/ml_pipelines/data_processing"
            base_parameters:
              input_table: "{{ env }}.raw.customer_data"
              output_table: "{{ env }}.processed.customer_data"
              processing_date: "{{ ds }}"
              # Reference job-level parameters using {{job.parameters.param_name}} syntax
              environment: "{{job.parameters.environment}}"
              run_id: "{{job.parameters.orchestrator_job_run_id}}"
              output_bucket: "{{job.parameters.output_bucket_name}}"
              profiler_enabled: "{{job.parameters.profiler_enabled}}"
              experiment_settings: "{{job.parameters.experiment_settings}}"
              catalog_map: "{{job.parameters.catalog_map}}"
              log_level: "{{job.parameters.log_level}}"
              llm_calls: "{{job.parameters.llm_calls}}"
              git_sha: "{{job.parameters.git_sha}}"
              batch_size: "{{job.parameters.batch_size}}"
              timeout_minutes: "{{job.parameters.timeout_minutes}}"
          libraries:
            - whl: "dbfs:/FileStore/wheels/data_processing-1.0.0-py3-none-any.whl"
            - pypi:
                package: "pandas"
                version: "2.0.0"

        # Example 2: Python wheel task with job parameter references in list format
        - task_key: stage_documents
          depends_on:
            - task_key: data_processing_notebook
          python_wheel_task:
            package_name: "ml_etl"
            entry_point: "main"
            parameters:
              [
                "--run_id={{job.parameters.orchestrator_job_run_id}}",
                "--module=ml_etl.projects.tune.tasks.test_task",
                "--class=TestTask",
                "--environment={{job.parameters.environment}}",
                "--profiler_enabled={{job.parameters.profiler_enabled}}",
                "--experiment_settings={{job.parameters.experiment_settings}}",
                "--catalog_map={{job.parameters.catalog_map}}",
                "--log_level={{job.parameters.log_level}}",
                "--llm_calls={{job.parameters.llm_calls}}",
                "--git_sha={{job.parameters.git_sha}}",
                "--batch_size={{job.parameters.batch_size}}",
                "--timeout_minutes={{job.parameters.timeout_minutes}}",
                "--input_table={{ env }}.processed.customer_data",
                "--output_table={{ env }}.features.customer_features",
              ]
          libraries:
            - whl: "dbfs:/FileStore/wheels/ml_processing-2.1.0-py3-none-any.whl"
            - pypi:
                package: "scikit-learn"
                version: "1.3.0"

        # Example 3: Spark JAR task with job parameter references
        - task_key: spark_processing_jar
          depends_on:
            - task_key: stage_documents
          spark_jar_task:
            main_class_name: "com.company.etl.DataProcessor"
            parameters:
              input_table: "{{ env }}.raw.transactions"
              output_table: "{{ env }}.processed.transactions"
              processing_date: "{{ ds }}"
              # Mix of direct values and job parameter references
              environment: "{{job.parameters.environment}}"
              run_id: "{{job.parameters.orchestrator_job_run_id}}"
              output_bucket: "{{job.parameters.output_bucket_name}}"
              profiler_enabled: "{{job.parameters.profiler_enabled}}"
              batch_size: "{{job.parameters.batch_size}}"
              timeout_minutes: "{{job.parameters.timeout_minutes}}"
              experiment_settings: "{{job.parameters.experiment_settings}}"
              catalog_map: "{{job.parameters.catalog_map}}"
              log_level: "{{job.parameters.log_level}}"
              llm_calls: "{{job.parameters.llm_calls}}"
              git_sha: "{{job.parameters.git_sha}}"
          libraries:
            - jar: "dbfs:/FileStore/jars/data_processor-1.0.0.jar"
            - maven:
                coordinates: "org.apache.spark:spark-streaming_2.12:3.4.0"

        # Example 4: Run existing job with job parameter references
        - task_key: existing_job_with_references
          depends_on:
            - task_key: spark_processing_jar
          run_job_task:
            job_id: 123456789
            job_parameters:
              input_table: "{{ env }}.processed.transactions"
              output_table: "{{ env }}.analytics.daily_summary"
              summary_date: "{{ ds }}"
              # Reference job-level parameters in job parameters
              environment: "{{job.parameters.environment}}"
              run_id: "{{job.parameters.orchestrator_job_run_id}}"
              output_bucket: "{{job.parameters.output_bucket_name}}"
              profiler_enabled: "{{job.parameters.profiler_enabled}}"
              experiment_settings: "{{job.parameters.experiment_settings}}"
              catalog_map: "{{job.parameters.catalog_map}}"
              log_level: "{{job.parameters.log_level}}"
              llm_calls: "{{job.parameters.llm_calls}}"
              git_sha: "{{job.parameters.git_sha}}"
              batch_size: "{{job.parameters.batch_size}}"
              timeout_minutes: "{{job.parameters.timeout_minutes}}"

        # Example 5: Condition task
        - task_key: check_data_quality
          depends_on:
            - task_key: existing_job_with_references
          condition_task:
            op: "GREATER_THAN"
            left: "{{ tasks.existing_job_with_references.outputs.records_processed }}"
            right: "{{job.parameters.batch_size}}"

        # Example 6: Spark Python task
        - task_key: "hello_world_spark_task"
          new_cluster:
            spark_version: "13.3.x-scala2.12"
            num_workers: 1 # 0 = serverless Spark compute
            node_type_id: "i3.xlarge"
            data_security_mode: "SINGLE_USER"
            runtime_engine: "PHOTON"
          spark_python_task:
            python_file: "../spark_example/hello_world_spark.py"
            parameters:
              - "Hello from Databricks Bundle!"
              - "{{ bundle.target }}"
          libraries:
            - pypi:
                package: "pyspark>=3.3.0"
