{"data": {"pipelineOrError": {"description": null, "modes": [{"description": null, "loggers": [{"configField": {"configType": {"description": "A configuration dictionary with typed fields", "fields": [{"configType": {"key": "String"}, "description": null, "isOptional": true, "name": "log_level"}, {"configType": {"key": "String"}, "description": null, "isOptional": true, "name": "name"}], "innerTypes": [{"description": "", "innerTypes": [], "isList": false, "isNullable": false, "isSelector": false, "key": "String", "name": "String"}], "isList": false, "isNullable": false, "isSelector": false, "key": "Dict.25", "name": null}}, "description": "The default colored console logger.", "name": "console"}], "name": "local", "resources": [{"configField": {"configType": {"description": "A configuration dictionary with typed fields", "fields": [{"configType": {"key": "String"}, "description": null, "isOptional": false, "name": "postgres_db_name"}, {"configType": {"key": "String"}, "description": null, "isOptional": false, "name": "postgres_hostname"}, {"configType": {"key": "String"}, "description": null, "isOptional": false, "name": "postgres_password"}, {"configType": {"key": "String"}, "description": null, "isOptional": false, "name": "postgres_username"}], "innerTypes": [{"description": "", "innerTypes": [], "isList": false, "isNullable": false, "isSelector": false, "key": "String", "name": "String"}], "isList": false, "isNullable": false, "isSelector": false, "key": "Dict.44", "name": null}}, "description": null, "name": "db_info"}, {"configField": {"configType": {"description": "A configuration dictionary with typed fields", "fields": [{"configType": {"key": "Bool"}, "description": null, "isOptional": true, "name": "overwrite"}, {"configType": {"key": "String"}, "description": null, "isOptional": false, "name": "target_folder"}], "innerTypes": [{"description": "", "innerTypes": [], "isList": false, "isNullable": false, "isSelector": false, "key": "String", "name": "String"}, {"description": "", "innerTypes": [], "isList": false, "isNullable": false, "isSelector": false, "key": "Bool", "name": "Bool"}], "isList": false, "isNullable": false, "isSelector": false, "key": "Dict.36", "name": null}}, "description": null, "name": "file_cache"}, {"configField": {"configType": {"description": "A configuration dictionary with typed fields", "fields": [{"configType": {"key": "Bool"}, "description": "Specifies whether to use an unsigned S3 session", "isOptional": true, "name": "use_unsigned_session"}], "innerTypes": [{"description": "", "innerTypes": [], "isList": false, "isNullable": false, "isSelector": false, "key": "Bool", "name": "Bool"}], "isList": false, "isNullable": false, "isSelector": false, "key": "Dict.38", "name": null}}, "description": null, "name": "s3"}, {"configField": null, "description": null, "name": "spark"}, {"configField": null, "description": null, "name": "tempfile"}]}, {"description": null, "loggers": [{"configField": {"configType": {"description": "A configuration dictionary with typed fields", "fields": [{"configType": {"key": "String"}, "description": null, "isOptional": true, "name": "log_level"}, {"configType": {"key": "String"}, "description": null, "isOptional": true, "name": "name"}], "innerTypes": [{"description": "", "innerTypes": [], "isList": false, "isNullable": false, "isSelector": false, "key": "String", "name": "String"}], "isList": false, "isNullable": false, "isSelector": false, "key": "Dict.25", "name": null}}, "description": "The default colored console logger.", "name": "console"}], "name": "prod", "resources": [{"configField": {"configType": {"description": "A configuration dictionary with typed fields", "fields": [{"configType": {"key": "String"}, "description": null, "isOptional": false, "name": "redshift_db_name"}, {"configType": {"key": "String"}, "description": null, "isOptional": false, "name": "redshift_hostname"}, {"configType": {"key": "String"}, "description": null, "isOptional": false, "name": "redshift_password"}, {"configType": {"key": "String"}, "description": null, "isOptional": false, "name": "redshift_username"}, {"configType": {"key": "String"}, "description": null, "isOptional": false, "name": "s3_temp_dir"}], "innerTypes": [{"description": "", "innerTypes": [], "isList": false, "isNullable": false, "isSelector": false, "key": "String", "name": "String"}], "isList": false, "isNullable": false, "isSelector": false, "key": "Dict.43", "name": null}}, "description": null, "name": "db_info"}, {"configField": {"configType": {"description": "A configuration dictionary with typed fields", "fields": [{"configType": {"key": "String"}, "description": null, "isOptional": false, "name": "bucket"}, {"configType": {"key": "String"}, "description": null, "isOptional": false, "name": "key"}, {"configType": {"key": "Bool"}, "description": null, "isOptional": true, "name": "overwrite"}], "innerTypes": [{"description": "", "innerTypes": [], "isList": false, "isNullable": false, "isSelector": false, "key": "String", "name": "String"}, {"description": "", "innerTypes": [], "isList": false, "isNullable": false, "isSelector": false, "key": "Bool", "name": "Bool"}], "isList": false, "isNullable": false, "isSelector": false, "key": "Dict.37", "name": null}}, "description": null, "name": "file_cache"}, {"configField": {"configType": {"description": "A configuration dictionary with typed fields", "fields": [{"configType": {"key": "Bool"}, "description": "Specifies whether to use an unsigned S3 session", "isOptional": true, "name": "use_unsigned_session"}], "innerTypes": [{"description": "", "innerTypes": [], "isList": false, "isNullable": false, "isSelector": false, "key": "Bool", "name": "Bool"}], "isList": false, "isNullable": false, "isSelector": false, "key": "Dict.38", "name": null}}, "description": null, "name": "s3"}, {"configField": null, "description": null, "name": "spark"}, {"configField": null, "description": null, "name": "tempfile"}]}, {"description": null, "loggers": [{"configField": {"configType": {"description": "A configuration dictionary with typed fields", "fields": [{"configType": {"key": "String"}, "description": null, "isOptional": true, "name": "log_level"}, {"configType": {"key": "String"}, "description": null, "isOptional": true, "name": "name"}], "innerTypes": [{"description": "", "innerTypes": [], "isList": false, "isNullable": false, "isSelector": false, "key": "String", "name": "String"}], "isList": false, "isNullable": false, "isSelector": false, "key": "Dict.25", "name": null}}, "description": "The default colored console logger.", "name": "console"}], "name": "test", "resources": [{"configField": {"configType": {"description": "A configuration dictionary with typed fields", "fields": [{"configType": {"key": "String"}, "description": null, "isOptional": false, "name": "redshift_db_name"}, {"configType": {"key": "String"}, "description": null, "isOptional": false, "name": "redshift_hostname"}, {"configType": {"key": "String"}, "description": null, "isOptional": false, "name": "redshift_password"}, {"configType": {"key": "String"}, "description": null, "isOptional": false, "name": "redshift_username"}, {"configType": {"key": "String"}, "description": null, "isOptional": false, "name": "s3_temp_dir"}], "innerTypes": [{"description": "", "innerTypes": [], "isList": false, "isNullable": false, "isSelector": false, "key": "String", "name": "String"}], "isList": false, "isNullable": false, "isSelector": false, "key": "Dict.43", "name": null}}, "description": null, "name": "db_info"}, {"configField": {"configType": {"description": "A configuration dictionary with typed fields", "fields": [{"configType": {"key": "Bool"}, "description": null, "isOptional": true, "name": "overwrite"}, {"configType": {"key": "String"}, "description": null, "isOptional": false, "name": "target_folder"}], "innerTypes": [{"description": "", "innerTypes": [], "isList": false, "isNullable": false, "isSelector": false, "key": "String", "name": "String"}, {"description": "", "innerTypes": [], "isList": false, "isNullable": false, "isSelector": false, "key": "Bool", "name": "Bool"}], "isList": false, "isNullable": false, "isSelector": false, "key": "Dict.36", "name": null}}, "description": null, "name": "file_cache"}, {"configField": {"configType": {"description": "A configuration dictionary with typed fields", "fields": [{"configType": {"key": "Bool"}, "description": "Specifies whether to use an unsigned S3 session", "isOptional": true, "name": "use_unsigned_session"}], "innerTypes": [{"description": "", "innerTypes": [], "isList": false, "isNullable": false, "isSelector": false, "key": "Bool", "name": "Bool"}], "isList": false, "isNullable": false, "isSelector": false, "key": "Dict.38", "name": null}}, "description": null, "name": "s3"}, {"configField": null, "description": null, "name": "spark"}, {"configField": null, "description": null, "name": "tempfile"}]}], "name": "airline_demo_ingest_pipeline", "solidHandles": [{"handleID": "april_on_time_s3_to_df", "parent": null, "solid": {"definition": {"__typename": "CompositeSolidDefinition", "description": "Ingest a zipped csv file from s3,\nstash in a keyed file store (does not download if already\npresent by default), unzip that file, and load it into a\nSpark Dataframe. See documentation in constituent solids for\nmore detail.", "inputMappings": [{"definition": {"name": "s3_coordinate"}, "mappedInput": {"definition": {"name": "s3_coordinate"}, "solid": {"name": "cache_file_from_s3"}}}, {"definition": {"name": "archive_member"}, "mappedInput": {"definition": {"name": "archive_member"}, "solid": {"name": "unzip_file_handle"}}}], "metadata": [], "name": "s3_to_df", "outputMappings": [{"definition": {"name": "result"}, "mappedOutput": {"definition": {"name": "result"}, "solid": {"name": "ingest_csv_file_handle_to_spark"}}}], "requiredResources": [{"resourceKey": "spark"}, {"resourceKey": "file_cache"}, {"resourceKey": "s3"}]}, "inputs": [{"definition": {"description": null, "name": "s3_coordinate", "type": {"description": null, "displayName": "S3Coordinate", "name": "S3Coordinate"}}, "dependsOn": []}, {"definition": {"description": null, "name": "archive_member", "type": {"description": null, "displayName": "String", "name": "String"}}, "dependsOn": []}], "name": "april_on_time_s3_to_df", "outputs": [{"definition": {"description": null, "name": "result", "type": {"description": "A Pyspark data frame.", "displayName": "DataFrame", "name": "DataFrame"}}, "dependedBy": [{"definition": {"name": "april_data", "type": {"displayName": "DataFrame"}}, "solid": {"name": "join_q2_data"}}]}]}}, {"handleID": "april_on_time_s3_to_df.cache_file_from_s3", "parent": {"handleID": "april_on_time_s3_to_df"}, "solid": {"definition": {"__typename": "SolidDefinition", "configDefinition": {"configType": {"description": "A configuration dictionary with typed fields", "fields": [{"configType": {"key": "String"}, "description": "Optionally specify the key for the file to be ingested into the keyed store. Defaults to the last path component of the downloaded s3 key.", "isOptional": true, "name": "file_key"}], "innerTypes": [{"description": "", "innerTypes": [], "isList": false, "isNullable": false, "isSelector": false, "key": "String", "name": "String"}], "isList": false, "isNullable": false, "isSelector": false, "key": "Dict.42", "name": null}}, "description": "This is a solid which caches a file in s3 into file cache.\n\nThe `file_cache` is a resource type that allows a solid author to save files\nand assign a key to them. The keyed file store can be backed by local file or any\nobject store (currently we support s3). This keyed file store can be configured\nto be at an external location so that is persists in a well known spot between runs.\nIt is designed for the case where there is an expensive download step that should not\noccur unless the downloaded file does not exist. Redownload can be instigated either\nby configuring the source to overwrite files or to just delete the file in the underlying\nstorage manually.\n\nThis works by downloading the file to a temporary file, and then ingesting it into\nthe file cache. In the case of a filesystem-backed file cache, this is a file\ncopy. In the case of a object-store-backed file cache, this is an upload.\n\nIn order to work this must be executed within a mode that provides an `s3`\nand `file_cache` resource.\n    ", "metadata": [], "name": "cache_file_from_s3", "requiredResources": [{"resourceKey": "file_cache"}, {"resourceKey": "s3"}]}, "inputs": [{"definition": {"description": null, "name": "s3_coordinate", "type": {"description": null, "displayName": "S3Coordinate", "name": "S3Coordinate"}}, "dependsOn": []}], "name": "cache_file_from_s3", "outputs": [{"definition": {"description": null, "name": "result", "type": {"description": "A file handle a reference to a file, which could be resident in the local file\n    system, an object store, or any arbitrary place where file can be stored.\n\n    This exists to handle the very common case where you wish to write a computation that reads,\n    transforms, and writes files, but is written in a way where the same code could work in local\n    developement as well as in a cluster where the files would be stored in globally available\n    object store such as s3.\n    ", "displayName": "FileHandle", "name": "FileHandle"}}, "dependedBy": [{"definition": {"name": "archive_file_handle", "type": {"displayName": "FileHandle"}}, "solid": {"name": "unzip_file_handle"}}]}]}}, {"handleID": "april_on_time_s3_to_df.ingest_csv_file_handle_to_spark", "parent": {"handleID": "april_on_time_s3_to_df"}, "solid": {"definition": {"__typename": "SolidDefinition", "configDefinition": null, "description": "Take a file handle that contains a csv with headers and load it\ninto a Spark DataFrame. It infers header names but does *not* infer schema.\n\nIt also ensures that the column names are valid parquet column names by\nfiltering out any of the following characters from column names:\n\nCharacters (within quotations): \"`[ ,;{}()\\n\\t=]`\"\n\n", "metadata": [], "name": "ingest_csv_file_handle_to_spark", "requiredResources": [{"resourceKey": "spark"}]}, "inputs": [{"definition": {"description": null, "name": "csv_file_handle", "type": {"description": "A file handle a reference to a file, which could be resident in the local file\n    system, an object store, or any arbitrary place where file can be stored.\n\n    This exists to handle the very common case where you wish to write a computation that reads,\n    transforms, and writes files, but is written in a way where the same code could work in local\n    developement as well as in a cluster where the files would be stored in globally available\n    object store such as s3.\n    ", "displayName": "FileHandle", "name": "FileHandle"}}, "dependsOn": [{"definition": {"name": "result", "type": {"displayName": "FileHandle"}}, "solid": {"name": "unzip_file_handle"}}]}], "name": "ingest_csv_file_handle_to_spark", "outputs": [{"definition": {"description": null, "name": "result", "type": {"description": "A Pyspark data frame.", "displayName": "DataFrame", "name": "DataFrame"}}, "dependedBy": []}]}}, {"handleID": "april_on_time_s3_to_df.unzip_file_handle", "parent": {"handleID": "april_on_time_s3_to_df"}, "solid": {"definition": {"__typename": "SolidDefinition", "configDefinition": null, "description": "Unzip a file that is resident in an archive file as a member.\n    This solid operates on FileHandles, meaning that their physical is dependent\n    on what system storage is operating in the pipeline. The physical file could\n    be on local disk, or it could be in s3. If on s3, this solid will download\n    that file to local disk, perform the unzip, upload that file back to s3, and\n    then return that file handle for downstream use in the computations.\n    ", "metadata": [], "name": "unzip_file_handle", "requiredResources": []}, "inputs": [{"definition": {"description": null, "name": "archive_file_handle", "type": {"description": "A file handle a reference to a file, which could be resident in the local file\n    system, an object store, or any arbitrary place where file can be stored.\n\n    This exists to handle the very common case where you wish to write a computation that reads,\n    transforms, and writes files, but is written in a way where the same code could work in local\n    developement as well as in a cluster where the files would be stored in globally available\n    object store such as s3.\n    ", "displayName": "FileHandle", "name": "FileHandle"}}, "dependsOn": [{"definition": {"name": "result", "type": {"displayName": "FileHandle"}}, "solid": {"name": "cache_file_from_s3"}}]}, {"definition": {"description": null, "name": "archive_member", "type": {"description": null, "displayName": "String", "name": "String"}}, "dependsOn": []}], "name": "unzip_file_handle", "outputs": [{"definition": {"description": null, "name": "result", "type": {"description": "A file handle a reference to a file, which could be resident in the local file\n    system, an object store, or any arbitrary place where file can be stored.\n\n    This exists to handle the very common case where you wish to write a computation that reads,\n    transforms, and writes files, but is written in a way where the same code could work in local\n    developement as well as in a cluster where the files would be stored in globally available\n    object store such as s3.\n    ", "displayName": "FileHandle", "name": "FileHandle"}}, "dependedBy": [{"definition": {"name": "csv_file_handle", "type": {"displayName": "FileHandle"}}, "solid": {"name": "ingest_csv_file_handle_to_spark"}}]}]}}, {"handleID": "download_q2_sfo_weather", "parent": null, "solid": {"definition": {"__typename": "SolidDefinition", "configDefinition": {"configType": {"description": "A configuration dictionary with typed fields", "fields": [{"configType": {"key": "String"}, "description": "Optionally specify the key for the file to be ingested into the keyed store. Defaults to the last path component of the downloaded s3 key.", "isOptional": true, "name": "file_key"}], "innerTypes": [{"description": "", "innerTypes": [], "isList": false, "isNullable": false, "isSelector": false, "key": "String", "name": "String"}], "isList": false, "isNullable": false, "isSelector": false, "key": "Dict.42", "name": null}}, "description": "This is a solid which caches a file in s3 into file cache.\n\nThe `file_cache` is a resource type that allows a solid author to save files\nand assign a key to them. The keyed file store can be backed by local file or any\nobject store (currently we support s3). This keyed file store can be configured\nto be at an external location so that is persists in a well known spot between runs.\nIt is designed for the case where there is an expensive download step that should not\noccur unless the downloaded file does not exist. Redownload can be instigated either\nby configuring the source to overwrite files or to just delete the file in the underlying\nstorage manually.\n\nThis works by downloading the file to a temporary file, and then ingesting it into\nthe file cache. In the case of a filesystem-backed file cache, this is a file\ncopy. In the case of a object-store-backed file cache, this is an upload.\n\nIn order to work this must be executed within a mode that provides an `s3`\nand `file_cache` resource.\n    ", "metadata": [], "name": "cache_file_from_s3", "requiredResources": [{"resourceKey": "file_cache"}, {"resourceKey": "s3"}]}, "inputs": [{"definition": {"description": null, "name": "s3_coordinate", "type": {"description": null, "displayName": "S3Coordinate", "name": "S3Coordinate"}}, "dependsOn": []}], "name": "download_q2_sfo_weather", "outputs": [{"definition": {"description": null, "name": "result", "type": {"description": "A file handle a reference to a file, which could be resident in the local file\n    system, an object store, or any arbitrary place where file can be stored.\n\n    This exists to handle the very common case where you wish to write a computation that reads,\n    transforms, and writes files, but is written in a way where the same code could work in local\n    developement as well as in a cluster where the files would be stored in globally available\n    object store such as s3.\n    ", "displayName": "FileHandle", "name": "FileHandle"}}, "dependedBy": [{"definition": {"name": "csv_file_handle", "type": {"displayName": "FileHandle"}}, "solid": {"name": "ingest_q2_sfo_weather"}}]}]}}, {"handleID": "ingest_q2_sfo_weather", "parent": null, "solid": {"definition": {"__typename": "SolidDefinition", "configDefinition": null, "description": "Take a file handle that contains a csv with headers and load it\ninto a Spark DataFrame. It infers header names but does *not* infer schema.\n\nIt also ensures that the column names are valid parquet column names by\nfiltering out any of the following characters from column names:\n\nCharacters (within quotations): \"`[ ,;{}()\\n\\t=]`\"\n\n", "metadata": [], "name": "ingest_csv_file_handle_to_spark", "requiredResources": [{"resourceKey": "spark"}]}, "inputs": [{"definition": {"description": null, "name": "csv_file_handle", "type": {"description": "A file handle a reference to a file, which could be resident in the local file\n    system, an object store, or any arbitrary place where file can be stored.\n\n    This exists to handle the very common case where you wish to write a computation that reads,\n    transforms, and writes files, but is written in a way where the same code could work in local\n    developement as well as in a cluster where the files would be stored in globally available\n    object store such as s3.\n    ", "displayName": "FileHandle", "name": "FileHandle"}}, "dependsOn": [{"definition": {"name": "result", "type": {"displayName": "FileHandle"}}, "solid": {"name": "download_q2_sfo_weather"}}]}], "name": "ingest_q2_sfo_weather", "outputs": [{"definition": {"description": null, "name": "result", "type": {"description": "A Pyspark data frame.", "displayName": "DataFrame", "name": "DataFrame"}}, "dependedBy": [{"definition": {"name": "sfo_weather_data", "type": {"displayName": "DataFrame"}}, "solid": {"name": "process_sfo_weather_data"}}]}]}}, {"handleID": "join_q2_data", "parent": null, "solid": {"definition": {"__typename": "SolidDefinition", "configDefinition": {"configType": {"description": "A configuration dictionary with typed fields", "fields": [{"configType": {"key": "Int"}, "description": "", "isOptional": false, "name": "subsample_pct"}], "innerTypes": [{"description": "", "innerTypes": [], "isList": false, "isNullable": false, "isSelector": false, "key": "Int", "name": "Int"}], "isList": false, "isNullable": false, "isSelector": false, "key": "Dict.163", "name": null}}, "description": "\n    This solid takes April, May, and June data and coalesces it into a q2 data set.\n    It then joins the that origin and destination airport with the data in the\n    master_cord_data.\n    ", "metadata": [], "name": "join_q2_data", "requiredResources": []}, "inputs": [{"definition": {"description": null, "name": "april_data", "type": {"description": "A Pyspark data frame.", "displayName": "DataFrame", "name": "DataFrame"}}, "dependsOn": [{"definition": {"name": "result", "type": {"displayName": "DataFrame"}}, "solid": {"name": "april_on_time_s3_to_df"}}]}, {"definition": {"description": null, "name": "may_data", "type": {"description": "A Pyspark data frame.", "displayName": "DataFrame", "name": "DataFrame"}}, "dependsOn": [{"definition": {"name": "result", "type": {"displayName": "DataFrame"}}, "solid": {"name": "may_on_time_s3_to_df"}}]}, {"definition": {"description": null, "name": "june_data", "type": {"description": "A Pyspark data frame.", "displayName": "DataFrame", "name": "DataFrame"}}, "dependsOn": [{"definition": {"name": "result", "type": {"displayName": "DataFrame"}}, "solid": {"name": "june_on_time_s3_to_df"}}]}, {"definition": {"description": null, "name": "master_cord_data", "type": {"description": "A Pyspark data frame.", "displayName": "DataFrame", "name": "DataFrame"}}, "dependsOn": [{"definition": {"name": "result", "type": {"displayName": "DataFrame"}}, "solid": {"name": "master_cord_s3_to_df"}}]}], "name": "join_q2_data", "outputs": [{"definition": {"description": null, "name": "result", "type": {"description": "A Pyspark data frame.", "displayName": "DataFrame", "name": "DataFrame"}}, "dependedBy": [{"definition": {"name": "data_frame", "type": {"displayName": "DataFrame"}}, "solid": {"name": "load_q2_on_time_data"}}]}]}}, {"handleID": "june_on_time_s3_to_df", "parent": null, "solid": {"definition": {"__typename": "CompositeSolidDefinition", "description": "Ingest a zipped csv file from s3,\nstash in a keyed file store (does not download if already\npresent by default), unzip that file, and load it into a\nSpark Dataframe. See documentation in constituent solids for\nmore detail.", "inputMappings": [{"definition": {"name": "s3_coordinate"}, "mappedInput": {"definition": {"name": "s3_coordinate"}, "solid": {"name": "cache_file_from_s3"}}}, {"definition": {"name": "archive_member"}, "mappedInput": {"definition": {"name": "archive_member"}, "solid": {"name": "unzip_file_handle"}}}], "metadata": [], "name": "s3_to_df", "outputMappings": [{"definition": {"name": "result"}, "mappedOutput": {"definition": {"name": "result"}, "solid": {"name": "ingest_csv_file_handle_to_spark"}}}], "requiredResources": [{"resourceKey": "spark"}, {"resourceKey": "file_cache"}, {"resourceKey": "s3"}]}, "inputs": [{"definition": {"description": null, "name": "s3_coordinate", "type": {"description": null, "displayName": "S3Coordinate", "name": "S3Coordinate"}}, "dependsOn": []}, {"definition": {"description": null, "name": "archive_member", "type": {"description": null, "displayName": "String", "name": "String"}}, "dependsOn": []}], "name": "june_on_time_s3_to_df", "outputs": [{"definition": {"description": null, "name": "result", "type": {"description": "A Pyspark data frame.", "displayName": "DataFrame", "name": "DataFrame"}}, "dependedBy": [{"definition": {"name": "june_data", "type": {"displayName": "DataFrame"}}, "solid": {"name": "join_q2_data"}}]}]}}, {"handleID": "june_on_time_s3_to_df.cache_file_from_s3", "parent": {"handleID": "june_on_time_s3_to_df"}, "solid": {"definition": {"__typename": "SolidDefinition", "configDefinition": {"configType": {"description": "A configuration dictionary with typed fields", "fields": [{"configType": {"key": "String"}, "description": "Optionally specify the key for the file to be ingested into the keyed store. Defaults to the last path component of the downloaded s3 key.", "isOptional": true, "name": "file_key"}], "innerTypes": [{"description": "", "innerTypes": [], "isList": false, "isNullable": false, "isSelector": false, "key": "String", "name": "String"}], "isList": false, "isNullable": false, "isSelector": false, "key": "Dict.42", "name": null}}, "description": "This is a solid which caches a file in s3 into file cache.\n\nThe `file_cache` is a resource type that allows a solid author to save files\nand assign a key to them. The keyed file store can be backed by local file or any\nobject store (currently we support s3). This keyed file store can be configured\nto be at an external location so that is persists in a well known spot between runs.\nIt is designed for the case where there is an expensive download step that should not\noccur unless the downloaded file does not exist. Redownload can be instigated either\nby configuring the source to overwrite files or to just delete the file in the underlying\nstorage manually.\n\nThis works by downloading the file to a temporary file, and then ingesting it into\nthe file cache. In the case of a filesystem-backed file cache, this is a file\ncopy. In the case of a object-store-backed file cache, this is an upload.\n\nIn order to work this must be executed within a mode that provides an `s3`\nand `file_cache` resource.\n    ", "metadata": [], "name": "cache_file_from_s3", "requiredResources": [{"resourceKey": "file_cache"}, {"resourceKey": "s3"}]}, "inputs": [{"definition": {"description": null, "name": "s3_coordinate", "type": {"description": null, "displayName": "S3Coordinate", "name": "S3Coordinate"}}, "dependsOn": []}], "name": "cache_file_from_s3", "outputs": [{"definition": {"description": null, "name": "result", "type": {"description": "A file handle a reference to a file, which could be resident in the local file\n    system, an object store, or any arbitrary place where file can be stored.\n\n    This exists to handle the very common case where you wish to write a computation that reads,\n    transforms, and writes files, but is written in a way where the same code could work in local\n    developement as well as in a cluster where the files would be stored in globally available\n    object store such as s3.\n    ", "displayName": "FileHandle", "name": "FileHandle"}}, "dependedBy": [{"definition": {"name": "archive_file_handle", "type": {"displayName": "FileHandle"}}, "solid": {"name": "unzip_file_handle"}}]}]}}, {"handleID": "june_on_time_s3_to_df.ingest_csv_file_handle_to_spark", "parent": {"handleID": "june_on_time_s3_to_df"}, "solid": {"definition": {"__typename": "SolidDefinition", "configDefinition": null, "description": "Take a file handle that contains a csv with headers and load it\ninto a Spark DataFrame. It infers header names but does *not* infer schema.\n\nIt also ensures that the column names are valid parquet column names by\nfiltering out any of the following characters from column names:\n\nCharacters (within quotations): \"`[ ,;{}()\\n\\t=]`\"\n\n", "metadata": [], "name": "ingest_csv_file_handle_to_spark", "requiredResources": [{"resourceKey": "spark"}]}, "inputs": [{"definition": {"description": null, "name": "csv_file_handle", "type": {"description": "A file handle a reference to a file, which could be resident in the local file\n    system, an object store, or any arbitrary place where file can be stored.\n\n    This exists to handle the very common case where you wish to write a computation that reads,\n    transforms, and writes files, but is written in a way where the same code could work in local\n    developement as well as in a cluster where the files would be stored in globally available\n    object store such as s3.\n    ", "displayName": "FileHandle", "name": "FileHandle"}}, "dependsOn": [{"definition": {"name": "result", "type": {"displayName": "FileHandle"}}, "solid": {"name": "unzip_file_handle"}}]}], "name": "ingest_csv_file_handle_to_spark", "outputs": [{"definition": {"description": null, "name": "result", "type": {"description": "A Pyspark data frame.", "displayName": "DataFrame", "name": "DataFrame"}}, "dependedBy": []}]}}, {"handleID": "june_on_time_s3_to_df.unzip_file_handle", "parent": {"handleID": "june_on_time_s3_to_df"}, "solid": {"definition": {"__typename": "SolidDefinition", "configDefinition": null, "description": "Unzip a file that is resident in an archive file as a member.\n    This solid operates on FileHandles, meaning that their physical is dependent\n    on what system storage is operating in the pipeline. The physical file could\n    be on local disk, or it could be in s3. If on s3, this solid will download\n    that file to local disk, perform the unzip, upload that file back to s3, and\n    then return that file handle for downstream use in the computations.\n    ", "metadata": [], "name": "unzip_file_handle", "requiredResources": []}, "inputs": [{"definition": {"description": null, "name": "archive_file_handle", "type": {"description": "A file handle a reference to a file, which could be resident in the local file\n    system, an object store, or any arbitrary place where file can be stored.\n\n    This exists to handle the very common case where you wish to write a computation that reads,\n    transforms, and writes files, but is written in a way where the same code could work in local\n    developement as well as in a cluster where the files would be stored in globally available\n    object store such as s3.\n    ", "displayName": "FileHandle", "name": "FileHandle"}}, "dependsOn": [{"definition": {"name": "result", "type": {"displayName": "FileHandle"}}, "solid": {"name": "cache_file_from_s3"}}]}, {"definition": {"description": null, "name": "archive_member", "type": {"description": null, "displayName": "String", "name": "String"}}, "dependsOn": []}], "name": "unzip_file_handle", "outputs": [{"definition": {"description": null, "name": "result", "type": {"description": "A file handle a reference to a file, which could be resident in the local file\n    system, an object store, or any arbitrary place where file can be stored.\n\n    This exists to handle the very common case where you wish to write a computation that reads,\n    transforms, and writes files, but is written in a way where the same code could work in local\n    developement as well as in a cluster where the files would be stored in globally available\n    object store such as s3.\n    ", "displayName": "FileHandle", "name": "FileHandle"}}, "dependedBy": [{"definition": {"name": "csv_file_handle", "type": {"displayName": "FileHandle"}}, "solid": {"name": "ingest_csv_file_handle_to_spark"}}]}]}}, {"handleID": "load_q2_on_time_data", "parent": null, "solid": {"definition": {"__typename": "SolidDefinition", "configDefinition": {"configType": {"description": "A configuration dictionary with typed fields", "fields": [{"configType": {"key": "String"}, "description": "", "isOptional": false, "name": "table_name"}], "innerTypes": [{"description": "", "innerTypes": [], "isList": false, "isNullable": false, "isSelector": false, "key": "String", "name": "String"}], "isList": false, "isNullable": false, "isSelector": false, "key": "Dict.160", "name": null}}, "description": null, "metadata": [], "name": "load_data_to_database_from_spark", "requiredResources": []}, "inputs": [{"definition": {"description": null, "name": "data_frame", "type": {"description": "A Pyspark data frame.", "displayName": "DataFrame", "name": "DataFrame"}}, "dependsOn": [{"definition": {"name": "result", "type": {"displayName": "DataFrame"}}, "solid": {"name": "join_q2_data"}}]}], "name": "load_q2_on_time_data", "outputs": [{"definition": {"description": null, "name": "table_name", "type": {"description": null, "displayName": "String", "name": "String"}}, "dependedBy": []}]}}, {"handleID": "load_q2_sfo_weather", "parent": null, "solid": {"definition": {"__typename": "SolidDefinition", "configDefinition": {"configType": {"description": "A configuration dictionary with typed fields", "fields": [{"configType": {"key": "String"}, "description": "", "isOptional": false, "name": "table_name"}], "innerTypes": [{"description": "", "innerTypes": [], "isList": false, "isNullable": false, "isSelector": false, "key": "String", "name": "String"}], "isList": false, "isNullable": false, "isSelector": false, "key": "Dict.160", "name": null}}, "description": null, "metadata": [], "name": "load_data_to_database_from_spark", "requiredResources": []}, "inputs": [{"definition": {"description": null, "name": "data_frame", "type": {"description": "A Pyspark data frame.", "displayName": "DataFrame", "name": "DataFrame"}}, "dependsOn": [{"definition": {"name": "result", "type": {"displayName": "DataFrame"}}, "solid": {"name": "process_sfo_weather_data"}}]}], "name": "load_q2_sfo_weather", "outputs": [{"definition": {"description": null, "name": "table_name", "type": {"description": null, "displayName": "String", "name": "String"}}, "dependedBy": []}]}}, {"handleID": "master_cord_s3_to_df", "parent": null, "solid": {"definition": {"__typename": "CompositeSolidDefinition", "description": "Ingest a zipped csv file from s3,\nstash in a keyed file store (does not download if already\npresent by default), unzip that file, and load it into a\nSpark Dataframe. See documentation in constituent solids for\nmore detail.", "inputMappings": [{"definition": {"name": "s3_coordinate"}, "mappedInput": {"definition": {"name": "s3_coordinate"}, "solid": {"name": "cache_file_from_s3"}}}, {"definition": {"name": "archive_member"}, "mappedInput": {"definition": {"name": "archive_member"}, "solid": {"name": "unzip_file_handle"}}}], "metadata": [], "name": "s3_to_df", "outputMappings": [{"definition": {"name": "result"}, "mappedOutput": {"definition": {"name": "result"}, "solid": {"name": "ingest_csv_file_handle_to_spark"}}}], "requiredResources": [{"resourceKey": "spark"}, {"resourceKey": "file_cache"}, {"resourceKey": "s3"}]}, "inputs": [{"definition": {"description": null, "name": "s3_coordinate", "type": {"description": null, "displayName": "S3Coordinate", "name": "S3Coordinate"}}, "dependsOn": []}, {"definition": {"description": null, "name": "archive_member", "type": {"description": null, "displayName": "String", "name": "String"}}, "dependsOn": []}], "name": "master_cord_s3_to_df", "outputs": [{"definition": {"description": null, "name": "result", "type": {"description": "A Pyspark data frame.", "displayName": "DataFrame", "name": "DataFrame"}}, "dependedBy": [{"definition": {"name": "master_cord_data", "type": {"displayName": "DataFrame"}}, "solid": {"name": "join_q2_data"}}]}]}}, {"handleID": "master_cord_s3_to_df.cache_file_from_s3", "parent": {"handleID": "master_cord_s3_to_df"}, "solid": {"definition": {"__typename": "SolidDefinition", "configDefinition": {"configType": {"description": "A configuration dictionary with typed fields", "fields": [{"configType": {"key": "String"}, "description": "Optionally specify the key for the file to be ingested into the keyed store. Defaults to the last path component of the downloaded s3 key.", "isOptional": true, "name": "file_key"}], "innerTypes": [{"description": "", "innerTypes": [], "isList": false, "isNullable": false, "isSelector": false, "key": "String", "name": "String"}], "isList": false, "isNullable": false, "isSelector": false, "key": "Dict.42", "name": null}}, "description": "This is a solid which caches a file in s3 into file cache.\n\nThe `file_cache` is a resource type that allows a solid author to save files\nand assign a key to them. The keyed file store can be backed by local file or any\nobject store (currently we support s3). This keyed file store can be configured\nto be at an external location so that is persists in a well known spot between runs.\nIt is designed for the case where there is an expensive download step that should not\noccur unless the downloaded file does not exist. Redownload can be instigated either\nby configuring the source to overwrite files or to just delete the file in the underlying\nstorage manually.\n\nThis works by downloading the file to a temporary file, and then ingesting it into\nthe file cache. In the case of a filesystem-backed file cache, this is a file\ncopy. In the case of a object-store-backed file cache, this is an upload.\n\nIn order to work this must be executed within a mode that provides an `s3`\nand `file_cache` resource.\n    ", "metadata": [], "name": "cache_file_from_s3", "requiredResources": [{"resourceKey": "file_cache"}, {"resourceKey": "s3"}]}, "inputs": [{"definition": {"description": null, "name": "s3_coordinate", "type": {"description": null, "displayName": "S3Coordinate", "name": "S3Coordinate"}}, "dependsOn": []}], "name": "cache_file_from_s3", "outputs": [{"definition": {"description": null, "name": "result", "type": {"description": "A file handle a reference to a file, which could be resident in the local file\n    system, an object store, or any arbitrary place where file can be stored.\n\n    This exists to handle the very common case where you wish to write a computation that reads,\n    transforms, and writes files, but is written in a way where the same code could work in local\n    developement as well as in a cluster where the files would be stored in globally available\n    object store such as s3.\n    ", "displayName": "FileHandle", "name": "FileHandle"}}, "dependedBy": [{"definition": {"name": "archive_file_handle", "type": {"displayName": "FileHandle"}}, "solid": {"name": "unzip_file_handle"}}]}]}}, {"handleID": "master_cord_s3_to_df.ingest_csv_file_handle_to_spark", "parent": {"handleID": "master_cord_s3_to_df"}, "solid": {"definition": {"__typename": "SolidDefinition", "configDefinition": null, "description": "Take a file handle that contains a csv with headers and load it\ninto a Spark DataFrame. It infers header names but does *not* infer schema.\n\nIt also ensures that the column names are valid parquet column names by\nfiltering out any of the following characters from column names:\n\nCharacters (within quotations): \"`[ ,;{}()\\n\\t=]`\"\n\n", "metadata": [], "name": "ingest_csv_file_handle_to_spark", "requiredResources": [{"resourceKey": "spark"}]}, "inputs": [{"definition": {"description": null, "name": "csv_file_handle", "type": {"description": "A file handle a reference to a file, which could be resident in the local file\n    system, an object store, or any arbitrary place where file can be stored.\n\n    This exists to handle the very common case where you wish to write a computation that reads,\n    transforms, and writes files, but is written in a way where the same code could work in local\n    developement as well as in a cluster where the files would be stored in globally available\n    object store such as s3.\n    ", "displayName": "FileHandle", "name": "FileHandle"}}, "dependsOn": [{"definition": {"name": "result", "type": {"displayName": "FileHandle"}}, "solid": {"name": "unzip_file_handle"}}]}], "name": "ingest_csv_file_handle_to_spark", "outputs": [{"definition": {"description": null, "name": "result", "type": {"description": "A Pyspark data frame.", "displayName": "DataFrame", "name": "DataFrame"}}, "dependedBy": []}]}}, {"handleID": "master_cord_s3_to_df.unzip_file_handle", "parent": {"handleID": "master_cord_s3_to_df"}, "solid": {"definition": {"__typename": "SolidDefinition", "configDefinition": null, "description": "Unzip a file that is resident in an archive file as a member.\n    This solid operates on FileHandles, meaning that their physical is dependent\n    on what system storage is operating in the pipeline. The physical file could\n    be on local disk, or it could be in s3. If on s3, this solid will download\n    that file to local disk, perform the unzip, upload that file back to s3, and\n    then return that file handle for downstream use in the computations.\n    ", "metadata": [], "name": "unzip_file_handle", "requiredResources": []}, "inputs": [{"definition": {"description": null, "name": "archive_file_handle", "type": {"description": "A file handle a reference to a file, which could be resident in the local file\n    system, an object store, or any arbitrary place where file can be stored.\n\n    This exists to handle the very common case where you wish to write a computation that reads,\n    transforms, and writes files, but is written in a way where the same code could work in local\n    developement as well as in a cluster where the files would be stored in globally available\n    object store such as s3.\n    ", "displayName": "FileHandle", "name": "FileHandle"}}, "dependsOn": [{"definition": {"name": "result", "type": {"displayName": "FileHandle"}}, "solid": {"name": "cache_file_from_s3"}}]}, {"definition": {"description": null, "name": "archive_member", "type": {"description": null, "displayName": "String", "name": "String"}}, "dependsOn": []}], "name": "unzip_file_handle", "outputs": [{"definition": {"description": null, "name": "result", "type": {"description": "A file handle a reference to a file, which could be resident in the local file\n    system, an object store, or any arbitrary place where file can be stored.\n\n    This exists to handle the very common case where you wish to write a computation that reads,\n    transforms, and writes files, but is written in a way where the same code could work in local\n    developement as well as in a cluster where the files would be stored in globally available\n    object store such as s3.\n    ", "displayName": "FileHandle", "name": "FileHandle"}}, "dependedBy": [{"definition": {"name": "csv_file_handle", "type": {"displayName": "FileHandle"}}, "solid": {"name": "ingest_csv_file_handle_to_spark"}}]}]}}, {"handleID": "may_on_time_s3_to_df", "parent": null, "solid": {"definition": {"__typename": "CompositeSolidDefinition", "description": "Ingest a zipped csv file from s3,\nstash in a keyed file store (does not download if already\npresent by default), unzip that file, and load it into a\nSpark Dataframe. See documentation in constituent solids for\nmore detail.", "inputMappings": [{"definition": {"name": "s3_coordinate"}, "mappedInput": {"definition": {"name": "s3_coordinate"}, "solid": {"name": "cache_file_from_s3"}}}, {"definition": {"name": "archive_member"}, "mappedInput": {"definition": {"name": "archive_member"}, "solid": {"name": "unzip_file_handle"}}}], "metadata": [], "name": "s3_to_df", "outputMappings": [{"definition": {"name": "result"}, "mappedOutput": {"definition": {"name": "result"}, "solid": {"name": "ingest_csv_file_handle_to_spark"}}}], "requiredResources": [{"resourceKey": "spark"}, {"resourceKey": "file_cache"}, {"resourceKey": "s3"}]}, "inputs": [{"definition": {"description": null, "name": "s3_coordinate", "type": {"description": null, "displayName": "S3Coordinate", "name": "S3Coordinate"}}, "dependsOn": []}, {"definition": {"description": null, "name": "archive_member", "type": {"description": null, "displayName": "String", "name": "String"}}, "dependsOn": []}], "name": "may_on_time_s3_to_df", "outputs": [{"definition": {"description": null, "name": "result", "type": {"description": "A Pyspark data frame.", "displayName": "DataFrame", "name": "DataFrame"}}, "dependedBy": [{"definition": {"name": "may_data", "type": {"displayName": "DataFrame"}}, "solid": {"name": "join_q2_data"}}]}]}}, {"handleID": "may_on_time_s3_to_df.cache_file_from_s3", "parent": {"handleID": "may_on_time_s3_to_df"}, "solid": {"definition": {"__typename": "SolidDefinition", "configDefinition": {"configType": {"description": "A configuration dictionary with typed fields", "fields": [{"configType": {"key": "String"}, "description": "Optionally specify the key for the file to be ingested into the keyed store. Defaults to the last path component of the downloaded s3 key.", "isOptional": true, "name": "file_key"}], "innerTypes": [{"description": "", "innerTypes": [], "isList": false, "isNullable": false, "isSelector": false, "key": "String", "name": "String"}], "isList": false, "isNullable": false, "isSelector": false, "key": "Dict.42", "name": null}}, "description": "This is a solid which caches a file in s3 into file cache.\n\nThe `file_cache` is a resource type that allows a solid author to save files\nand assign a key to them. The keyed file store can be backed by local file or any\nobject store (currently we support s3). This keyed file store can be configured\nto be at an external location so that is persists in a well known spot between runs.\nIt is designed for the case where there is an expensive download step that should not\noccur unless the downloaded file does not exist. Redownload can be instigated either\nby configuring the source to overwrite files or to just delete the file in the underlying\nstorage manually.\n\nThis works by downloading the file to a temporary file, and then ingesting it into\nthe file cache. In the case of a filesystem-backed file cache, this is a file\ncopy. In the case of a object-store-backed file cache, this is an upload.\n\nIn order to work this must be executed within a mode that provides an `s3`\nand `file_cache` resource.\n    ", "metadata": [], "name": "cache_file_from_s3", "requiredResources": [{"resourceKey": "file_cache"}, {"resourceKey": "s3"}]}, "inputs": [{"definition": {"description": null, "name": "s3_coordinate", "type": {"description": null, "displayName": "S3Coordinate", "name": "S3Coordinate"}}, "dependsOn": []}], "name": "cache_file_from_s3", "outputs": [{"definition": {"description": null, "name": "result", "type": {"description": "A file handle a reference to a file, which could be resident in the local file\n    system, an object store, or any arbitrary place where file can be stored.\n\n    This exists to handle the very common case where you wish to write a computation that reads,\n    transforms, and writes files, but is written in a way where the same code could work in local\n    developement as well as in a cluster where the files would be stored in globally available\n    object store such as s3.\n    ", "displayName": "FileHandle", "name": "FileHandle"}}, "dependedBy": [{"definition": {"name": "archive_file_handle", "type": {"displayName": "FileHandle"}}, "solid": {"name": "unzip_file_handle"}}]}]}}, {"handleID": "may_on_time_s3_to_df.ingest_csv_file_handle_to_spark", "parent": {"handleID": "may_on_time_s3_to_df"}, "solid": {"definition": {"__typename": "SolidDefinition", "configDefinition": null, "description": "Take a file handle that contains a csv with headers and load it\ninto a Spark DataFrame. It infers header names but does *not* infer schema.\n\nIt also ensures that the column names are valid parquet column names by\nfiltering out any of the following characters from column names:\n\nCharacters (within quotations): \"`[ ,;{}()\\n\\t=]`\"\n\n", "metadata": [], "name": "ingest_csv_file_handle_to_spark", "requiredResources": [{"resourceKey": "spark"}]}, "inputs": [{"definition": {"description": null, "name": "csv_file_handle", "type": {"description": "A file handle a reference to a file, which could be resident in the local file\n    system, an object store, or any arbitrary place where file can be stored.\n\n    This exists to handle the very common case where you wish to write a computation that reads,\n    transforms, and writes files, but is written in a way where the same code could work in local\n    developement as well as in a cluster where the files would be stored in globally available\n    object store such as s3.\n    ", "displayName": "FileHandle", "name": "FileHandle"}}, "dependsOn": [{"definition": {"name": "result", "type": {"displayName": "FileHandle"}}, "solid": {"name": "unzip_file_handle"}}]}], "name": "ingest_csv_file_handle_to_spark", "outputs": [{"definition": {"description": null, "name": "result", "type": {"description": "A Pyspark data frame.", "displayName": "DataFrame", "name": "DataFrame"}}, "dependedBy": []}]}}, {"handleID": "may_on_time_s3_to_df.unzip_file_handle", "parent": {"handleID": "may_on_time_s3_to_df"}, "solid": {"definition": {"__typename": "SolidDefinition", "configDefinition": null, "description": "Unzip a file that is resident in an archive file as a member.\n    This solid operates on FileHandles, meaning that their physical is dependent\n    on what system storage is operating in the pipeline. The physical file could\n    be on local disk, or it could be in s3. If on s3, this solid will download\n    that file to local disk, perform the unzip, upload that file back to s3, and\n    then return that file handle for downstream use in the computations.\n    ", "metadata": [], "name": "unzip_file_handle", "requiredResources": []}, "inputs": [{"definition": {"description": null, "name": "archive_file_handle", "type": {"description": "A file handle a reference to a file, which could be resident in the local file\n    system, an object store, or any arbitrary place where file can be stored.\n\n    This exists to handle the very common case where you wish to write a computation that reads,\n    transforms, and writes files, but is written in a way where the same code could work in local\n    developement as well as in a cluster where the files would be stored in globally available\n    object store such as s3.\n    ", "displayName": "FileHandle", "name": "FileHandle"}}, "dependsOn": [{"definition": {"name": "result", "type": {"displayName": "FileHandle"}}, "solid": {"name": "cache_file_from_s3"}}]}, {"definition": {"description": null, "name": "archive_member", "type": {"description": null, "displayName": "String", "name": "String"}}, "dependsOn": []}], "name": "unzip_file_handle", "outputs": [{"definition": {"description": null, "name": "result", "type": {"description": "A file handle a reference to a file, which could be resident in the local file\n    system, an object store, or any arbitrary place where file can be stored.\n\n    This exists to handle the very common case where you wish to write a computation that reads,\n    transforms, and writes files, but is written in a way where the same code could work in local\n    developement as well as in a cluster where the files would be stored in globally available\n    object store such as s3.\n    ", "displayName": "FileHandle", "name": "FileHandle"}}, "dependedBy": [{"definition": {"name": "csv_file_handle", "type": {"displayName": "FileHandle"}}, "solid": {"name": "ingest_csv_file_handle_to_spark"}}]}]}}, {"handleID": "process_q2_coupon_data", "parent": null, "solid": {"definition": {"__typename": "CompositeSolidDefinition", "description": "Ingest zipped csv file from s3, load into a Spark\nDataFrame, optionally subsample it (via configuring the\nsubsample_spark_dataset, solid), canonicalize the column names, and then\nload it into a data warehouse.\n", "inputMappings": [{"definition": {"name": "s3_coordinate"}, "mappedInput": {"definition": {"name": "s3_coordinate"}, "solid": {"name": "s3_to_df"}}}, {"definition": {"name": "archive_member"}, "mappedInput": {"definition": {"name": "archive_member"}, "solid": {"name": "s3_to_df"}}}], "metadata": [], "name": "s3_to_dw_table", "outputMappings": [{"definition": {"name": "result"}, "mappedOutput": {"definition": {"name": "table_name"}, "solid": {"name": "load_data_to_database_from_spark"}}}], "requiredResources": [{"resourceKey": "spark"}, {"resourceKey": "file_cache"}, {"resourceKey": "s3"}]}, "inputs": [{"definition": {"description": null, "name": "s3_coordinate", "type": {"description": null, "displayName": "S3Coordinate", "name": "S3Coordinate"}}, "dependsOn": []}, {"definition": {"description": null, "name": "archive_member", "type": {"description": null, "displayName": "String", "name": "String"}}, "dependsOn": []}], "name": "process_q2_coupon_data", "outputs": [{"definition": {"description": null, "name": "result", "type": {"description": null, "displayName": "String", "name": "String"}}, "dependedBy": []}]}}, {"handleID": "process_q2_coupon_data.canonicalize_column_names", "parent": {"handleID": "process_q2_coupon_data"}, "solid": {"definition": {"__typename": "SolidDefinition", "configDefinition": null, "description": null, "metadata": [], "name": "canonicalize_column_names", "requiredResources": []}, "inputs": [{"definition": {"description": null, "name": "data_frame", "type": {"description": "A Pyspark data frame.", "displayName": "DataFrame", "name": "DataFrame"}}, "dependsOn": [{"definition": {"name": "result", "type": {"displayName": "DataFrame"}}, "solid": {"name": "subsample_spark_dataset"}}]}], "name": "canonicalize_column_names", "outputs": [{"definition": {"description": null, "name": "result", "type": {"description": "A Pyspark data frame.", "displayName": "DataFrame", "name": "DataFrame"}}, "dependedBy": [{"definition": {"name": "data_frame", "type": {"displayName": "DataFrame"}}, "solid": {"name": "load_data_to_database_from_spark"}}]}]}}, {"handleID": "process_q2_coupon_data.load_data_to_database_from_spark", "parent": {"handleID": "process_q2_coupon_data"}, "solid": {"definition": {"__typename": "SolidDefinition", "configDefinition": {"configType": {"description": "A configuration dictionary with typed fields", "fields": [{"configType": {"key": "String"}, "description": "", "isOptional": false, "name": "table_name"}], "innerTypes": [{"description": "", "innerTypes": [], "isList": false, "isNullable": false, "isSelector": false, "key": "String", "name": "String"}], "isList": false, "isNullable": false, "isSelector": false, "key": "Dict.160", "name": null}}, "description": null, "metadata": [], "name": "load_data_to_database_from_spark", "requiredResources": []}, "inputs": [{"definition": {"description": null, "name": "data_frame", "type": {"description": "A Pyspark data frame.", "displayName": "DataFrame", "name": "DataFrame"}}, "dependsOn": [{"definition": {"name": "result", "type": {"displayName": "DataFrame"}}, "solid": {"name": "canonicalize_column_names"}}]}], "name": "load_data_to_database_from_spark", "outputs": [{"definition": {"description": null, "name": "table_name", "type": {"description": null, "displayName": "String", "name": "String"}}, "dependedBy": []}]}}, {"handleID": "process_q2_coupon_data.s3_to_df", "parent": {"handleID": "process_q2_coupon_data"}, "solid": {"definition": {"__typename": "CompositeSolidDefinition", "description": "Ingest a zipped csv file from s3,\nstash in a keyed file store (does not download if already\npresent by default), unzip that file, and load it into a\nSpark Dataframe. See documentation in constituent solids for\nmore detail.", "inputMappings": [{"definition": {"name": "s3_coordinate"}, "mappedInput": {"definition": {"name": "s3_coordinate"}, "solid": {"name": "cache_file_from_s3"}}}, {"definition": {"name": "archive_member"}, "mappedInput": {"definition": {"name": "archive_member"}, "solid": {"name": "unzip_file_handle"}}}], "metadata": [], "name": "s3_to_df", "outputMappings": [{"definition": {"name": "result"}, "mappedOutput": {"definition": {"name": "result"}, "solid": {"name": "ingest_csv_file_handle_to_spark"}}}], "requiredResources": [{"resourceKey": "spark"}, {"resourceKey": "file_cache"}, {"resourceKey": "s3"}]}, "inputs": [{"definition": {"description": null, "name": "s3_coordinate", "type": {"description": null, "displayName": "S3Coordinate", "name": "S3Coordinate"}}, "dependsOn": []}, {"definition": {"description": null, "name": "archive_member", "type": {"description": null, "displayName": "String", "name": "String"}}, "dependsOn": []}], "name": "s3_to_df", "outputs": [{"definition": {"description": null, "name": "result", "type": {"description": "A Pyspark data frame.", "displayName": "DataFrame", "name": "DataFrame"}}, "dependedBy": [{"definition": {"name": "data_frame", "type": {"displayName": "DataFrame"}}, "solid": {"name": "subsample_spark_dataset"}}]}]}}, {"handleID": "process_q2_coupon_data.s3_to_df.cache_file_from_s3", "parent": {"handleID": "process_q2_coupon_data.s3_to_df"}, "solid": {"definition": {"__typename": "SolidDefinition", "configDefinition": {"configType": {"description": "A configuration dictionary with typed fields", "fields": [{"configType": {"key": "String"}, "description": "Optionally specify the key for the file to be ingested into the keyed store. Defaults to the last path component of the downloaded s3 key.", "isOptional": true, "name": "file_key"}], "innerTypes": [{"description": "", "innerTypes": [], "isList": false, "isNullable": false, "isSelector": false, "key": "String", "name": "String"}], "isList": false, "isNullable": false, "isSelector": false, "key": "Dict.42", "name": null}}, "description": "This is a solid which caches a file in s3 into file cache.\n\nThe `file_cache` is a resource type that allows a solid author to save files\nand assign a key to them. The keyed file store can be backed by local file or any\nobject store (currently we support s3). This keyed file store can be configured\nto be at an external location so that is persists in a well known spot between runs.\nIt is designed for the case where there is an expensive download step that should not\noccur unless the downloaded file does not exist. Redownload can be instigated either\nby configuring the source to overwrite files or to just delete the file in the underlying\nstorage manually.\n\nThis works by downloading the file to a temporary file, and then ingesting it into\nthe file cache. In the case of a filesystem-backed file cache, this is a file\ncopy. In the case of a object-store-backed file cache, this is an upload.\n\nIn order to work this must be executed within a mode that provides an `s3`\nand `file_cache` resource.\n    ", "metadata": [], "name": "cache_file_from_s3", "requiredResources": [{"resourceKey": "file_cache"}, {"resourceKey": "s3"}]}, "inputs": [{"definition": {"description": null, "name": "s3_coordinate", "type": {"description": null, "displayName": "S3Coordinate", "name": "S3Coordinate"}}, "dependsOn": []}], "name": "cache_file_from_s3", "outputs": [{"definition": {"description": null, "name": "result", "type": {"description": "A file handle a reference to a file, which could be resident in the local file\n    system, an object store, or any arbitrary place where file can be stored.\n\n    This exists to handle the very common case where you wish to write a computation that reads,\n    transforms, and writes files, but is written in a way where the same code could work in local\n    developement as well as in a cluster where the files would be stored in globally available\n    object store such as s3.\n    ", "displayName": "FileHandle", "name": "FileHandle"}}, "dependedBy": [{"definition": {"name": "archive_file_handle", "type": {"displayName": "FileHandle"}}, "solid": {"name": "unzip_file_handle"}}]}]}}, {"handleID": "process_q2_coupon_data.s3_to_df.ingest_csv_file_handle_to_spark", "parent": {"handleID": "process_q2_coupon_data.s3_to_df"}, "solid": {"definition": {"__typename": "SolidDefinition", "configDefinition": null, "description": "Take a file handle that contains a csv with headers and load it\ninto a Spark DataFrame. It infers header names but does *not* infer schema.\n\nIt also ensures that the column names are valid parquet column names by\nfiltering out any of the following characters from column names:\n\nCharacters (within quotations): \"`[ ,;{}()\\n\\t=]`\"\n\n", "metadata": [], "name": "ingest_csv_file_handle_to_spark", "requiredResources": [{"resourceKey": "spark"}]}, "inputs": [{"definition": {"description": null, "name": "csv_file_handle", "type": {"description": "A file handle a reference to a file, which could be resident in the local file\n    system, an object store, or any arbitrary place where file can be stored.\n\n    This exists to handle the very common case where you wish to write a computation that reads,\n    transforms, and writes files, but is written in a way where the same code could work in local\n    developement as well as in a cluster where the files would be stored in globally available\n    object store such as s3.\n    ", "displayName": "FileHandle", "name": "FileHandle"}}, "dependsOn": [{"definition": {"name": "result", "type": {"displayName": "FileHandle"}}, "solid": {"name": "unzip_file_handle"}}]}], "name": "ingest_csv_file_handle_to_spark", "outputs": [{"definition": {"description": null, "name": "result", "type": {"description": "A Pyspark data frame.", "displayName": "DataFrame", "name": "DataFrame"}}, "dependedBy": []}]}}, {"handleID": "process_q2_coupon_data.s3_to_df.unzip_file_handle", "parent": {"handleID": "process_q2_coupon_data.s3_to_df"}, "solid": {"definition": {"__typename": "SolidDefinition", "configDefinition": null, "description": "Unzip a file that is resident in an archive file as a member.\n    This solid operates on FileHandles, meaning that their physical is dependent\n    on what system storage is operating in the pipeline. The physical file could\n    be on local disk, or it could be in s3. If on s3, this solid will download\n    that file to local disk, perform the unzip, upload that file back to s3, and\n    then return that file handle for downstream use in the computations.\n    ", "metadata": [], "name": "unzip_file_handle", "requiredResources": []}, "inputs": [{"definition": {"description": null, "name": "archive_file_handle", "type": {"description": "A file handle a reference to a file, which could be resident in the local file\n    system, an object store, or any arbitrary place where file can be stored.\n\n    This exists to handle the very common case where you wish to write a computation that reads,\n    transforms, and writes files, but is written in a way where the same code could work in local\n    developement as well as in a cluster where the files would be stored in globally available\n    object store such as s3.\n    ", "displayName": "FileHandle", "name": "FileHandle"}}, "dependsOn": [{"definition": {"name": "result", "type": {"displayName": "FileHandle"}}, "solid": {"name": "cache_file_from_s3"}}]}, {"definition": {"description": null, "name": "archive_member", "type": {"description": null, "displayName": "String", "name": "String"}}, "dependsOn": []}], "name": "unzip_file_handle", "outputs": [{"definition": {"description": null, "name": "result", "type": {"description": "A file handle a reference to a file, which could be resident in the local file\n    system, an object store, or any arbitrary place where file can be stored.\n\n    This exists to handle the very common case where you wish to write a computation that reads,\n    transforms, and writes files, but is written in a way where the same code could work in local\n    developement as well as in a cluster where the files would be stored in globally available\n    object store such as s3.\n    ", "displayName": "FileHandle", "name": "FileHandle"}}, "dependedBy": [{"definition": {"name": "csv_file_handle", "type": {"displayName": "FileHandle"}}, "solid": {"name": "ingest_csv_file_handle_to_spark"}}]}]}}, {"handleID": "process_q2_coupon_data.subsample_spark_dataset", "parent": {"handleID": "process_q2_coupon_data"}, "solid": {"definition": {"__typename": "SolidDefinition", "configDefinition": {"configType": {"description": "A configuration dictionary with typed fields", "fields": [{"configType": {"key": "Int"}, "description": "The integer percentage of rows to sample from the input dataset.", "isOptional": false, "name": "subsample_pct"}], "innerTypes": [{"description": "", "innerTypes": [], "isList": false, "isNullable": false, "isSelector": false, "key": "Int", "name": "Int"}], "isList": false, "isNullable": false, "isSelector": false, "key": "Dict.161", "name": null}}, "description": "Subsample a spark dataset via the configuration option.", "metadata": [], "name": "subsample_spark_dataset", "requiredResources": []}, "inputs": [{"definition": {"description": null, "name": "data_frame", "type": {"description": "A Pyspark data frame.", "displayName": "DataFrame", "name": "DataFrame"}}, "dependsOn": [{"definition": {"name": "result", "type": {"displayName": "DataFrame"}}, "solid": {"name": "s3_to_df"}}]}], "name": "subsample_spark_dataset", "outputs": [{"definition": {"description": null, "name": "result", "type": {"description": "A Pyspark data frame.", "displayName": "DataFrame", "name": "DataFrame"}}, "dependedBy": [{"definition": {"name": "data_frame", "type": {"displayName": "DataFrame"}}, "solid": {"name": "canonicalize_column_names"}}]}]}}, {"handleID": "process_q2_market_data", "parent": null, "solid": {"definition": {"__typename": "CompositeSolidDefinition", "description": "Ingest zipped csv file from s3, load into a Spark\nDataFrame, optionally subsample it (via configuring the\nsubsample_spark_dataset, solid), canonicalize the column names, and then\nload it into a data warehouse.\n", "inputMappings": [{"definition": {"name": "s3_coordinate"}, "mappedInput": {"definition": {"name": "s3_coordinate"}, "solid": {"name": "s3_to_df"}}}, {"definition": {"name": "archive_member"}, "mappedInput": {"definition": {"name": "archive_member"}, "solid": {"name": "s3_to_df"}}}], "metadata": [], "name": "s3_to_dw_table", "outputMappings": [{"definition": {"name": "result"}, "mappedOutput": {"definition": {"name": "table_name"}, "solid": {"name": "load_data_to_database_from_spark"}}}], "requiredResources": [{"resourceKey": "spark"}, {"resourceKey": "file_cache"}, {"resourceKey": "s3"}]}, "inputs": [{"definition": {"description": null, "name": "s3_coordinate", "type": {"description": null, "displayName": "S3Coordinate", "name": "S3Coordinate"}}, "dependsOn": []}, {"definition": {"description": null, "name": "archive_member", "type": {"description": null, "displayName": "String", "name": "String"}}, "dependsOn": []}], "name": "process_q2_market_data", "outputs": [{"definition": {"description": null, "name": "result", "type": {"description": null, "displayName": "String", "name": "String"}}, "dependedBy": []}]}}, {"handleID": "process_q2_market_data.canonicalize_column_names", "parent": {"handleID": "process_q2_market_data"}, "solid": {"definition": {"__typename": "SolidDefinition", "configDefinition": null, "description": null, "metadata": [], "name": "canonicalize_column_names", "requiredResources": []}, "inputs": [{"definition": {"description": null, "name": "data_frame", "type": {"description": "A Pyspark data frame.", "displayName": "DataFrame", "name": "DataFrame"}}, "dependsOn": [{"definition": {"name": "result", "type": {"displayName": "DataFrame"}}, "solid": {"name": "subsample_spark_dataset"}}]}], "name": "canonicalize_column_names", "outputs": [{"definition": {"description": null, "name": "result", "type": {"description": "A Pyspark data frame.", "displayName": "DataFrame", "name": "DataFrame"}}, "dependedBy": [{"definition": {"name": "data_frame", "type": {"displayName": "DataFrame"}}, "solid": {"name": "load_data_to_database_from_spark"}}]}]}}, {"handleID": "process_q2_market_data.load_data_to_database_from_spark", "parent": {"handleID": "process_q2_market_data"}, "solid": {"definition": {"__typename": "SolidDefinition", "configDefinition": {"configType": {"description": "A configuration dictionary with typed fields", "fields": [{"configType": {"key": "String"}, "description": "", "isOptional": false, "name": "table_name"}], "innerTypes": [{"description": "", "innerTypes": [], "isList": false, "isNullable": false, "isSelector": false, "key": "String", "name": "String"}], "isList": false, "isNullable": false, "isSelector": false, "key": "Dict.160", "name": null}}, "description": null, "metadata": [], "name": "load_data_to_database_from_spark", "requiredResources": []}, "inputs": [{"definition": {"description": null, "name": "data_frame", "type": {"description": "A Pyspark data frame.", "displayName": "DataFrame", "name": "DataFrame"}}, "dependsOn": [{"definition": {"name": "result", "type": {"displayName": "DataFrame"}}, "solid": {"name": "canonicalize_column_names"}}]}], "name": "load_data_to_database_from_spark", "outputs": [{"definition": {"description": null, "name": "table_name", "type": {"description": null, "displayName": "String", "name": "String"}}, "dependedBy": []}]}}, {"handleID": "process_q2_market_data.s3_to_df", "parent": {"handleID": "process_q2_market_data"}, "solid": {"definition": {"__typename": "CompositeSolidDefinition", "description": "Ingest a zipped csv file from s3,\nstash in a keyed file store (does not download if already\npresent by default), unzip that file, and load it into a\nSpark Dataframe. See documentation in constituent solids for\nmore detail.", "inputMappings": [{"definition": {"name": "s3_coordinate"}, "mappedInput": {"definition": {"name": "s3_coordinate"}, "solid": {"name": "cache_file_from_s3"}}}, {"definition": {"name": "archive_member"}, "mappedInput": {"definition": {"name": "archive_member"}, "solid": {"name": "unzip_file_handle"}}}], "metadata": [], "name": "s3_to_df", "outputMappings": [{"definition": {"name": "result"}, "mappedOutput": {"definition": {"name": "result"}, "solid": {"name": "ingest_csv_file_handle_to_spark"}}}], "requiredResources": [{"resourceKey": "spark"}, {"resourceKey": "file_cache"}, {"resourceKey": "s3"}]}, "inputs": [{"definition": {"description": null, "name": "s3_coordinate", "type": {"description": null, "displayName": "S3Coordinate", "name": "S3Coordinate"}}, "dependsOn": []}, {"definition": {"description": null, "name": "archive_member", "type": {"description": null, "displayName": "String", "name": "String"}}, "dependsOn": []}], "name": "s3_to_df", "outputs": [{"definition": {"description": null, "name": "result", "type": {"description": "A Pyspark data frame.", "displayName": "DataFrame", "name": "DataFrame"}}, "dependedBy": [{"definition": {"name": "data_frame", "type": {"displayName": "DataFrame"}}, "solid": {"name": "subsample_spark_dataset"}}]}]}}, {"handleID": "process_q2_market_data.s3_to_df.cache_file_from_s3", "parent": {"handleID": "process_q2_market_data.s3_to_df"}, "solid": {"definition": {"__typename": "SolidDefinition", "configDefinition": {"configType": {"description": "A configuration dictionary with typed fields", "fields": [{"configType": {"key": "String"}, "description": "Optionally specify the key for the file to be ingested into the keyed store. Defaults to the last path component of the downloaded s3 key.", "isOptional": true, "name": "file_key"}], "innerTypes": [{"description": "", "innerTypes": [], "isList": false, "isNullable": false, "isSelector": false, "key": "String", "name": "String"}], "isList": false, "isNullable": false, "isSelector": false, "key": "Dict.42", "name": null}}, "description": "This is a solid which caches a file in s3 into file cache.\n\nThe `file_cache` is a resource type that allows a solid author to save files\nand assign a key to them. The keyed file store can be backed by local file or any\nobject store (currently we support s3). This keyed file store can be configured\nto be at an external location so that is persists in a well known spot between runs.\nIt is designed for the case where there is an expensive download step that should not\noccur unless the downloaded file does not exist. Redownload can be instigated either\nby configuring the source to overwrite files or to just delete the file in the underlying\nstorage manually.\n\nThis works by downloading the file to a temporary file, and then ingesting it into\nthe file cache. In the case of a filesystem-backed file cache, this is a file\ncopy. In the case of a object-store-backed file cache, this is an upload.\n\nIn order to work this must be executed within a mode that provides an `s3`\nand `file_cache` resource.\n    ", "metadata": [], "name": "cache_file_from_s3", "requiredResources": [{"resourceKey": "file_cache"}, {"resourceKey": "s3"}]}, "inputs": [{"definition": {"description": null, "name": "s3_coordinate", "type": {"description": null, "displayName": "S3Coordinate", "name": "S3Coordinate"}}, "dependsOn": []}], "name": "cache_file_from_s3", "outputs": [{"definition": {"description": null, "name": "result", "type": {"description": "A file handle a reference to a file, which could be resident in the local file\n    system, an object store, or any arbitrary place where file can be stored.\n\n    This exists to handle the very common case where you wish to write a computation that reads,\n    transforms, and writes files, but is written in a way where the same code could work in local\n    developement as well as in a cluster where the files would be stored in globally available\n    object store such as s3.\n    ", "displayName": "FileHandle", "name": "FileHandle"}}, "dependedBy": [{"definition": {"name": "archive_file_handle", "type": {"displayName": "FileHandle"}}, "solid": {"name": "unzip_file_handle"}}]}]}}, {"handleID": "process_q2_market_data.s3_to_df.ingest_csv_file_handle_to_spark", "parent": {"handleID": "process_q2_market_data.s3_to_df"}, "solid": {"definition": {"__typename": "SolidDefinition", "configDefinition": null, "description": "Take a file handle that contains a csv with headers and load it\ninto a Spark DataFrame. It infers header names but does *not* infer schema.\n\nIt also ensures that the column names are valid parquet column names by\nfiltering out any of the following characters from column names:\n\nCharacters (within quotations): \"`[ ,;{}()\\n\\t=]`\"\n\n", "metadata": [], "name": "ingest_csv_file_handle_to_spark", "requiredResources": [{"resourceKey": "spark"}]}, "inputs": [{"definition": {"description": null, "name": "csv_file_handle", "type": {"description": "A file handle a reference to a file, which could be resident in the local file\n    system, an object store, or any arbitrary place where file can be stored.\n\n    This exists to handle the very common case where you wish to write a computation that reads,\n    transforms, and writes files, but is written in a way where the same code could work in local\n    developement as well as in a cluster where the files would be stored in globally available\n    object store such as s3.\n    ", "displayName": "FileHandle", "name": "FileHandle"}}, "dependsOn": [{"definition": {"name": "result", "type": {"displayName": "FileHandle"}}, "solid": {"name": "unzip_file_handle"}}]}], "name": "ingest_csv_file_handle_to_spark", "outputs": [{"definition": {"description": null, "name": "result", "type": {"description": "A Pyspark data frame.", "displayName": "DataFrame", "name": "DataFrame"}}, "dependedBy": []}]}}, {"handleID": "process_q2_market_data.s3_to_df.unzip_file_handle", "parent": {"handleID": "process_q2_market_data.s3_to_df"}, "solid": {"definition": {"__typename": "SolidDefinition", "configDefinition": null, "description": "Unzip a file that is resident in an archive file as a member.\n    This solid operates on FileHandles, meaning that their physical is dependent\n    on what system storage is operating in the pipeline. The physical file could\n    be on local disk, or it could be in s3. If on s3, this solid will download\n    that file to local disk, perform the unzip, upload that file back to s3, and\n    then return that file handle for downstream use in the computations.\n    ", "metadata": [], "name": "unzip_file_handle", "requiredResources": []}, "inputs": [{"definition": {"description": null, "name": "archive_file_handle", "type": {"description": "A file handle a reference to a file, which could be resident in the local file\n    system, an object store, or any arbitrary place where file can be stored.\n\n    This exists to handle the very common case where you wish to write a computation that reads,\n    transforms, and writes files, but is written in a way where the same code could work in local\n    developement as well as in a cluster where the files would be stored in globally available\n    object store such as s3.\n    ", "displayName": "FileHandle", "name": "FileHandle"}}, "dependsOn": [{"definition": {"name": "result", "type": {"displayName": "FileHandle"}}, "solid": {"name": "cache_file_from_s3"}}]}, {"definition": {"description": null, "name": "archive_member", "type": {"description": null, "displayName": "String", "name": "String"}}, "dependsOn": []}], "name": "unzip_file_handle", "outputs": [{"definition": {"description": null, "name": "result", "type": {"description": "A file handle a reference to a file, which could be resident in the local file\n    system, an object store, or any arbitrary place where file can be stored.\n\n    This exists to handle the very common case where you wish to write a computation that reads,\n    transforms, and writes files, but is written in a way where the same code could work in local\n    developement as well as in a cluster where the files would be stored in globally available\n    object store such as s3.\n    ", "displayName": "FileHandle", "name": "FileHandle"}}, "dependedBy": [{"definition": {"name": "csv_file_handle", "type": {"displayName": "FileHandle"}}, "solid": {"name": "ingest_csv_file_handle_to_spark"}}]}]}}, {"handleID": "process_q2_market_data.subsample_spark_dataset", "parent": {"handleID": "process_q2_market_data"}, "solid": {"definition": {"__typename": "SolidDefinition", "configDefinition": {"configType": {"description": "A configuration dictionary with typed fields", "fields": [{"configType": {"key": "Int"}, "description": "The integer percentage of rows to sample from the input dataset.", "isOptional": false, "name": "subsample_pct"}], "innerTypes": [{"description": "", "innerTypes": [], "isList": false, "isNullable": false, "isSelector": false, "key": "Int", "name": "Int"}], "isList": false, "isNullable": false, "isSelector": false, "key": "Dict.161", "name": null}}, "description": "Subsample a spark dataset via the configuration option.", "metadata": [], "name": "subsample_spark_dataset", "requiredResources": []}, "inputs": [{"definition": {"description": null, "name": "data_frame", "type": {"description": "A Pyspark data frame.", "displayName": "DataFrame", "name": "DataFrame"}}, "dependsOn": [{"definition": {"name": "result", "type": {"displayName": "DataFrame"}}, "solid": {"name": "s3_to_df"}}]}], "name": "subsample_spark_dataset", "outputs": [{"definition": {"description": null, "name": "result", "type": {"description": "A Pyspark data frame.", "displayName": "DataFrame", "name": "DataFrame"}}, "dependedBy": [{"definition": {"name": "data_frame", "type": {"displayName": "DataFrame"}}, "solid": {"name": "canonicalize_column_names"}}]}]}}, {"handleID": "process_q2_ticket_data", "parent": null, "solid": {"definition": {"__typename": "CompositeSolidDefinition", "description": "Ingest zipped csv file from s3, load into a Spark\nDataFrame, optionally subsample it (via configuring the\nsubsample_spark_dataset, solid), canonicalize the column names, and then\nload it into a data warehouse.\n", "inputMappings": [{"definition": {"name": "s3_coordinate"}, "mappedInput": {"definition": {"name": "s3_coordinate"}, "solid": {"name": "s3_to_df"}}}, {"definition": {"name": "archive_member"}, "mappedInput": {"definition": {"name": "archive_member"}, "solid": {"name": "s3_to_df"}}}], "metadata": [], "name": "s3_to_dw_table", "outputMappings": [{"definition": {"name": "result"}, "mappedOutput": {"definition": {"name": "table_name"}, "solid": {"name": "load_data_to_database_from_spark"}}}], "requiredResources": [{"resourceKey": "spark"}, {"resourceKey": "file_cache"}, {"resourceKey": "s3"}]}, "inputs": [{"definition": {"description": null, "name": "s3_coordinate", "type": {"description": null, "displayName": "S3Coordinate", "name": "S3Coordinate"}}, "dependsOn": []}, {"definition": {"description": null, "name": "archive_member", "type": {"description": null, "displayName": "String", "name": "String"}}, "dependsOn": []}], "name": "process_q2_ticket_data", "outputs": [{"definition": {"description": null, "name": "result", "type": {"description": null, "displayName": "String", "name": "String"}}, "dependedBy": []}]}}, {"handleID": "process_q2_ticket_data.canonicalize_column_names", "parent": {"handleID": "process_q2_ticket_data"}, "solid": {"definition": {"__typename": "SolidDefinition", "configDefinition": null, "description": null, "metadata": [], "name": "canonicalize_column_names", "requiredResources": []}, "inputs": [{"definition": {"description": null, "name": "data_frame", "type": {"description": "A Pyspark data frame.", "displayName": "DataFrame", "name": "DataFrame"}}, "dependsOn": [{"definition": {"name": "result", "type": {"displayName": "DataFrame"}}, "solid": {"name": "subsample_spark_dataset"}}]}], "name": "canonicalize_column_names", "outputs": [{"definition": {"description": null, "name": "result", "type": {"description": "A Pyspark data frame.", "displayName": "DataFrame", "name": "DataFrame"}}, "dependedBy": [{"definition": {"name": "data_frame", "type": {"displayName": "DataFrame"}}, "solid": {"name": "load_data_to_database_from_spark"}}]}]}}, {"handleID": "process_q2_ticket_data.load_data_to_database_from_spark", "parent": {"handleID": "process_q2_ticket_data"}, "solid": {"definition": {"__typename": "SolidDefinition", "configDefinition": {"configType": {"description": "A configuration dictionary with typed fields", "fields": [{"configType": {"key": "String"}, "description": "", "isOptional": false, "name": "table_name"}], "innerTypes": [{"description": "", "innerTypes": [], "isList": false, "isNullable": false, "isSelector": false, "key": "String", "name": "String"}], "isList": false, "isNullable": false, "isSelector": false, "key": "Dict.160", "name": null}}, "description": null, "metadata": [], "name": "load_data_to_database_from_spark", "requiredResources": []}, "inputs": [{"definition": {"description": null, "name": "data_frame", "type": {"description": "A Pyspark data frame.", "displayName": "DataFrame", "name": "DataFrame"}}, "dependsOn": [{"definition": {"name": "result", "type": {"displayName": "DataFrame"}}, "solid": {"name": "canonicalize_column_names"}}]}], "name": "load_data_to_database_from_spark", "outputs": [{"definition": {"description": null, "name": "table_name", "type": {"description": null, "displayName": "String", "name": "String"}}, "dependedBy": []}]}}, {"handleID": "process_q2_ticket_data.s3_to_df", "parent": {"handleID": "process_q2_ticket_data"}, "solid": {"definition": {"__typename": "CompositeSolidDefinition", "description": "Ingest a zipped csv file from s3,\nstash in a keyed file store (does not download if already\npresent by default), unzip that file, and load it into a\nSpark Dataframe. See documentation in constituent solids for\nmore detail.", "inputMappings": [{"definition": {"name": "s3_coordinate"}, "mappedInput": {"definition": {"name": "s3_coordinate"}, "solid": {"name": "cache_file_from_s3"}}}, {"definition": {"name": "archive_member"}, "mappedInput": {"definition": {"name": "archive_member"}, "solid": {"name": "unzip_file_handle"}}}], "metadata": [], "name": "s3_to_df", "outputMappings": [{"definition": {"name": "result"}, "mappedOutput": {"definition": {"name": "result"}, "solid": {"name": "ingest_csv_file_handle_to_spark"}}}], "requiredResources": [{"resourceKey": "spark"}, {"resourceKey": "file_cache"}, {"resourceKey": "s3"}]}, "inputs": [{"definition": {"description": null, "name": "s3_coordinate", "type": {"description": null, "displayName": "S3Coordinate", "name": "S3Coordinate"}}, "dependsOn": []}, {"definition": {"description": null, "name": "archive_member", "type": {"description": null, "displayName": "String", "name": "String"}}, "dependsOn": []}], "name": "s3_to_df", "outputs": [{"definition": {"description": null, "name": "result", "type": {"description": "A Pyspark data frame.", "displayName": "DataFrame", "name": "DataFrame"}}, "dependedBy": [{"definition": {"name": "data_frame", "type": {"displayName": "DataFrame"}}, "solid": {"name": "subsample_spark_dataset"}}]}]}}, {"handleID": "process_q2_ticket_data.s3_to_df.cache_file_from_s3", "parent": {"handleID": "process_q2_ticket_data.s3_to_df"}, "solid": {"definition": {"__typename": "SolidDefinition", "configDefinition": {"configType": {"description": "A configuration dictionary with typed fields", "fields": [{"configType": {"key": "String"}, "description": "Optionally specify the key for the file to be ingested into the keyed store. Defaults to the last path component of the downloaded s3 key.", "isOptional": true, "name": "file_key"}], "innerTypes": [{"description": "", "innerTypes": [], "isList": false, "isNullable": false, "isSelector": false, "key": "String", "name": "String"}], "isList": false, "isNullable": false, "isSelector": false, "key": "Dict.42", "name": null}}, "description": "This is a solid which caches a file in s3 into file cache.\n\nThe `file_cache` is a resource type that allows a solid author to save files\nand assign a key to them. The keyed file store can be backed by local file or any\nobject store (currently we support s3). This keyed file store can be configured\nto be at an external location so that is persists in a well known spot between runs.\nIt is designed for the case where there is an expensive download step that should not\noccur unless the downloaded file does not exist. Redownload can be instigated either\nby configuring the source to overwrite files or to just delete the file in the underlying\nstorage manually.\n\nThis works by downloading the file to a temporary file, and then ingesting it into\nthe file cache. In the case of a filesystem-backed file cache, this is a file\ncopy. In the case of a object-store-backed file cache, this is an upload.\n\nIn order to work this must be executed within a mode that provides an `s3`\nand `file_cache` resource.\n    ", "metadata": [], "name": "cache_file_from_s3", "requiredResources": [{"resourceKey": "file_cache"}, {"resourceKey": "s3"}]}, "inputs": [{"definition": {"description": null, "name": "s3_coordinate", "type": {"description": null, "displayName": "S3Coordinate", "name": "S3Coordinate"}}, "dependsOn": []}], "name": "cache_file_from_s3", "outputs": [{"definition": {"description": null, "name": "result", "type": {"description": "A file handle a reference to a file, which could be resident in the local file\n    system, an object store, or any arbitrary place where file can be stored.\n\n    This exists to handle the very common case where you wish to write a computation that reads,\n    transforms, and writes files, but is written in a way where the same code could work in local\n    developement as well as in a cluster where the files would be stored in globally available\n    object store such as s3.\n    ", "displayName": "FileHandle", "name": "FileHandle"}}, "dependedBy": [{"definition": {"name": "archive_file_handle", "type": {"displayName": "FileHandle"}}, "solid": {"name": "unzip_file_handle"}}]}]}}, {"handleID": "process_q2_ticket_data.s3_to_df.ingest_csv_file_handle_to_spark", "parent": {"handleID": "process_q2_ticket_data.s3_to_df"}, "solid": {"definition": {"__typename": "SolidDefinition", "configDefinition": null, "description": "Take a file handle that contains a csv with headers and load it\ninto a Spark DataFrame. It infers header names but does *not* infer schema.\n\nIt also ensures that the column names are valid parquet column names by\nfiltering out any of the following characters from column names:\n\nCharacters (within quotations): \"`[ ,;{}()\\n\\t=]`\"\n\n", "metadata": [], "name": "ingest_csv_file_handle_to_spark", "requiredResources": [{"resourceKey": "spark"}]}, "inputs": [{"definition": {"description": null, "name": "csv_file_handle", "type": {"description": "A file handle a reference to a file, which could be resident in the local file\n    system, an object store, or any arbitrary place where file can be stored.\n\n    This exists to handle the very common case where you wish to write a computation that reads,\n    transforms, and writes files, but is written in a way where the same code could work in local\n    developement as well as in a cluster where the files would be stored in globally available\n    object store such as s3.\n    ", "displayName": "FileHandle", "name": "FileHandle"}}, "dependsOn": [{"definition": {"name": "result", "type": {"displayName": "FileHandle"}}, "solid": {"name": "unzip_file_handle"}}]}], "name": "ingest_csv_file_handle_to_spark", "outputs": [{"definition": {"description": null, "name": "result", "type": {"description": "A Pyspark data frame.", "displayName": "DataFrame", "name": "DataFrame"}}, "dependedBy": []}]}}, {"handleID": "process_q2_ticket_data.s3_to_df.unzip_file_handle", "parent": {"handleID": "process_q2_ticket_data.s3_to_df"}, "solid": {"definition": {"__typename": "SolidDefinition", "configDefinition": null, "description": "Unzip a file that is resident in an archive file as a member.\n    This solid operates on FileHandles, meaning that their physical is dependent\n    on what system storage is operating in the pipeline. The physical file could\n    be on local disk, or it could be in s3. If on s3, this solid will download\n    that file to local disk, perform the unzip, upload that file back to s3, and\n    then return that file handle for downstream use in the computations.\n    ", "metadata": [], "name": "unzip_file_handle", "requiredResources": []}, "inputs": [{"definition": {"description": null, "name": "archive_file_handle", "type": {"description": "A file handle a reference to a file, which could be resident in the local file\n    system, an object store, or any arbitrary place where file can be stored.\n\n    This exists to handle the very common case where you wish to write a computation that reads,\n    transforms, and writes files, but is written in a way where the same code could work in local\n    developement as well as in a cluster where the files would be stored in globally available\n    object store such as s3.\n    ", "displayName": "FileHandle", "name": "FileHandle"}}, "dependsOn": [{"definition": {"name": "result", "type": {"displayName": "FileHandle"}}, "solid": {"name": "cache_file_from_s3"}}]}, {"definition": {"description": null, "name": "archive_member", "type": {"description": null, "displayName": "String", "name": "String"}}, "dependsOn": []}], "name": "unzip_file_handle", "outputs": [{"definition": {"description": null, "name": "result", "type": {"description": "A file handle a reference to a file, which could be resident in the local file\n    system, an object store, or any arbitrary place where file can be stored.\n\n    This exists to handle the very common case where you wish to write a computation that reads,\n    transforms, and writes files, but is written in a way where the same code could work in local\n    developement as well as in a cluster where the files would be stored in globally available\n    object store such as s3.\n    ", "displayName": "FileHandle", "name": "FileHandle"}}, "dependedBy": [{"definition": {"name": "csv_file_handle", "type": {"displayName": "FileHandle"}}, "solid": {"name": "ingest_csv_file_handle_to_spark"}}]}]}}, {"handleID": "process_q2_ticket_data.subsample_spark_dataset", "parent": {"handleID": "process_q2_ticket_data"}, "solid": {"definition": {"__typename": "SolidDefinition", "configDefinition": {"configType": {"description": "A configuration dictionary with typed fields", "fields": [{"configType": {"key": "Int"}, "description": "The integer percentage of rows to sample from the input dataset.", "isOptional": false, "name": "subsample_pct"}], "innerTypes": [{"description": "", "innerTypes": [], "isList": false, "isNullable": false, "isSelector": false, "key": "Int", "name": "Int"}], "isList": false, "isNullable": false, "isSelector": false, "key": "Dict.161", "name": null}}, "description": "Subsample a spark dataset via the configuration option.", "metadata": [], "name": "subsample_spark_dataset", "requiredResources": []}, "inputs": [{"definition": {"description": null, "name": "data_frame", "type": {"description": "A Pyspark data frame.", "displayName": "DataFrame", "name": "DataFrame"}}, "dependsOn": [{"definition": {"name": "result", "type": {"displayName": "DataFrame"}}, "solid": {"name": "s3_to_df"}}]}], "name": "subsample_spark_dataset", "outputs": [{"definition": {"description": null, "name": "result", "type": {"description": "A Pyspark data frame.", "displayName": "DataFrame", "name": "DataFrame"}}, "dependedBy": [{"definition": {"name": "data_frame", "type": {"displayName": "DataFrame"}}, "solid": {"name": "canonicalize_column_names"}}]}]}}, {"handleID": "process_sfo_weather_data", "parent": null, "solid": {"definition": {"__typename": "SolidDefinition", "configDefinition": null, "description": null, "metadata": [], "name": "process_sfo_weather_data", "requiredResources": []}, "inputs": [{"definition": {"description": null, "name": "sfo_weather_data", "type": {"description": "A Pyspark data frame.", "displayName": "DataFrame", "name": "DataFrame"}}, "dependsOn": [{"definition": {"name": "result", "type": {"displayName": "DataFrame"}}, "solid": {"name": "ingest_q2_sfo_weather"}}]}], "name": "process_sfo_weather_data", "outputs": [{"definition": {"description": null, "name": "result", "type": {"description": "A Pyspark data frame.", "displayName": "DataFrame", "name": "DataFrame"}}, "dependedBy": [{"definition": {"name": "data_frame", "type": {"displayName": "DataFrame"}}, "solid": {"name": "load_q2_sfo_weather"}}]}]}}], "solids": [{"name": "april_on_time_s3_to_df"}, {"name": "download_q2_sfo_weather"}, {"name": "ingest_q2_sfo_weather"}, {"name": "join_q2_data"}, {"name": "june_on_time_s3_to_df"}, {"name": "load_q2_on_time_data"}, {"name": "load_q2_sfo_weather"}, {"name": "master_cord_s3_to_df"}, {"name": "may_on_time_s3_to_df"}, {"name": "process_q2_coupon_data"}, {"name": "process_q2_market_data"}, {"name": "process_q2_ticket_data"}, {"name": "process_sfo_weather_data"}]}}}
