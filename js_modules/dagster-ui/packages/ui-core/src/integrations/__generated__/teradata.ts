/**
 * THIS FILE IS GENERATED BY `yarn generate-integration-docs`.
 *
 * DO NOT EDIT MANUALLY.
 */

import {IntegrationFrontmatter} from '../types';
import teradataLogo from './logos/teradata.svg';

export const logo = teradataLogo;

export const frontmatter: IntegrationFrontmatter = {
  id: 'teradata',
  status: 'published',
  name: 'Teradata',
  title: 'Dagster & Teradata',
  excerpt:
    'The community-supported `dagster-teradata` package provides an integration with Teradata.',
  partnerlink: '',
  categories: [],
  enabledBy: [],
  enables: [],
  tags: [],
};

export const content =
  'The community-supported `dagster-teradata` package provides an integration with Teradata Vantage.\n\nFor more information, see the [dagster-teradata GitHub repository](https://github.com/dagster-io/community-integrations/tree/main/libraries/dagster-teradata).\n\n# dagster-teradata with Teradata Vantage\n\nTo begin integrating Dagster with Teradata Vantage for building and managing ETL pipelines, this guide provides step-by-step instructions on installing and configuring the required packages, setting up a Dagster project, and implementing a pipeline that interacts with Teradata Vantage.\n\n## Prerequisites\n\n- Access to a Teradata Vantage instance.\n\n  :::note\n  If you need a test instance of Vantage, you can provision one for free at [https://clearscape.teradata.com](https://clearscape.teradata.com/sign-in?utm_source=dev_portal&utm_medium=quickstart_tutorial&utm_campaign=quickstarts)\n  :::\n\n- Python **3.9** or higher, Python **3.12** is recommended.\n- pip\n\n## Install dagster-teradata\n\nWith your virtual environment active, the next step is to install dagster and the Teradata provider package (dagster-teradata) to interact with Teradata Vantage.\n\n1. Install the Required Packages:\n\n   ```bash\n   pip install dagster dagster-webserver dagster-teradata\n   ```\n\n2. Note about Optional Dependencies:\n\n   a) `dagster-teradata` relies on dagster-aws for ingesting data from an S3 bucket into Teradata Vantage. Since `dagster-aws` is an optional dependency, users can install it by running:\n\n   ```bash\n   pip install dagster-teradata[aws]\n   ```\n\n   b) `dagster-teradata` also relies on `dagster-azure` for ingesting data from an Azure Blob Storage container into Teradata Vantage. To install this dependency, run:\n\n   ```bash\n   pip install dagster-teradata[azure]\n   ```\n\n3. Verify the Installation:\n\n   To confirm that Dagster is correctly installed, run:\n\n   ```bash\n   dagster –version\n   ```\n\n   If installed correctly, it should show the version of Dagster.\n\n## Initialize a Dagster Project\n\nNow that you have the necessary packages installed, the next step is to create a new Dagster project.\n\n### Scaffold a New Dagster Project\n\nRun the following command:\n\n```bash\ndagster project scaffold --name dagster-quickstart\n```\n\nThis command will create a new project named dagster-quickstart. It will automatically generate the following directory structure:\n\n```bash\ndagster-quickstart\n│   pyproject.toml\n│   README.md\n│   setup.cfg\n│   setup.py\n│\n├───dagster_quickstart\n│       assets.py\n│       definitions.py\n│       __init__.py\n│\n└───dagster_quickstart_tests\n        test_assets.py\n        __init__.py\n```\n\nRefer [here](https://docs.dagster.io/guides/build/projects/dagster-project-file-reference) to know more above this directory structure\n\n## Create Sample Data\n\nTo simulate an ETL pipeline, create a CSV file with sample data that your pipeline will process.\n\n**Create the CSV File:** Inside the dagster_quickstart/data/ directory, create a file named sample_data.csv with the following content:\n\n```bash\nid,name,age,city\n1,Alice,28,New York\n2,Bob,35,San Francisco\n3,Charlie,42,Chicago\n4,Diana,31,Los Angeles\n```\n\nThis file represents sample data that will be used as input for your ETL pipeline.\n\n## Define Assets for the ETL Pipeline\n\nNow, we\'ll define a series of assets for the ETL pipeline inside the assets.py file.\n\nEdit the assets.py File: Open the dagster_quickstart/assets.py file and add the following code to define the pipeline:\n\n```python\nimport pandas as pd\nfrom dagster import asset\n\n@asset(required_resource_keys={"teradata"})\ndef read_csv_file(context):\n    df = pd.read_csv("dagster_quickstart/data/sample_data.csv")\n    context.log.info(df)\n    return df\n\n@asset(required_resource_keys={"teradata"})\ndef drop_table(context):\n    result = context.resources.teradata.drop_table(["tmp_table"])\n    context.log.info(result)\n\n@asset(required_resource_keys={"teradata"})\ndef create_table(context, drop_table):\n    result = context.resources.teradata.execute_query(\'\'\'CREATE TABLE tmp_table (\n                                                            id INTEGER,\n                                                            name VARCHAR(50),\n                                                            age INTEGER,\n                                                            city VARCHAR(50));\'\'\')\n    context.log.info(result)\n\n@asset(required_resource_keys={"teradata"}, deps=[read_csv_file])\ndef insert_rows(context, create_table, read_csv_file):\n    data_tuples = [tuple(row) for row in read_csv_file.to_numpy()]\n    for row in data_tuples:\n        result = context.resources.teradata.execute_query(\n            f"INSERT INTO tmp_table (id, name, age, city) VALUES ({row[0]}, \'{row[1]}\', {row[2]}, \'{row[3]}\');"\n        )\n        context.log.info(result)\n\n@asset(required_resource_keys={"teradata"})\ndef read_table(context, insert_rows):\n    result = context.resources.teradata.execute_query("select * from tmp_table;", True)\n    context.log.info(result)\n\n```\n\nThis Dagster pipeline defines a series of assets that interact with Teradata. It starts by reading data from a CSV file, then drops and recreates a table in Teradata. After that, it inserts rows from the CSV into the table and finally retrieves the data from the table.\n\n## Define the Pipeline Definitions\n\nThe next step is to configure the pipeline by defining the necessary resources and jobs.\n\n**Edit the definitions.py File**: Open dagster_quickstart/definitions.py and define your Dagster pipeline as follows:\n\n```python\nfrom dagster import EnvVar, Definitions\nfrom dagster_teradata import TeradataResource\n\nfrom .assets import read_csv_file, read_table, create_table, drop_table, insert_rows\n\n# Define the pipeline and resources\ndefs = Definitions(\n    assets=[read_csv_file, read_table, create_table, drop_table, insert_rows],\n    resources={\n        "teradata": TeradataResource(\n            host=EnvVar("TERADATA_HOST"),\n            user=EnvVar("TERADATA_USER"),\n            password=EnvVar("TERADATA_PASSWORD"),\n            database=EnvVar("TERADATA_DATABASE"),\n        )\n    }\n)\n```\n\nThis code sets up a Dagster project that interacts with Teradata by defining assets and resources\n\n1. It imports necessary modules, including pandas, Dagster, and dagster-teradata.\n2. It imports asset functions (read_csv_file, read_table, create_table, drop_table, insert_rows) from the assets.py module.\n3. It registers these assets with Dagster using Definitions, allowing Dagster to track and execute them.\n4. It defines a Teradata resource (TeradataResource) that reads database connection details from environment variables (TERADATA_HOST, TERADATA_USER, TERADATA_PASSWORD, TERADATA_DATABASE).\n\n## Running the Pipeline\n\nAfter setting up the project, you can now run your Dagster pipeline:\n\n1. **Start the Dagster Dev Server:** In your terminal, navigate to the root directory of your project and run:\n   dagster dev\n   After executing the command dagster dev, the Dagster logs will be displayed directly in the terminal. Any errors encountered during startup will also be logged here. Once you see a message similar to:\n\n   ```bash\n   2025-02-04 09:15:46 +0530 - dagster-webserver - INFO - Serving dagster-webserver on http://127.0.0.1:3000 in process 32564,\n   ```\n\n   It indicates that the Dagster webserver is running successfully. At this point, you can proceed to the next step.\n\n   <br />\n\n2. **Access the Dagster UI:** Open a web browser and navigate to http://127.0.0.1:3000. This will open the Dagster UI where you can manage and monitor your pipelines.\n   <br />\n3. **Run the Pipeline:**\n\n- In the top navigation of the Dagster UI, click Assets > View global asset lineage.\n- Click Materialize to execute the pipeline.\n- In the popup window, click View to see the details of the pipeline run.\n  <br />\n\n4. **Monitor the Run:** The Dagster UI allows you to visualize the pipeline\'s progress, view logs, and inspect the status of each step. You can switch between different views to see the execution logs and metadata for each asset.\n\n## Below are some of the operations provided by the TeradataResource:\n\n### 1. Execute a Query (`execute_query`)\n\nThis operation executes a SQL query within Teradata Vantage.\n\n**Args:**\n\n- `sql` (str) – The query to be executed.\n- `fetch_results` (bool, optional) – If True, fetch the query results. Defaults to False.\n- `single_result_row` (bool, optional) – If True, return only the first row of the result set. Effective only if `fetch_results` is True. Defaults to False.\n\n### 2. Execute Multiple Queries (`execute_queries`)\n\nThis operation executes a series of SQL queries within Teradata Vantage.\n\n**Args:**\n\n- `sql_queries` (Sequence[str]) – List of queries to be executed in series.\n- `fetch_results` (bool, optional) – If True, fetch the query results. Defaults to False.\n- `single_result_row` (bool, optional) – If True, return only the first row of the result set. Effective only if `fetch_results` is True. Defaults to False.\n\n### 3. Drop a Database (`drop_database`)\n\nThis operation drops one or more databases from Teradata Vantage.\n\n**Args:**\n\n- `databases` (Union[str, Sequence[str]]) – Database name or list of database names to drop.\n\n### 4. Drop a Table (`drop_table`)\n\nThis operation drops one or more tables from Teradata Vantage.\n\n**Args:**\n\n- `tables` (Union[str, Sequence[str]]) – Table name or list of table names to drop.\n\n---\n\n## Data Transfer from AWS S3 to Teradata Vantage Using dagster-teradata:\n\n```python\nimport os\n\nfrom dagster import job, op, Definitions, EnvVar, DagsterError\nfrom dagster_aws.s3 import S3Resource, s3_resource\nfrom dagster_teradata import TeradataResource, teradata_resource\n\ns3_resource = S3Resource(\n    aws_access_key_id=os.getenv("AWS_ACCESS_KEY_ID"),\n    aws_secret_access_key=os.getenv("AWS_SECRET_ACCESS_KEY"),\n    aws_session_token=os.getenv("AWS_SESSION_TOKEN"),\n)\n\ntd_resource = TeradataResource(\n    host=os.getenv("TERADATA_HOST"),\n    user=os.getenv("TERADATA_USER"),\n    password=os.getenv("TERADATA_PASSWORD"),\n    database=os.getenv("TERADATA_DATABASE"),\n)\n\n@op(required_resource_keys={"teradata"})\ndef drop_existing_table(context):\n     context.resources.teradata.drop_table("people")\n     return "Tables Dropped"\n\n@op(required_resource_keys={"teradata", "s3"})\ndef ingest_s3_to_teradata(context, status):\n    if status == "Tables Dropped":\n        context.resources.teradata.s3_to_teradata(s3_resource, os.getenv("AWS_S3_LOCATION"), "people")\n    else:\n        raise DagsterError("Tables not dropped")\n\n@job(resource_defs={"teradata": td_resource, "s3": s3_resource})\ndef example_job():\n     ingest_s3_to_teradata(drop_existing_table())\n\ndefs = Definitions(\n    jobs=[example_job]\n)\n```\n\nThe `s3_to_teradata` method is used to load data from an S3 bucket into a Teradata table. It leverages Teradata Vantage Native Object Store (NOS), which allows direct querying and loading of external object store data (like AWS S3) into Teradata tables.\n\n### Arguments Supported by `s3_blob_to_teradata`\n\n- **s3 (S3Resource)**:\n  The `S3Resource` object used to interact with the S3 bucket.\n\n- **s3_source_key (str)**:\n  The URI specifying the location of the S3 bucket. The URI format is:\n  `/s3/YOUR-BUCKET.s3.amazonaws.com/YOUR-BUCKET-NAME`\n  For more details, refer to:\n  [Teradata Documentation - Native Object Store](https://docs.teradata.com/search/documents?query=native+object+store&sort=last_update&virtual-field=title_only&content-lang=en-US)\n\n- **teradata_table (str)**:  \n  The name of the Teradata table to which the data will be loaded.\n\n- **public_bucket (bool)**:  \n  Indicates whether the provided S3 bucket is public. If `True`, the objects within the bucket can be accessed via a URL without authentication. If `False`, the bucket is considered private, and authentication must be provided.  \n  Defaults to `False`.\n\n- **teradata_authorization_name (str)**:  \n  The name of the Teradata Authorization Database Object, which controls access to the S3 object store.  \n  For more details, refer to:\n  [Teradata Vantage Native Object Store - Setting Up Access](https://docs.teradata.com/r/Enterprise_IntelliFlex_VMware/Teradata-VantageTM-Native-Object-Store-Getting-Started-Guide-17.20/Setting-Up-Access/Controlling-Foreign-Table-Access-with-an-AUTHORIZATION-Object)\n\n---\n\n## Data Transfer from Azure Blob to Teradata Vantage Using dagster-teradata:\n\n```python\nimport os\n\nfrom dagster import job, op, Definitions, EnvVar, DagsterError\nfrom dagster_azure.adls2 import ADLS2Resource, ADLS2SASToken\nfrom dagster_teradata import TeradataResource, teradata_resource\n\nazure_resource = ADLS2Resource(\n    storage_account="",\n    credential=ADLS2SASToken(token=""),\n)\n\ntd_resource = TeradataResource(\n    host=os.getenv("TERADATA_HOST"),\n    user=os.getenv("TERADATA_USER"),\n    password=os.getenv("TERADATA_PASSWORD"),\n    database=os.getenv("TERADATA_DATABASE"),\n)\n\n@op(required_resource_keys={"teradata"})\ndef drop_existing_table(context):\n     context.resources.teradata.drop_table("people")\n     return "Tables Dropped"\n\n@op(required_resource_keys={"teradata", "azure"})\ndef ingest_azure_to_teradata(context, status):\n    if status == "Tables Dropped":\n        context.resources.teradata.azure_blob_to_teradata(azure_resource, "/az/akiaxox5jikeotfww4ul.blob.core.windows.net/td-usgs/CSVDATA/09380000/2018/06/", "people", True)\n    else:\n        raise DagsterError("Tables not dropped")\n\n@job(resource_defs={"teradata": td_resource, "azure": azure_resource})\ndef example_job():\n     ingest_azure_to_teradata(drop_existing_table())\n\ndefs = Definitions(\n    jobs=[example_job]\n)\n```\n\nThe `azure_blob_to_teradata` method is used to load data from Azure Data Lake Storage (ADLS) into a Teradata table. This method leverages Teradata Vantage Native Object Store (NOS) to directly query and load external object store data (such as Azure Blob Storage) into Teradata.\n\n### Arguments Supported by `azure_blob_to_teradata`\n\n- **azure (ADLS2Resource)**:  \n  The `ADLS2Resource` object used to interact with the Azure Blob Storage.\n\n- **blob_source_key (str)**:  \n  The URI specifying the location of the Azure Blob object. The format is:\n  `/az/YOUR-STORAGE-ACCOUNT.blob.core.windows.net/YOUR-CONTAINER/YOUR-BLOB-LOCATION`  \n  For more details, refer to the Teradata documentation:  \n  [Teradata Documentation - Native Object Store](https://docs.teradata.com/search/documents?query=native+object+store&sort=last_update&virtual-field=title_only&content-lang=en-US)\n\n- **teradata_table (str)**:  \n  The name of the Teradata table where the data will be loaded.\n\n- **public_bucket (bool, optional)**:  \n  Indicates whether the Azure Blob container is public. If `True`, the objects in the container can be accessed without authentication.  \n  Defaults to `False`.\n\n- **teradata_authorization_name (str, optional)**:  \n  The name of the Teradata Authorization Database Object used to control access to the Azure Blob object store. This is required for secure access to private containers.  \n  Defaults to an empty string.  \n  For more details, refer to the documentation:  \n  [Teradata Vantage Native Object Store - Setting Up Access](https://docs.teradata.com/r/Enterprise_IntelliFlex_VMware/Teradata-VantageTM-Native-Object-Store-Getting-Started-Guide-17.20/Setting-Up-Access/Controlling-Foreign-Table-Access-with-an-AUTHORIZATION-Object)\n\n### Transfer data from Private Blob Storage Container to Teradata instance\n\nTo successfully transfer data from a Private Blob Storage Container to a Teradata instance, the following prerequisites are necessary.\n\n- An Azure account. You can start with a [free account](https://azure.microsoft.com/free/).\n- Create an [Azure storage account](https://docs.microsoft.com/en-us/azure/storage/common/storage-quickstart-create-account?tabs=azure-portal)\n- Create a [blob container](https://learn.microsoft.com/en-us/azure/storage/blobs/blob-containers-portal) under Azure storage account\n- [Upload](https://learn.microsoft.com/en-us/azure/storage/blobs/storage-quickstart-blobs-portal) CSV/JSON/Parquest format files to blob container\n- Create a Teradata Authorization object with the Azure Blob Storage Account and the Account Secret Key\n\n  ```sql\n  CREATE AUTHORIZATION azure_authorization USER \'azuretestquickstart\' PASSWORD \'AZURE_BLOB_ACCOUNT_SECRET_KEY\'\n  ```\n\n  :::note\n  Replace `AZURE_BLOB_ACCOUNT_SECRET_KEY` with Azure storage account `azuretestquickstart` [access key](https://learn.microsoft.com/en-us/azure/storage/common/storage-account-keys-manage?toc=%2Fazure%2Fstorage%2Fblobs%2Ftoc.json&bc=%2Fazure%2Fstorage%2Fblobs%2Fbreadcrumb%2Ftoc.json&tabs=azure-portal)\n  :::\n\n---\n\n## Manage VantageCloud Lake Compute Clusters with dagster-teradata:\n\n```python\nfrom dagster import Definitions, DagsterError, op, materialize, job\nfrom dagster_dbt import DbtCliResource\nfrom dagster_teradata import teradata_resource, TeradataResource\n\nfrom .assets import jaffle_shop_dbt_assets\nfrom .project import jaffle_shop_project\nfrom .schedules import schedules\n\n@op(required_resource_keys={"teradata"})\ndef create_compute_cluster(context):\n    context.resources.teradata.create_teradata_compute_cluster(\n        "ShippingCG01",\n        "Shipping",\n        "STANDARD",\n        "TD_COMPUTE_MEDIUM",\n        "MIN_COMPUTE_COUNT(1) MAX_COMPUTE_COUNT(1) INITIALLY_SUSPENDED(\'FALSE\')",\n    )\n    return "Compute Cluster Created"\n\n@op(required_resource_keys={"teradata", "dbt"})\ndef run_dbt(context, status):\n    if status == "Compute Cluster Created":\n        materialize(\n            [jaffle_shop_dbt_assets],\n            resources={\n                "dbt": DbtCliResource(project_dir=jaffle_shop_project)\n            }\n        )\n        return "DBT Run Completed"\n    else:\n        raise DagsterError("DBT Run Failed")\n\n@op(required_resource_keys={"teradata"})\ndef drop_compute_cluster(context, status):\n    if status == "DBT Run Completed":\n        context.resources.teradata.drop_teradata_compute_cluster("ShippingCG01", "Shipping", True)\n    else:\n        raise DagsterError("DBT Run Failed")\n\n@job(resource_defs={"teradata": teradata_resource, "dbt": DbtCliResource})\ndef example_job():\n    drop_compute_cluster(run_dbt(create_compute_cluster()))\n\ndefs = Definitions(\n    assets=[jaffle_shop_dbt_assets],\n    jobs=[example_job],\n    schedules=schedules,\n    resources={\n        "dbt": DbtCliResource(project_dir=jaffle_shop_project),\n        "teradata": TeradataResource(),\n    },\n)\n```\n\nTeradata VantageCloud Lake provides robust compute cluster management capabilities, enabling users to dynamically allocate, suspend, resume, and delete compute resources. These operations are fully supported through **`dagster-teradata`**, allowing users to manage compute clusters directly within their Dagster pipelines. This integration ensures optimal performance, scalability, and cost efficiency. The following operations facilitate seamless compute cluster management within Dagster:\n\n### 1. Create a Compute Cluster (`create_teradata_compute_cluster`)\n\nThis operation provisions a new compute cluster within Teradata VantageCloud Lake using `dagster-teradata`. It enables users to define the cluster\'s configuration, including compute profiles, resource allocation, and query execution strategies, directly within a Dagster job.\n\n**Args:**\n\n- `compute_profile_name` (str) – Specifies the name of the compute profile.\n- `compute_group_name` (str) – Identifies the compute group to which the profile belongs.\n- `query_strategy` (str, optional, default="STANDARD") – Defines the method used by the Teradata Optimizer to execute SQL queries efficiently. Acceptable values:\n  - `STANDARD` – The default strategy at the database level, optimized for general query execution.\n  - `ANALYTIC` – Optimized for complex analytical workloads.\n- `compute_map` (Optional[str], default=None) – Maps compute resources to specific nodes within the cluster.\n- `compute_attribute` (Optional[str], default=None) – Specifies additional configuration attributes for the compute profile, such as:\n  - `MIN_COMPUTE_COUNT(1) MAX_COMPUTE_COUNT(5) INITIALLY_SUSPENDED(\'FALSE\')`\n- `timeout` (int, optional, default=constants.CC_OPR_TIME_OUT) – The maximum duration (in seconds) to wait for the cluster creation process to complete. Default: 20 minutes.\n\n### 2. Suspend a Compute Cluster (`suspend_teradata_compute_cluster`)\n\nThis operation temporarily suspends a compute cluster within Teradata VantageCloud Lake using **`dagster-teradata`**, reducing resource consumption while retaining the compute profile for future use.\n\n**Args:**\n\n- `compute_profile_name` (str) – Specifies the name of the compute profile.\n- `compute_group_name` (str) – Identifies the compute group associated with the profile.\n- `timeout` (int, optional, default=constants.CC_OPR_TIME_OUT) – The maximum wait time for the suspension process to complete. Default: 20 minutes.\n\n### 3. Resume a Compute Cluster (`resume_teradata_compute_cluster`)\n\nThis operation restores a previously suspended compute cluster using **`dagster-teradata`**, allowing workloads to resume execution within a Dagster pipeline.\n\n**Args:**\n\n- `compute_profile_name` (str) – Specifies the name of the compute profile.\n- `compute_group_name` (str) – Identifies the compute group associated with the profile.\n- `timeout` (int, optional, default=constants.CC_OPR_TIME_OUT) – The maximum wait time for the resumption process to complete. Default: 20 minutes.\n\n### 4. Delete a Compute Cluster (`drop_teradata_compute_cluster`)\n\nThis operation removes a compute cluster from Teradata VantageCloud Lake using **`dagster-teradata`**, with an option to delete the associated compute group. You can run this operation directly from your Dagster workflow.\n\n**Args:**\n\n- `compute_profile_name` (str) – Specifies the name of the compute profile.\n- `compute_group_name` (str) – Identifies the compute group associated with the profile.\n- `delete_compute_group` (bool, optional, default=False) – Determines whether the compute group should be deleted:\n  - `True` – Deletes the compute group.\n  - `False` – Retains the compute group without modifications.\n\nThese operations are designed to be fully integrated into **`dagster-teradata`** for managing compute clusters in Teradata VantageCloud Lake. By utilizing these operations within Dagster jobs, users can optimize resource allocation, perform complex transformations, and automate compute cluster management to align with workload demands.\n\n---\n\n## Further reading\n\n- [dagster-teradata with Teradata Vantage](https://developers.teradata.com/quickstarts/manage-data/use-dagster-with-teradata-vantage/)\n- [Data Transfer from AWS S3 to Teradata Vantage Using dagster-teradata](https://developers.teradata.com/quickstarts/manage-data/dagster-teradata-s3-to-teradata-transfer/)\n- [Data Transfer from Azure Blob to Teradata Vantage Using dagster-teradata](https://developers.teradata.com/quickstarts/manage-data/dagster-teradata-azure-to-teradata-transfer/)\n- [Manage VantageCloud Lake Compute Clusters with dagster-teradata](https://developers.teradata.com/quickstarts/vantagecloud-lake/vantagecloud-lake-compute-cluster-dagster/)\n- [Teradata Authorization](https://docs.teradata.com/r/Enterprise_IntelliFlex_VMware/SQL-Data-Definition-Language-Syntax-and-Examples/Authorization-Statements-for-External-Routines/CREATE-AUTHORIZATION-and-REPLACE-AUTHORIZATION)\n- [Teradata VantageCloud Lake Compute Clusters](https://docs.teradata.com/r/Teradata-VantageCloud-Lake/Managing-Compute-Resources/Compute-Clusters)';
