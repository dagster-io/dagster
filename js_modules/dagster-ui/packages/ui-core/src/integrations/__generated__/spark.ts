/**
 * THIS FILE IS GENERATED BY `yarn generate-integration-docs`.
 *
 * DO NOT EDIT MANUALLY.
 */

import {IntegrationFrontmatter} from '../types';
import sparkLogo from './logos/spark.svg';

export const logo = sparkLogo;

export const frontmatter: IntegrationFrontmatter = {
  id: 'spark',
  status: 'published',
  name: 'Spark',
  title: 'Dagster & Spark',
  excerpt: 'Configure and run Spark jobs.',
  partnerlink: '',
  categories: ['Compute'],
  enabledBy: ['dagster-pyspark'],
  enables: [],
  tags: ['dagster-supported', 'compute'],
};

export const content =
  "Spark jobs typically execute on infrastructure that's specialized for Spark. Spark applications are typically not containerized or executed on Kubernetes.\n\nRunning Spark code often requires submitting code to a Databricks or EMR cluster. `dagster-pyspark` provides a Spark class with methods for configuration and constructing the `spark-submit` command for a Spark job.\n\n### About Apache Spark\n\n**Apache Spark** is an open source unified analytics engine for large-scale data processing. Spark provides an interface for programming clusters with implicit data parallelism and fault tolerance. It also provides libraries for graph computation, SQL for structured data processing, ML, and data science.\n\n## Using Dagster Pipes to run Spark jobs\n\n[Dagster pipes](/guides/build/external-pipelines/) is our toolkit for orchestrating remote compute from Dagster. It allows you to run code outside of the Dagster process, and stream logs and events back to Dagster. This is the recommended approach for running Spark jobs.\n\nWith Pipes, the code inside the asset or op definition submits a Spark job to an external system like Databricks or AWS EMR, usually pointing to a jar or zip of Python files that contain the actual Spark data transformations and actions.\n\nYou can either use one of the available Pipes Clients or make your own. The available Pipes Clients for popular Spark providers are:\n\n- [Databricks](/guides/build/external-pipelines/databricks-pipeline)\n- [AWS Glue](/guides/build/external-pipelines/aws/aws-glue-pipeline)\n- [AWS EMR](/guides/build/external-pipelines/aws/aws-emr-pipeline)\n- [AWS EMR on EKS](/guides/build/external-pipelines/aws/aws-emr-containers-pipeline)\n- [AWS EMR Serverless](/guides/build/external-pipelines/aws/aws-emr-serverless-pipeline)\n\nExisting Spark jobs can be used with Pipes without any modifications. In this case, Dagster will be receiving logs from the job, but not events like asset checks or attached metadata.\n\nAdditionally, it's possible to send events to Dagster from the job by utilizing the `dagster_pipes` module. This requires minimal code changes on the job side.\n\nThis approach also works for Spark jobs written in Java or Scala, although we don't have Pipes implementations for emitting events from those languages yet.";
