---
title: 'gcp (dagster-gcp)'
title_meta: 'gcp (dagster-gcp) API Documentation - Build Better Data Pipelines | Python Reference Documentation for Dagster'
description: 'gcp (dagster-gcp) Dagster API | Comprehensive Python API documentation for Dagster, the data orchestration platform. Learn how to build, test, and maintain data pipelines with our detailed guides and examples.'
last_update:
  date: '2025-01-23'
---

<div class="section" id="gcp-dagster-gcp">


# GCP (dagster-gcp)

<div class="section" id="bigquery">


## BigQuery

Related Guides:

  - [Using Dagster with BigQuery](https://docs.dagster.io/integrations/bigquery)
  - [BigQuery I/O manager reference](https://docs.dagster.io/integrations/bigquery/reference)


<div class="section" id="bigquery-resource">


### BigQuery Resource

<dl>
    <dt><Link id='dagster_gcp.BigQueryResource'>dagster_gcp.BigQueryResource ResourceDefinition</Link></dt>
    <dd>

        <div className='lineblock'> </div>

    Resource for interacting with Google BigQuery.

    Examples:

        ```python
        from dagster import Definitions, asset
        from dagster_gcp import BigQueryResource

        @asset
        def my_table(bigquery: BigQueryResource):
            with bigquery.get_client() as client:
                client.query("SELECT * FROM my_dataset.my_table")

        defs = Definitions(
            assets=[my_table],
            resources=\{
                "bigquery": BigQueryResource(project="my-project")
            }
        )
        ```

    </dd>

</dl>
</div>


<div class="section" id="bigquery-i-o-manager">


### BigQuery I/O Manager

<dl>
    <dt><Link id='dagster_gcp.BigQueryIOManager'>dagster_gcp.BigQueryIOManager IOManagerDefinition</Link></dt>
    <dd>

        <div className='lineblock'> </div>

    Base class for an I/O manager definition that reads inputs from and writes outputs to BigQuery.

    Examples:

        ```python
        from dagster_gcp import BigQueryIOManager
        from dagster_bigquery_pandas import BigQueryPandasTypeHandler
        from dagster import Definitions, EnvVar

        class MyBigQueryIOManager(BigQueryIOManager):
            @staticmethod
            def type_handlers() -> Sequence[DbTypeHandler]:
                return [BigQueryPandasTypeHandler()]

        @asset(
            key_prefix=["my_dataset"]  # my_dataset will be used as the dataset in BigQuery
        )
        def my_table() -> pd.DataFrame:  # the name of the asset will be the table name
            ...

        defs = Definitions(
            assets=[my_table],
            resources=\{
                "io_manager": MyBigQueryIOManager(project=EnvVar("GCP_PROJECT"))
            }
        )
        ```
    You can set a default dataset to store the assets using the `dataset` configuration value of the BigQuery I/O
    Manager. This dataset will be used if no other dataset is specified directly on an asset or op.

        ```python
        defs = Definitions(
            assets=[my_table],
            resources=\{
                    "io_manager": MyBigQueryIOManager(project=EnvVar("GCP_PROJECT"), dataset="my_dataset")
                }
        )
        ```
    On individual assets, you an also specify the dataset where they should be stored using metadata or
    by adding a `key_prefix` to the asset key. If both `key_prefix` and metadata are defined, the metadata will
    take precedence.

        ```python
        @asset(
            key_prefix=["my_dataset"]  # will be used as the dataset in BigQuery
        )
        def my_table() -> pd.DataFrame:
            ...

        @asset(
            # note that the key needs to be "schema"
            metadata=\{"schema": "my_dataset"}  # will be used as the dataset in BigQuery
        )
        def my_other_table() -> pd.DataFrame:
            ...
        ```
    For ops, the dataset can be specified by including a “schema” entry in output metadata.

        ```python
        @op(
            out=\{"my_table": Out(metadata=\{"schema": "my_schema"})}
        )
        def make_my_table() -> pd.DataFrame:
            ...
        ```
    If none of these is provided, the dataset will default to “public”.

    To only use specific columns of a table as input to a downstream op or asset, add the metadata `columns` to the
    [`In`](../ops.mdx#dagster.In) or [`AssetIn`](../assets.mdx#dagster.AssetIn).

        ```python
        @asset(
            ins=\{"my_table": AssetIn("my_table", metadata=\{"columns": ["a"]})}
        )
        def my_table_a(my_table: pd.DataFrame) -> pd.DataFrame:
            # my_table will just contain the data from column "a"
            ...
        ```
    If you cannot upload a file to your Dagster deployment, or otherwise cannot
    [authenticate with GCP](https://cloud.google.com/docs/authentication/provide-credentials-adc)
    via a standard method, you can provide a service account key as the `gcp_credentials` configuration.
    Dagster will store this key in a temporary file and set `GOOGLE_APPLICATION_CREDENTIALS` to point to the file.
    After the run completes, the file will be deleted, and `GOOGLE_APPLICATION_CREDENTIALS` will be
    unset. The key must be base64 encoded to avoid issues with newlines in the keys. You can retrieve
    the base64 encoded with this shell command: `cat $GOOGLE_APPLICATION_CREDENTIALS | base64`


    </dd>

</dl>
</div>


<div class="section" id="bigquery-ops">


### BigQuery Ops

<dl>
    <dt><Link id='dagster_gcp.bq_create_dataset'>dagster_gcp.bq_create_dataset</Link></dt>
    <dd>

    BigQuery Create Dataset.

    This op encapsulates creating a BigQuery dataset.

    Expects a BQ client to be provisioned in resources as context.resources.bigquery.


    </dd>

</dl>
<dl>
    <dt><Link id='dagster_gcp.bq_delete_dataset'>dagster_gcp.bq_delete_dataset</Link></dt>
    <dd>

    BigQuery Delete Dataset.

    This op encapsulates deleting a BigQuery dataset.

    Expects a BQ client to be provisioned in resources as context.resources.bigquery.


    </dd>

</dl>
<dl>
    <dt><Link id='dagster_gcp.bq_op_for_queries'>dagster_gcp.bq_op_for_queries</Link></dt>
    <dd>

    Executes BigQuery SQL queries.

    Expects a BQ client to be provisioned in resources as context.resources.bigquery.


    </dd>

</dl>
<dl>
    <dt><Link id='dagster_gcp.import_df_to_bq'>dagster_gcp.import_df_to_bq</Link></dt>
    <dd>

    </dd>

</dl>
<dl>
    <dt><Link id='dagster_gcp.import_file_to_bq'>dagster_gcp.import_file_to_bq</Link></dt>
    <dd>

    </dd>

</dl>
<dl>
    <dt><Link id='dagster_gcp.import_gcs_paths_to_bq'>dagster_gcp.import_gcs_paths_to_bq</Link></dt>
    <dd>

    </dd>

</dl>
</div>


<div class="section" id="data-freshness">


### Data Freshness

<dl>
    <dt><Link id='dagster_gcp.fetch_last_updated_timestamps'>dagster_gcp.fetch_last_updated_timestamps</Link></dt>
    <dd>

    Get the last updated timestamps of a list BigQuery table.

    Note that this only works on BigQuery tables, and not views.

    Parameters: 
      - <strong>client</strong> (<em>bigquery.Client</em>) – The BigQuery client.
      - <strong>dataset_id</strong> (<em>str</em>) – The BigQuery dataset ID.
      - <strong>table_ids</strong> (<em>Sequence</em><em>[</em><em>str</em><em>]</em>) – The table IDs to get the last updated timestamp for.


    Returns: A mapping of table IDs to their last updated timestamps (UTC).Return type: Mapping[str, datetime]

    </dd>

</dl>
</div>


<div class="section" id="other">


### Other

<dl>
    <dt><Link id='dagster_gcp.BigQueryError'>class dagster_gcp.BigQueryError</Link></dt>
    <dd>

    </dd>

</dl>
</div></div>


<div class="section" id="gcs">


## GCS

<div class="section" id="gcs-resource">


### GCS Resource

<dl>
    <dt><Link id='dagster_gcp.GCSResource'>dagster_gcp.GCSResource ResourceDefinition</Link></dt>
    <dd>

        <div className='lineblock'> </div>

    Resource for interacting with Google Cloud Storage.

    Example:

        ```default
        @asset
        def my_asset(gcs: GCSResource):
            with gcs.get_client() as client:
                # client is a google.cloud.storage.Client
                ...
        ```

    </dd>

</dl>
</div>


<div class="section" id="gcs-i-o-manager">


### GCS I/O Manager

<dl>
    <dt><Link id='dagster_gcp.GCSPickleIOManager'>dagster_gcp.GCSPickleIOManager IOManagerDefinition</Link></dt>
    <dd>

        <div className='lineblock'> </div>

    Persistent IO manager using GCS for storage.

    Serializes objects via pickling. Suitable for objects storage for distributed executors, so long
    as each execution node has network connectivity and credentials for GCS and the backing bucket.

    Assigns each op output to a unique filepath containing run ID, step key, and output name.
    Assigns each asset to a single filesystem path, at `\<base_dir>/\<asset_key>`. If the asset key
    has multiple components, the final component is used as the name of the file, and the preceding
    components as parent directories under the base_dir.

    Subsequent materializations of an asset will overwrite previous materializations of that asset.
    With a base directory of `/my/base/path`, an asset with key
    `AssetKey(["one", "two", "three"])` would be stored in a file called `three` in a directory
    with path `/my/base/path/one/two/`.

    Example usage:

    1. Attach this IO manager to a set of assets.
        ```python
        from dagster import asset, Definitions
        from dagster_gcp.gcs import GCSPickleIOManager, GCSResource

        @asset
        def asset1():
            # create df ...
            return df

        @asset
        def asset2(asset1):
            return asset1[:5]

        defs = Definitions(
            assets=[asset1, asset2],
            resources=\{
                "io_manager": GCSPickleIOManager(
                    gcs_bucket="my-cool-bucket",
                    gcs_prefix="my-cool-prefix",
                    gcs=GCSResource(project="my-cool-project")
                ),

            }
        )
        ```
    2. Attach this IO manager to your job to make it available to your ops.
        ```python
        from dagster import job
        from dagster_gcp.gcs import GCSPickleIOManager, GCSResource

        @job(
            resource_defs=\{
                "io_manager": GCSPickleIOManager(
                    gcs=GCSResource(project="my-cool-project")
                    gcs_bucket="my-cool-bucket",
                    gcs_prefix="my-cool-prefix"
                ),
            }
        )
        def my_job():
            ...
        ```

    </dd>

</dl>
</div>


<div class="section" id="gcs-sensor">


### GCS Sensor

<dl>
    <dt><Link id='dagster_gcp.gcs.sensor.get_gcs_keys'>dagster_gcp.gcs.sensor.get_gcs_keys</Link></dt>
    <dd>

    Return a list of updated keys in a GCS bucket.

    Parameters: 
      - <strong>bucket</strong> (<em>str</em>) – The name of the GCS bucket.
      - <strong>prefix</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) – The prefix to filter the keys by.
      - <strong>since_key</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) – The key to start from. If provided, only keys updated after this key will be returned.
      - <strong>gcs_session</strong> (<em>Optional</em><em>[</em><em>google.cloud.storage.client.Client</em><em>]</em>) – A GCS client session. If not provided, a new session will be created.


    Returns: A list of keys in the bucket, sorted by update time, that are newer than the <cite>since_key</cite>.Return type: List[str]
    Example:

        ```python
        @resource
        def google_cloud_storage_client(context):
            return storage.Client().from_service_account_json("my-service-account.json")

        @sensor(job=my_job, required_resource_keys=\{"google_cloud_storage_client"})
        def my_gcs_sensor(context):
            since_key = context.cursor or None
            new_gcs_keys = get_gcs_keys(
                "my-bucket",
                prefix="data",
                since_key=since_key,
                gcs_session=context.resources.google_cloud_storage_client
            )

            if not new_gcs_keys:
                return SkipReason("No new gcs files found for bucket 'my-bucket'.")

            for gcs_key in new_gcs_keys:
                yield RunRequest(run_key=gcs_key, run_config=\{
                    "ops": \{
                        "gcs_files": \{
                            "config": \{
                                "gcs_key": gcs_key
                            }
                        }
                    }
                })

            last_key = new_gcs_keys[-1]
            context.update_cursor(last_key)
        ```

    </dd>

</dl>
</div>


<div class="section" id="file-manager-experimental">


### File Manager (Experimental)

<dl>
    <dt><Link id='dagster_gcp.GCSFileHandle'>class dagster_gcp.GCSFileHandle</Link></dt>
    <dd>
    A reference to a file on GCS.
    </dd>

</dl>
<dl>
    <dt><Link id='dagster_gcp.GCSFileManagerResource'>dagster_gcp.GCSFileManagerResource ResourceDefinition</Link></dt>
    <dd>

        <div className='lineblock'> </div>

    FileManager that provides abstract access to GCS.


    </dd>

</dl>
</div>


<div class="section" id="gcs-compute-log-manager">


### GCS Compute Log Manager

<dl>
    <dt><Link id='dagster_gcp.gcs.GCSComputeLogManager'>class dagster_gcp.gcs.GCSComputeLogManager</Link></dt>
    <dd>

    Logs op compute function stdout and stderr to GCS.

    Users should not instantiate this class directly. Instead, use a YAML block in `dagster.yaml`
    such as the following:

        ```YAML
        compute_logs:
          module: dagster_gcp.gcs.compute_log_manager
          class: GCSComputeLogManager
          config:
            bucket: "mycorp-dagster-compute-logs"
            local_dir: "/tmp/cool"
            prefix: "dagster-test-"
            upload_interval: 30
        ```
    There are more configuration examples in the instance documentation guide: [https://docs.dagster.io/deployment/dagster-instance#compute-log-storage](https://docs.dagster.io/deployment/dagster-instance#compute-log-storage)

    Parameters: 
      - <strong>bucket</strong> (<em>str</em>) – The name of the GCS bucket to which to log.
      - <strong>local_dir</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) – Path to the local directory in which to stage logs. Default:
      - <strong>prefix</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) – Prefix for the log file keys.
      - <strong>json_credentials_envvar</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) – Environment variable that contains the JSON with a private key
      - <strong>upload_interval</strong> – (Optional[int]): Interval in seconds to upload partial log files to GCS. By default, will only upload when the capture is complete.
      - <strong>show_url_only</strong> – (Optional[bool]): Only show the URL of the log file in the UI, instead of fetching and displaying the full content. Default False.
      - <strong>inst_data</strong> (<em>Optional</em><em>[</em>[*ConfigurableClassData*](../internals.mdx#dagster._serdes.ConfigurableClassData)<em>]</em>) – Serializable representation of the compute



    </dd>

</dl>
</div></div>


<div class="section" id="dataproc">


## Dataproc

<div class="section" id="dataproc-resource">


### Dataproc Resource

<dl>
    <dt><Link id='dagster_gcp.DataprocResource'>dagster_gcp.DataprocResource ResourceDefinition</Link></dt>
    <dd>

        <div className='lineblock'> </div>

    Resource for connecting to a Dataproc cluster.

    Example:

        ```default
        @asset
        def my_asset(dataproc: DataprocResource):
            with dataproc.get_client() as client:
                # client is a dagster_gcp.DataprocClient
                ...
        ```

    </dd>

</dl>
</div>


<div class="section" id="dataproc-ops">


### Dataproc Ops

<dl>
    <dt><Link id='dagster_gcp.dataproc_op'>dagster_gcp.dataproc_op `=` \<dagster._core.definitions.op_definition.OpDefinition object></Link></dt>
    <dd>

        <div className='lineblock'> </div>


    </dd>

</dl>
</div></div>


<div class="section" id="legacy">

## Legacy

<dl>
    <dt><Link id='dagster_gcp.ConfigurablePickledObjectGCSIOManager'>dagster_gcp.ConfigurablePickledObjectGCSIOManager IOManagerDefinition</Link></dt>
    <dd>

        <div className='lineblock'> </div>

        :::danger[deprecated]
        This API will be removed in version 2.0.
         Please use GCSPickleIOManager instead..

        :::

    Renamed to GCSPickleIOManager. See GCSPickleIOManager for documentation.


    </dd>

</dl>
<dl>
    <dt><Link id='dagster_gcp.bigquery_resource'>dagster_gcp.bigquery_resource ResourceDefinition</Link></dt>
    <dd>

        <div className='lineblock'> </div>


    </dd>

</dl>
<dl>
    <dt><Link id='dagster_gcp.build_bigquery_io_manager'>dagster_gcp.build_bigquery_io_manager IOManagerDefinition</Link></dt>
    <dd>

        <div className='lineblock'> </div>

        :::warning[experimental]
        This API may break in future versions, even between dot releases.


        :::

    Builds an I/O manager definition that reads inputs from and writes outputs to BigQuery.

    Parameters: 
      - <strong>type_handlers</strong> (<em>Sequence</em><em>[</em><em>DbTypeHandler</em><em>]</em>) – Each handler defines how to translate between
      - <strong>default_load_type</strong> (<em>Type</em>) – When an input has no type annotation, load it as this type.


    Returns: IOManagerDefinition
    Examples:

        ```python
        from dagster_gcp import build_bigquery_io_manager
        from dagster_bigquery_pandas import BigQueryPandasTypeHandler
        from dagster import Definitions

        @asset(
            key_prefix=["my_prefix"],
            metadata=\{"schema": "my_dataset"} # will be used as the dataset in BigQuery
        )
        def my_table() -> pd.DataFrame:  # the name of the asset will be the table name
            ...

        @asset(
            key_prefix=["my_dataset"]  # my_dataset will be used as the dataset in BigQuery
        )
        def my_second_table() -> pd.DataFrame:  # the name of the asset will be the table name
            ...

        bigquery_io_manager = build_bigquery_io_manager([BigQueryPandasTypeHandler()])

        defs = Definitions(
            assets=[my_table, my_second_table],
            resources=\{
                "io_manager": bigquery_io_manager.configured(\{
                    "project" : \{"env": "GCP_PROJECT"}
                })
            }
        )
        ```
    You can set a default dataset to store the assets using the `dataset` configuration value of the BigQuery I/O
    Manager. This dataset will be used if no other dataset is specified directly on an asset or op.

        ```python
        defs = Definitions(
            assets=[my_table],
            resources=\{
                    "io_manager": bigquery_io_manager.configured(\{
                        "project" : \{"env": "GCP_PROJECT"}
                        "dataset": "my_dataset"
                    })
                }
        )
        ```
    On individual assets, you an also specify the dataset where they should be stored using metadata or
    by adding a `key_prefix` to the asset key. If both `key_prefix` and metadata are defined, the metadata will
    take precedence.

        ```python
        @asset(
            key_prefix=["my_dataset"]  # will be used as the dataset in BigQuery
        )
        def my_table() -> pd.DataFrame:
            ...

        @asset(
            # note that the key needs to be "schema"
            metadata=\{"schema": "my_dataset"}  # will be used as the dataset in BigQuery
        )
        def my_other_table() -> pd.DataFrame:
            ...
        ```
    For ops, the dataset can be specified by including a “schema” entry in output metadata.

        ```python
        @op(
            out=\{"my_table": Out(metadata=\{"schema": "my_schema"})}
        )
        def make_my_table() -> pd.DataFrame:
            ...
        ```
    If none of these is provided, the dataset will default to “public”.

    To only use specific columns of a table as input to a downstream op or asset, add the metadata `columns` to the
    [`In`](../ops.mdx#dagster.In) or [`AssetIn`](../assets.mdx#dagster.AssetIn).

        ```python
        @asset(
            ins=\{"my_table": AssetIn("my_table", metadata=\{"columns": ["a"]})}
        )
        def my_table_a(my_table: pd.DataFrame) -> pd.DataFrame:
            # my_table will just contain the data from column "a"
            ...
        ```
    If you cannot upload a file to your Dagster deployment, or otherwise cannot
    [authenticate with GCP](https://cloud.google.com/docs/authentication/provide-credentials-adc)
    via a standard method, you can provide a service account key as the `gcp_credentials` configuration.
    Dagster willstore this key in a temporary file and set `GOOGLE_APPLICATION_CREDENTIALS` to point to the file.
    After the run completes, the file will be deleted, and `GOOGLE_APPLICATION_CREDENTIALS` will be
    unset. The key must be base64 encoded to avoid issues with newlines in the keys. You can retrieve
    the base64 encoded with this shell command: `cat $GOOGLE_APPLICATION_CREDENTIALS | base64`


    </dd>

</dl>
<dl>
    <dt><Link id='dagster_gcp.gcs_resource'>dagster_gcp.gcs_resource ResourceDefinition</Link></dt>
    <dd>

        <div className='lineblock'> </div>


    </dd>

</dl>
<dl>
    <dt><Link id='dagster_gcp.gcs_pickle_io_manager'>dagster_gcp.gcs_pickle_io_manager IOManagerDefinition</Link></dt>
    <dd>

        <div className='lineblock'> </div>

    Persistent IO manager using GCS for storage.

    Serializes objects via pickling. Suitable for objects storage for distributed executors, so long
    as each execution node has network connectivity and credentials for GCS and the backing bucket.

    Assigns each op output to a unique filepath containing run ID, step key, and output name.
    Assigns each asset to a single filesystem path, at `\<base_dir>/\<asset_key>`. If the asset key
    has multiple components, the final component is used as the name of the file, and the preceding
    components as parent directories under the base_dir.

    Subsequent materializations of an asset will overwrite previous materializations of that asset.
    With a base directory of `/my/base/path`, an asset with key
    `AssetKey(["one", "two", "three"])` would be stored in a file called `three` in a directory
    with path `/my/base/path/one/two/`.

    Example usage:

    1. Attach this IO manager to a set of assets.
        ```python
        from dagster import Definitions, asset
        from dagster_gcp.gcs import gcs_pickle_io_manager, gcs_resource

        @asset
        def asset1():
            # create df ...
            return df

        @asset
        def asset2(asset1):
            return asset1[:5]

        defs = Definitions(
            assets=[asset1, asset2],
            resources=\{
                    "io_manager": gcs_pickle_io_manager.configured(
                        \{"gcs_bucket": "my-cool-bucket", "gcs_prefix": "my-cool-prefix"}
                    ),
                    "gcs": gcs_resource.configured(\{"project": "my-cool-project"}),
                },
        )
        ```
    2. Attach this IO manager to your job to make it available to your ops.
        ```python
        from dagster import job
        from dagster_gcp.gcs import gcs_pickle_io_manager, gcs_resource

        @job(
            resource_defs=\{
                "io_manager": gcs_pickle_io_manager.configured(
                    \{"gcs_bucket": "my-cool-bucket", "gcs_prefix": "my-cool-prefix"}
                ),
                "gcs": gcs_resource.configured(\{"project": "my-cool-project"}),
            },
        )
        def my_job():
            ...
        ```

    </dd>

</dl>
<dl>
    <dt><Link id='dagster_gcp.gcs_file_manager'>dagster_gcp.gcs_file_manager ResourceDefinition</Link></dt>
    <dd>

    FileManager that provides abstract access to GCS.

    Implements the [`FileManager`](../internals.mdx#dagster._core.storage.file_manager.FileManager) API.


    </dd>

</dl>
<dl>
    <dt><Link id='dagster_gcp.dataproc_resource'>dagster_gcp.dataproc_resource ResourceDefinition</Link></dt>
    <dd>

        <div className='lineblock'> </div>


    </dd>

</dl>
</div></div>
