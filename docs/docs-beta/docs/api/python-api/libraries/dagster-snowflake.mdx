---
title: 'snowflake (dagster-snowflake)'
title_meta: 'snowflake (dagster-snowflake) API Documentation - Build Better Data Pipelines | Python Reference Documentation for Dagster'
description: 'snowflake (dagster-snowflake) Dagster API | Comprehensive Python API documentation for Dagster, the data orchestration platform. Learn how to build, test, and maintain data pipelines with our detailed guides and examples.'
last_update:
  date: '2025-01-23'
---

<div class="section" id="snowflake-dagster-snowflake">


# Snowflake (dagster-snowflake)

This library provides an integration with the [Snowflake](https://www.snowflake.com/) data
warehouse.

To use this library, you should first ensure that you have an appropriate [Snowflake user](https://docs.snowflake.net/manuals/user-guide/admin-user-management.html) configured to access
your data warehouse.

Related Guides:

  - [Using Dagster with Snowflake](https://docs.dagster.io/integrations/snowflake)
  - [Snowflake I/O manager reference](https://docs.dagster.io/integrations/snowflake/reference)
  - [Transitioning data pipelines from development to production](https://docs.dagster.io/guides/dagster/transitioning-data-pipelines-from-development-to-production)
  - [Testing against production with Dagster+ Branch Deployments](https://docs.dagster.io/guides/dagster/branch_deployments)


<div class="section" id="i-o-manager">


## I/O Manager

<dl>
    <dt><Link id='dagster_snowflake.SnowflakeIOManager'>dagster_snowflake.SnowflakeIOManager IOManagerDefinition</Link></dt>
    <dd>

        <div className='lineblock'> </div>

    Base class for an IO manager definition that reads inputs from and writes outputs to Snowflake.

    Examples:

        ```python
        from dagster_snowflake import SnowflakeIOManager
        from dagster_snowflake_pandas import SnowflakePandasTypeHandler
        from dagster_snowflake_pyspark import SnowflakePySparkTypeHandler
        from dagster import Definitions, EnvVar

        class MySnowflakeIOManager(SnowflakeIOManager):
            @staticmethod
            def type_handlers() -> Sequence[DbTypeHandler]:
                return [SnowflakePandasTypeHandler(), SnowflakePySparkTypeHandler()]

        @asset(
            key_prefix=["my_schema"]  # will be used as the schema in snowflake
        )
        def my_table() -> pd.DataFrame:  # the name of the asset will be the table name
            ...

        defs = Definitions(
            assets=[my_table],
            resources=\{
                "io_manager": MySnowflakeIOManager(database="my_database", account=EnvVar("SNOWFLAKE_ACCOUNT"), ...)
            }
        )
        ```
    You can set a default schema to store the assets using the `schema` configuration value of the Snowflake I/O
    Manager. This schema will be used if no other schema is specified directly on an asset or op.

        ```python
        defs = Definitions(
            assets=[my_table]
            resources=\{
                "io_manager" MySnowflakeIOManager(database="my_database", schema="my_schema", ...)
            }
        )
        ```
    On individual assets, you an also specify the schema where they should be stored using metadata or
    by adding a `key_prefix` to the asset key. If both `key_prefix` and metadata are defined, the metadata will
    take precedence.

        ```python
        @asset(
            key_prefix=["my_schema"]  # will be used as the schema in snowflake
        )
        def my_table() -> pd.DataFrame:
            ...

        @asset(
            metadata=\{"schema": "my_schema"}  # will be used as the schema in snowflake
        )
        def my_other_table() -> pd.DataFrame:
            ...
        ```
    For ops, the schema can be specified by including a “schema” entry in output metadata.

        ```python
        @op(
            out=\{"my_table": Out(metadata=\{"schema": "my_schema"})}
        )
        def make_my_table() -> pd.DataFrame:
            ...
        ```
    If none of these is provided, the schema will default to “public”.

    To only use specific columns of a table as input to a downstream op or asset, add the metadata `columns` to the
    In or AssetIn.

        ```python
        @asset(
            ins=\{"my_table": AssetIn("my_table", metadata=\{"columns": ["a"]})}
        )
        def my_table_a(my_table: pd.DataFrame) -> pd.DataFrame:
            # my_table will just contain the data from column "a"
            ...
        ```

    </dd>

</dl>
</div>


<div class="section" id="resource">


## Resource

<dl>
    <dt><Link id='dagster_snowflake.SnowflakeResource'>dagster_snowflake.SnowflakeResource ResourceDefinition</Link></dt>
    <dd>

        <div className='lineblock'> </div>

    A resource for connecting to the Snowflake data warehouse.

    If connector configuration is not set, SnowflakeResource.get_connection() will return a
    [snowflake.connector.Connection](https://docs.snowflake.com/en/developer-guide/python-connector/python-connector-api#object-connection)
    object. If connector=”sqlalchemy” configuration is set, then SnowflakeResource.get_connection() will
    return a [SQLAlchemy Connection](https://docs.sqlalchemy.org/en/20/core/connections.html#sqlalchemy.engine.Connection)
    or a [SQLAlchemy raw connection](https://docs.sqlalchemy.org/en/20/core/connections.html#sqlalchemy.engine.Engine.raw_connection).

    A simple example of loading data into Snowflake and subsequently querying that data is shown below:

    Examples:

        ```python
        from dagster import job, op
        from dagster_snowflake import SnowflakeResource

        @op
        def get_one(snowflake_resource: SnowflakeResource):
            with snowflake_resource.get_connection() as conn:
                # conn is a snowflake.connector.Connection object
                conn.cursor().execute("SELECT 1")

        @job
        def my_snowflake_job():
            get_one()

        my_snowflake_job.execute_in_process(
            resources=\{
                'snowflake_resource': SnowflakeResource(
                    account=EnvVar("SNOWFLAKE_ACCOUNT"),
                    user=EnvVar("SNOWFLAKE_USER"),
                    password=EnvVar("SNOWFLAKE_PASSWORD")
                    database="MY_DATABASE",
                    schema="MY_SCHEMA",
                    warehouse="MY_WAREHOUSE"
                )
            }
        )
        ```

    </dd>

</dl>
<dl>
    <dt><Link id='dagster_snowflake.SnowflakeConnection'>class dagster_snowflake.SnowflakeConnection</Link></dt>
    <dd>

    A connection to Snowflake that can execute queries. In general this class should not be
    directly instantiated, but rather used as a resource in an op or asset via the
    [`snowflake_resource()`](#dagster_snowflake.snowflake_resource).

    Note that the SnowflakeConnection is only used by the snowflake_resource. The Pythonic SnowflakeResource does
    not use this SnowflakeConnection class.

    <dl>
        <dt><Link id='dagster_snowflake.SnowflakeConnection.execute_queries'>execute_queries</Link></dt>
        <dd>

        Execute multiple queries in Snowflake.

        Parameters: 
          - <strong>sql_queries</strong> (<em>str</em>) – List of queries to be executed in series
          - <strong>parameters</strong> (<em>Optional</em><em>[</em><em>Union</em><em>[</em><em>Sequence</em><em>[</em><em>Any</em><em>]</em><em>, </em><em>Mapping</em><em>[</em><em>Any</em><em>, </em><em>Any</em><em>]</em><em>]</em><em>]</em>) – Parameters to be passed to every query. See the
          - <strong>fetch_results</strong> (<em>bool</em>) – If True, will return the results of the queries as a list. Defaults to False. If True
          - <strong>use_pandas_result</strong> (<em>bool</em>) – If True, will return the results of the queries as a list of a Pandas DataFrames.


        Returns: The results of the queries as a list if fetch_results or use_pandas_result is True,
        otherwise returns None
        Examples:

            ```python
            @op
            def create_fresh_database(snowflake: SnowflakeResource):
                queries = ["DROP DATABASE IF EXISTS MY_DATABASE", "CREATE DATABASE MY_DATABASE"]
                snowflake.execute_queries(
                    sql_queries=queries
                )
            ```

        </dd>

    </dl>
    <dl>
        <dt><Link id='dagster_snowflake.SnowflakeConnection.execute_query'>execute_query</Link></dt>
        <dd>

        Execute a query in Snowflake.

        Parameters: 
          - <strong>sql</strong> (<em>str</em>) – the query to be executed
          - <strong>parameters</strong> (<em>Optional</em><em>[</em><em>Union</em><em>[</em><em>Sequence</em><em>[</em><em>Any</em><em>]</em><em>, </em><em>Mapping</em><em>[</em><em>Any</em><em>, </em><em>Any</em><em>]</em><em>]</em><em>]</em>) – Parameters to be passed to the query. See the
          - <strong>fetch_results</strong> (<em>bool</em>) – If True, will return the result of the query. Defaults to False. If True
          - <strong>use_pandas_result</strong> (<em>bool</em>) – If True, will return the result of the query as a Pandas DataFrame.


        Returns: The result of the query if fetch_results or use_pandas_result is True, otherwise returns None
        Examples:

            ```python
            @op
            def drop_database(snowflake: SnowflakeResource):
                snowflake.execute_query(
                    "DROP DATABASE IF EXISTS MY_DATABASE"
                )
            ```

        </dd>

    </dl>
    <dl>
        <dt><Link id='dagster_snowflake.SnowflakeConnection.get_connection'>get_connection</Link></dt>
        <dd>

        Gets a connection to Snowflake as a context manager.

        If using the execute_query, execute_queries, or load_table_from_local_parquet methods,
        you do not need to create a connection using this context manager.

        Parameters: <strong>raw_conn</strong> (<em>bool</em>) – If using the sqlalchemy connector, you can set raw_conn to True to create a raw
        connection. Defaults to True.
        Examples:

            ```python
            @op(
                required_resource_keys=\{"snowflake"}
            )
            def get_query_status(query_id):
                with context.resources.snowflake.get_connection() as conn:
                    # conn is a Snowflake Connection object or a SQLAlchemy Connection if
                    # sqlalchemy is specified as the connector in the Snowflake Resource config

                    return conn.get_query_status(query_id)
            ```

        </dd>

    </dl>
    <dl>
        <dt><Link id='dagster_snowflake.SnowflakeConnection.load_table_from_local_parquet'>load_table_from_local_parquet</Link></dt>
        <dd>

        Stores the content of a parquet file to a Snowflake table.

        Parameters: 
          - <strong>src</strong> (<em>str</em>) – the name of the file to store in Snowflake
          - <strong>table</strong> (<em>str</em>) – the name of the table to store the data. If the table does not exist, it will


        Examples:

            ```python
            import pandas as pd
            import pyarrow as pa
            import pyarrow.parquet as pq

            @op
            def write_parquet_file(snowflake: SnowflakeResource):
                df = pd.DataFrame(\{"one": [1, 2, 3], "ten": [11, 12, 13]})
                table = pa.Table.from_pandas(df)
                pq.write_table(table, "example.parquet')
                snowflake.load_table_from_local_parquet(
                    src="example.parquet",
                    table="MY_TABLE"
                )
            ```

        </dd>

    </dl>

    </dd>

</dl>
</div>


<div class="section" id="data-freshness">


## Data Freshness

<dl>
    <dt><Link id='dagster_snowflake.fetch_last_updated_timestamps'>dagster_snowflake.fetch_last_updated_timestamps</Link></dt>
    <dd>

    Fetch the last updated times of a list of tables in Snowflake.

    If the underlying query to fetch the last updated time returns no results, a ValueError will be raised.

    Parameters: 
      - <strong>snowflake_connection</strong> (<em>Union</em><em>[</em><em>SqlDbConnection</em><em>, </em>[*SnowflakeConnection*](#dagster_snowflake.SnowflakeConnection)<em>]</em>) – A connection to Snowflake.
      - <strong>schema</strong> (<em>str</em>) – The schema of the tables to fetch the last updated time for.
      - <strong>tables</strong> (<em>Sequence</em><em>[</em><em>str</em><em>]</em>) – A list of table names to fetch the last updated time for.
      - <strong>database</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) – The database of the table. Only required if the connection
      - <strong>ignore_missing_tables</strong> (<em>Optional</em><em>[</em><em>bool</em><em>]</em>) – If True, tables not found in Snowflake


    Returns: A dictionary of table names to their last updated time in UTC.Return type: Mapping[str, datetime]

    </dd>

</dl>
</div>


<div class="section" id="ops">


## Ops

<dl>
    <dt><Link id='dagster_snowflake.snowflake_op_for_query'>dagster_snowflake.snowflake_op_for_query</Link></dt>
    <dd>

    This function is an op factory that constructs an op to execute a snowflake query.

    Note that you can only use <cite>snowflake_op_for_query</cite> if you know the query you’d like to
    execute at graph construction time. If you’d like to execute queries dynamically during
    job execution, you should manually execute those queries in your custom op using the
    snowflake resource.

    Parameters: 
      - <strong>sql</strong> (<em>str</em>) – The sql query that will execute against the provided snowflake resource.
      - <strong>parameters</strong> (<em>dict</em>) – The parameters for the sql query.


    Returns: Returns the constructed op definition.Return type: [OpDefinition](../ops.mdx#dagster.OpDefinition)

    </dd>

</dl>
</div>


<div class="section" id="legacy">

## Legacy

<dl>
    <dt><Link id='dagster_snowflake.build_snowflake_io_manager'>dagster_snowflake.build_snowflake_io_manager IOManagerDefinition</Link></dt>
    <dd>

        <div className='lineblock'> </div>

    Builds an IO manager definition that reads inputs from and writes outputs to Snowflake.

    Parameters: 
      - <strong>type_handlers</strong> (<em>Sequence</em><em>[</em><em>DbTypeHandler</em><em>]</em>) – Each handler defines how to translate between
      - <strong>default_load_type</strong> (<em>Type</em>) – When an input has no type annotation, load it as this type.


    Returns: IOManagerDefinition
    Examples:

        ```python
        from dagster_snowflake import build_snowflake_io_manager
        from dagster_snowflake_pandas import SnowflakePandasTypeHandler
        from dagster_snowflake_pyspark import SnowflakePySparkTypeHandler
        from dagster import Definitions

        @asset(
            key_prefix=["my_prefix"]
            metadata=\{"schema": "my_schema"} # will be used as the schema in snowflake
        )
        def my_table() -> pd.DataFrame:  # the name of the asset will be the table name
            ...

        @asset(
            key_prefix=["my_schema"]  # will be used as the schema in snowflake
        )
        def my_second_table() -> pd.DataFrame:  # the name of the asset will be the table name
            ...

        snowflake_io_manager = build_snowflake_io_manager([SnowflakePandasTypeHandler(), SnowflakePySparkTypeHandler()])

        defs = Definitions(
            assets=[my_table, my_second_table],
            resources=\{
                "io_manager": snowflake_io_manager.configured(\{
                    "database": "my_database",
                    "account" : \{"env": "SNOWFLAKE_ACCOUNT"}
                    ...
                })
            }
        )
        ```
    You can set a default schema to store the assets using the `schema` configuration value of the Snowflake I/O
    Manager. This schema will be used if no other schema is specified directly on an asset or op.

        ```python
        defs = Definitions(
            assets=[my_table]
            resources=\{"io_manager" snowflake_io_manager.configured(
                \{"database": "my_database", "schema": "my_schema", ...} # will be used as the schema
            )}
        )
        ```
    On individual assets, you an also specify the schema where they should be stored using metadata or
    by adding a `key_prefix` to the asset key. If both `key_prefix` and metadata are defined, the metadata will
    take precedence.

        ```python
        @asset(
            key_prefix=["my_schema"]  # will be used as the schema in snowflake
        )
        def my_table() -> pd.DataFrame:
            ...

        @asset(
            metadata=\{"schema": "my_schema"}  # will be used as the schema in snowflake
        )
        def my_other_table() -> pd.DataFrame:
            ...
        ```
    For ops, the schema can be specified by including a “schema” entry in output metadata.

        ```python
        @op(
            out=\{"my_table": Out(metadata=\{"schema": "my_schema"})}
        )
        def make_my_table() -> pd.DataFrame:
            ...
        ```
    If none of these is provided, the schema will default to “public”.

    To only use specific columns of a table as input to a downstream op or asset, add the metadata `columns` to the
    In or AssetIn.

        ```python
        @asset(
            ins=\{"my_table": AssetIn("my_table", metadata=\{"columns": ["a"]})}
        )
        def my_table_a(my_table: pd.DataFrame) -> pd.DataFrame:
            # my_table will just contain the data from column "a"
            ...
        ```

    </dd>

</dl>
<dl>
    <dt><Link id='dagster_snowflake.snowflake_resource'>dagster_snowflake.snowflake_resource ResourceDefinition</Link></dt>
    <dd>

        <div className='lineblock'> </div>

    A resource for connecting to the Snowflake data warehouse. The returned resource object is an
    instance of [`SnowflakeConnection`](#dagster_snowflake.SnowflakeConnection).

    A simple example of loading data into Snowflake and subsequently querying that data is shown below:

    Examples:

        ```python
        from dagster import job, op
        from dagster_snowflake import snowflake_resource

        @op(required_resource_keys=\{'snowflake'})
        def get_one(context):
            context.resources.snowflake.execute_query('SELECT 1')

        @job(resource_defs=\{'snowflake': snowflake_resource})
        def my_snowflake_job():
            get_one()

        my_snowflake_job.execute_in_process(
            run_config=\{
                'resources': \{
                    'snowflake': \{
                        'config': \{
                            'account': \{'env': 'SNOWFLAKE_ACCOUNT'},
                            'user': \{'env': 'SNOWFLAKE_USER'},
                            'password': \{'env': 'SNOWFLAKE_PASSWORD'},
                            'database': \{'env': 'SNOWFLAKE_DATABASE'},
                            'schema': \{'env': 'SNOWFLAKE_SCHEMA'},
                            'warehouse': \{'env': 'SNOWFLAKE_WAREHOUSE'},
                        }
                    }
                }
            }
        )
        ```

    </dd>

</dl>
</div></div>
