---
title: 'polars (dagster-polars)'
title_meta: 'polars (dagster-polars) API Documentation - Build Better Data Pipelines | Python Reference Documentation for Dagster'
description: 'polars (dagster-polars) Dagster API | Comprehensive Python API documentation for Dagster, the data orchestration platform. Learn how to build, test, and maintain data pipelines with our detailed guides and examples.'
last_update:
  date: '2025-01-23'
---

<div class="section" id="polars-dagster-polars">


# Polars (dagster-polars)

This library provides Dagster integration with [Polars](https://pola.rs).
It allows using Polars eager or lazy DataFrames as inputs and outputs with Dagster’s <cite>@asset</cite> and <cite>@op</cite>.
Type annotations are used to control whether to load an eager or lazy DataFrame. Lazy DataFrames can be sinked as output.
Multiple serialization formats (Parquet, Delta Lake, BigQuery) and filesystems (local, S3, GCS, …) are supported.

Comprehensive list of <cite>dagster-polars</cite> behavior for supported type annotations can be found in [Type Annotations](#types)`Type Annotations` section.

</div>


<div class="section" id="installation">


# Installation

    ```default
    pip install dagster-polars
    ```
Some IOManagers (like [`PolarsDeltaIOManager`](#dagster_polars.PolarsDeltaIOManager)) may require additional dependencies, which are provided with extras like <cite>dagster-polars[delta]</cite>.
Please check the documentation for each IOManager for more details.

</div>


<div class="section" id="quickstart">


# Quickstart

Common filesystem-based IOManagers features highlights, using [`PolarsParquetIOManager`](#dagster_polars.PolarsParquetIOManager) as an example (see [`BasePolarsUPathIOManager`](#dagster_polars.BasePolarsUPathIOManager) for the full list of features provided by <cite>dagster-polars</cite>):

Type annotations are not required. By default an eager <cite>pl.DataFrame</cite> will be loaded.

    ```python
    from dagster import asset
    import polars as pl

    @asset(io_manager_key="polars_parquet_io_manager")
    def upstream():
        return DataFrame(\{"foo": [1, 2, 3]})

    @asset(io_manager_key="polars_parquet_io_manager")
    def downstream(upstream) -> pl.LazyFrame:
        assert isinstance(upstream, pl.DataFrame)
        return upstream.lazy()  # LazyFrame will be sinked
    ```
Lazy <cite>pl.LazyFrame</cite> can be scanned by annotating the input with <cite>pl.LazyFrame</cite>, and returning a <cite>pl.LazyFrame</cite> will sink it:

    ```python
    @asset(io_manager_key="polars_parquet_io_manager")
    def downstream(upstream: pl.LazyFrame) -> pl.LazyFrame:
        assert isinstance(upstream, pl.LazyFrame)
        return upstream
    ```
The same logic applies to partitioned assets:

    ```python
    @asset
    def downstream(partitioned_upstream: Dict[str, pl.LazyFrame]):
        assert isinstance(partitioned_upstream, dict)
        assert isinstance(partitioned_upstream["my_partition"], pl.LazyFrame)
    ```
<cite>Optional</cite> inputs and outputs are supported:

    ```python
    @asset
    def upstream() -> Optional[pl.DataFrame]:
        if has_data:
            return DataFrame(\{"foo": [1, 2, 3]})  # type check will pass
        else:
            return None  # type check will pass and `dagster_polars` will skip writing the output completely

    @asset
    def downstream(upstream: Optional[pl.LazyFrame]):  # upstream will be None if it doesn't exist in storage
        ...
    ```
By default all the IOManagers store separate partitions as physically separated locations, such as:

  - <cite>/my/asset/key/partition_0.extension</cite>
  - <cite>/my/asset/key/partition_1.extension</cite>


This mode is useful for e.g. snapshotting.

Some IOManagers (like [`PolarsDeltaIOManager`](#dagster_polars.PolarsDeltaIOManager)) support reading and writing partitions in storage-native format in the same location.
This mode can be typically enabled by setting <cite>“partition_by”</cite> metadata value. For example, [`PolarsDeltaIOManager`](#dagster_polars.PolarsDeltaIOManager) would store different partitions in the same <cite>/my/asset/key.delta</cite> directory, which will be properly partitioned.

This mode should be preferred for true partitioning.

</div>


<div class="section" id="type-annotations">
<Link id="types"></Link>


# Type Annotations

Type aliases like <cite>DataFrameWithPartitions</cite> are provided by `dagster_polars.types` for convenience.

# Supported type annotations and <cite>dagster-polars</cite> behavior

| Type annotation | Type Alias | Behavior |
| :------------------------ | :--------- | :-------------------------------------------------------------------------- |
| <cite>DataFrame</cite> |  | read/write a<cite>DataFrame</cite> |
| <cite>LazyFrame</cite> |  | read/sink a<cite>LazyFrame</cite> |
| <cite>Optional[DataFrame]</cite> |  | read/write a<cite>DataFrame</cite>. Do nothing if no data is found in storage or the output is<cite>None</cite> |
| <cite>Optional[LazyFrame]</cite> |  | read a<cite>LazyFrame</cite>. Do nothing if no data is found in storage |
| <cite>Dict[str, DataFrame]</cite> | <cite>DataFrameWithPartitions</cite> | read multiple<cite>DataFrame`s as `Dict[str, DataFrame]</cite>. Raises an error for missing partitions, unless<cite>“allow_missing_partitions”</cite>input metadata is set to<cite>True</cite> |
| <cite>Dict[str, LazyFrame]</cite> | <cite>LazyFramePartitions</cite> | read multiple<cite>LazyFrame`s as `Dict[str, LazyFrame]</cite>. Raises an error for missing partitions, unless<cite>“allow_missing_partitions”</cite>input metadata is set to<cite>True</cite> |


Generic builtins (like <cite>tuple[…]</cite> instead of <cite>Tuple[…]</cite>) are supported for Python >= 3.9.

</div>


<div class="section" id="api-documentation">

# API Documentation

<dl>
    <dt><Link id='dagster_polars.BasePolarsUPathIOManager'>dagster_polars.BasePolarsUPathIOManager IOManagerDefinition</Link></dt>
    <dd>

        <div className='lineblock'> </div>

    Base class for <cite>dagster-polars</cite> IOManagers.

    Doesn’t define a specific storage format.

    To implement a specific storage format (parquet, csv, etc), inherit from this class and implement the <cite>write_df_to_path</cite>, <cite>sink_df_to_path</cite> and <cite>scan_df_from_path</cite> methods.

    Features:
          - All the features of [`UPathIOManager`](../io-managers.mdx#dagster.UPathIOManager) - works with local and remote filesystems (like S3), supports loading multiple partitions with respect to [`PartitionMapping`](../partitions.mdx#dagster.PartitionMapping), and more
          - loads the correct type - <cite>polars.DataFrame</cite>, <cite>polars.LazyFrame</cite>, or other types defined in `dagster_polars.types` - based on the input type annotation (or <cite>dagster.DagsterType</cite>’s <cite>typing_type</cite>)
          - can sink lazy <cite>pl.LazyFrame</cite> DataFrames
          - handles <cite>Nones</cite> with <cite>Optional</cite> types by skipping loading missing inputs or saving <cite>None</cite> outputs
          - logs various metadata about the DataFrame - size, schema, sample, stats, …
          - the <cite>“columns”</cite> input metadata value can be used to select a subset of columns to load



    </dd>

</dl>
<dl>
    <dt><Link id='dagster_polars.PolarsParquetIOManager'>dagster_polars.PolarsParquetIOManager IOManagerDefinition</Link></dt>
    <dd>

        <div className='lineblock'> </div>

        :::warning[experimental]
        This API may break in future versions, even between dot releases.


        :::

    Implements reading and writing Polars DataFrames in Apache Parquet format.

    Features:
          - All features provided by [`BasePolarsUPathIOManager`](#dagster_polars.BasePolarsUPathIOManager).
          - All read/write options can be set via corresponding metadata or config parameters (metadata takes precedence).
          - Supports reading partitioned Parquet datasets (for example, often produced by Spark).
          - Supports reading/writing custom metadata in the Parquet file’s schema as json-serialized bytes at <cite>“dagster_polars_metadata”</cite> key.


    Examples:

        ```python
        from dagster import asset
        from dagster_polars import PolarsParquetIOManager
        import polars as pl

        @asset(
            io_manager_key="polars_parquet_io_manager",
            key_prefix=["my_dataset"]
        )
        def my_asset() -> pl.DataFrame:  # data will be stored at \<base_dir>/my_dataset/my_asset.parquet
            ...

        defs = Definitions(
            assets=[my_table],
            resources=\{
                "polars_parquet_io_manager": PolarsParquetIOManager(base_dir="s3://my-bucket/my-dir")
            }
        )
        ```
    Reading partitioned Parquet datasets:

        ```python
        from dagster import SourceAsset

        my_asset = SourceAsset(
            key=["path", "to", "dataset"],
            io_manager_key="polars_parquet_io_manager",
            metadata=\{
                "partition_by": ["year", "month", "day"]
            }
        )
        ```

    </dd>

</dl>
<dl>
    <dt><Link id='dagster_polars.PolarsDeltaIOManager'>dagster_polars.PolarsDeltaIOManager IOManagerDefinition</Link></dt>
    <dd>

        <div className='lineblock'> </div>

        :::warning[experimental]
        This API may break in future versions, even between dot releases.


        :::

    Implements writing and reading DeltaLake tables.

    Features:
          - All features provided by [`BasePolarsUPathIOManager`](#dagster_polars.BasePolarsUPathIOManager).
          - All read/write options can be set via corresponding metadata or config parameters (metadata takes precedence).
          - Supports native DeltaLake partitioning by storing different asset partitions in the same DeltaLake table.


    Install <cite>dagster-polars[delta]</cite> to use this IOManager.

    Examples:

        ```python
        from dagster import asset
        from dagster_polars import PolarsDeltaIOManager
        import polars as pl

        @asset(
            io_manager_key="polars_delta_io_manager",
            key_prefix=["my_dataset"]
        )
        def my_asset() -> pl.DataFrame:  # data will be stored at \<base_dir>/my_dataset/my_asset.delta
            ...

        defs = Definitions(
            assets=[my_table],
            resources=\{
                "polars_parquet_io_manager": PolarsDeltaIOManager(base_dir="s3://my-bucket/my-dir")
            }
        )
        ```
    Appending to a DeltaLake table:

        ```python
        @asset(
            io_manager_key="polars_delta_io_manager",
            metadata=\{
                "mode": "append"
            },
        )
        def my_table() -> pl.DataFrame:
            ...
        ```
    Using native DeltaLake partitioning by storing different asset partitions in the same DeltaLake table:

        ```python
        from dagster import AssetExecutionContext, DailyPartitionedDefinition
        from dagster_polars import LazyFramePartitions

        @asset(
            io_manager_key="polars_delta_io_manager",
            metadata=\{
                "partition_by": "partition_col"
            },
            partitions_def=StaticPartitionsDefinition(["a, "b", "c"])
        )
        def upstream(context: AssetExecutionContext) -> pl.DataFrame:
            df = ...

            # column with the partition_key must match `partition_by` metadata value
            return df.with_columns(pl.lit(context.partition_key).alias("partition_col"))

        @asset
        def downstream(upstream: pl.LazyFrame) -> pl.DataFrame:
            ...
        ```
    When using <cite>MuiltiPartitionsDefinition</cite>, <cite>partition_by</cite> metadata value should be a dictionary mapping dimensions to column names.

        ```python
        from dagster import AssetExecutionContext, DailyPartitionedDefinition, MultiPartitionsDefinition, StaticPartitionsDefinition
        from dagster_polars import LazyFramePartitions

        @asset(
            io_manager_key="polars_delta_io_manager",
            metadata=\{
                "partition_by": \{"time": "date", "clients": "client"}  # dimension -> column mapping
            },
            partitions_def=MultiPartitionsDefinition(
                \{
                    "date": DailyPartitionedDefinition(...),
                    "clients": StaticPartitionsDefinition(...)
                }
            )
        )
        def upstream(context: AssetExecutionContext) -> pl.DataFrame:
            df = ...

            partition_keys_by_dimension = context.partition_key.keys_by_dimension

            return df.with_columns(
                pl.lit(partition_keys_by_dimension["time"]).alias("date"),  # time dimension matches date column
                pl.lit(partition_keys_by_dimension["clients"]).alias("client")  # clients dimension matches client column
            )


        @asset
        def downstream(upstream: pl.LazyFrame) -> pl.DataFrame:
            ...
        ```

    </dd>

</dl>
<dl>
    <dt><Link id='dagster_polars.PolarsBigQueryIOManager'>dagster_polars.PolarsBigQueryIOManager IOManagerDefinition</Link></dt>
    <dd>

        <div className='lineblock'> </div>

    Implements reading and writing Polars DataFrames from/to [BigQuery](https://cloud.google.com/bigquery)).

    Features:
    - All `DBIOManager` features
    - Supports writing partitioned tables (<cite>“partition_expr”</cite> input metadata key must be specified).

    Returns: IOManagerDefinition
    Examples:

        ```python
        from dagster import Definitions, EnvVar
        from dagster_polars import PolarsBigQueryIOManager

        @asset(
            key_prefix=["my_dataset"]  # will be used as the dataset in BigQuery
        )
        def my_table() -> pl.DataFrame:  # the name of the asset will be the table name
            ...

        defs = Definitions(
            assets=[my_table],
            resources=\{
                "io_manager": PolarsBigQueryIOManager(project=EnvVar("GCP_PROJECT"))
            }
        )
        ```
    You can tell Dagster in which dataset to create tables by setting the “dataset” configuration value.
    If you do not provide a dataset as configuration to the I/O manager, Dagster will determine a dataset based
    on the assets and ops using the I/O Manager. For assets, the dataset will be determined from the asset key,
    as shown in the above example. The final prefix before the asset name will be used as the dataset. For example,
    if the asset “my_table” had the key prefix [“gcp”, “bigquery”, “my_dataset”], the dataset “my_dataset” will be
    used. For ops, the dataset can be specified by including a “schema” entry in output metadata. If “schema” is
    not provided via config or on the asset/op, “public” will be used for the dataset.

        ```python
        @op(
            out=\{"my_table": Out(metadata=\{"schema": "my_dataset"})}
        )
        def make_my_table() -> pl.DataFrame:
            # the returned value will be stored at my_dataset.my_table
            ...
        ```
    To only use specific columns of a table as input to a downstream op or asset, add the metadata “columns” to the
    In or AssetIn.

        ```python
        @asset(
            ins=\{"my_table": AssetIn("my_table", metadata=\{"columns": ["a"]})}
        )
        def my_table_a(my_table: pl.DataFrame) -> pd.DataFrame:
            # my_table will just contain the data from column "a"
            ...
        ```
    If you cannot upload a file to your Dagster deployment, or otherwise cannot
    [authenticate with GCP](https://cloud.google.com/docs/authentication/provide-credentials-adc)
    via a standard method, you can provide a service account key as the “gcp_credentials” configuration.
    Dagster will store this key in a temporary file and set GOOGLE_APPLICATION_CREDENTIALS to point to the file.
    After the run completes, the file will be deleted, and GOOGLE_APPLICATION_CREDENTIALS will be
    unset. The key must be base64 encoded to avoid issues with newlines in the keys. You can retrieve
    the base64 encoded key with this shell command: cat $GOOGLE_APPLICATION_CREDENTIALS | base64

    The “write_disposition” metadata key can be used to set the <cite>write_disposition</cite> parameter
    of <cite>bigquery.JobConfig</cite>. For example, set it to <cite>“WRITE_APPEND”</cite> to append to an existing table intead of
    overwriting it.

    Install <cite>dagster-polars[gcp]</cite> to use this IOManager.


    </dd>

</dl>
</div>
