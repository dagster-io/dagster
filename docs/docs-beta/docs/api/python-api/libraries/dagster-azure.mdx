---
title: 'azure (dagster-azure)'
title_meta: 'azure (dagster-azure) API Documentation - Build Better Data Pipelines | Python Reference Documentation for Dagster'
description: 'azure (dagster-azure) Dagster API | Comprehensive Python API documentation for Dagster, the data orchestration platform. Learn how to build, test, and maintain data pipelines with our detailed guides and examples.'
last_update:
  date: '2025-02-03'
---

<div class="section" id="azure-dagster-azure">


# Azure (dagster-azure)

Utilities for using Azure Storage Accounts with Dagster. This is mostly aimed at Azure Data Lake
Storage Gen 2 (ADLS2) but also contains some utilities for Azure Blob Storage.

<div class="section" id="resources">


## Resources

<dl>
    <dt><Link id='dagster_azure.adls2.ADLS2Resource'>dagster_azure.adls2.ADLS2Resource ResourceDefinition</Link></dt>
    <dd>

        <div className='lineblock'> </div>

    Resource containing clients to access Azure Data Lake Storage Gen2.

    Contains a client for both the Data Lake and Blob APIs, to work around the limitations
    of each.

    Example usage:

    Attach this resource to your Definitions to be used by assets and jobs.

        ```python
        from dagster import Definitions, asset, job, op
        from dagster_azure.adls2 import ADLS2Resource, ADLS2SASToken

        @asset
        def asset1(adls2: ADLS2Resource):
            adls2.adls2_client.list_file_systems()
            ...

        @op
        def my_op(adls2: ADLS2Resource):
            adls2.adls2_client.list_file_systems()
            ...

        @job
        def my_job():
            my_op()

        defs = Definitions(
            assets=[asset1],
            jobs=[my_job],
            resources={
                "adls2": ADLS2Resource(
                    storage_account="my-storage-account",
                    credential=ADLS2SASToken(token="my-sas-token"),
                )
            },
        )
        ```
    Attach this resource to your job to make it available to your ops.

        ```python
        from dagster import job, op
        from dagster_azure.adls2 import ADLS2Resource, ADLS2SASToken

        @op
        def my_op(adls2: ADLS2Resource):
            adls2.adls2_client.list_file_systems()
            ...

        @job(
            resource_defs={
                "adls2": ADLS2Resource(
                    storage_account="my-storage-account",
                    credential=ADLS2SASToken(token="my-sas-token"),
                )
            },
        )
        def my_job():
            my_op()
        ```

    </dd>

</dl>
<dl>
    <dt><Link id='dagster_azure.adls2.FakeADLS2Resource'>dagster_azure.adls2.FakeADLS2Resource ResourceDefinition</Link></dt>
    <dd>

        <div className='lineblock'> </div>

    Stateful mock of an ADLS2Resource for testing.

    Wraps a `mock.MagicMock`. Containers are implemented using an in-memory dict.


    </dd>

</dl>
<dl>
    <dt><Link id='dagster_azure.blob.AzureBlobComputeLogManager'>class dagster_azure.blob.AzureBlobComputeLogManager</Link></dt>
    <dd>

    Logs op compute function stdout and stderr to Azure Blob Storage.

    This is also compatible with Azure Data Lake Storage.

    Users should not instantiate this class directly. Instead, use a YAML block in `dagster.yaml`. Examples provided below
    will show how to configure with various credentialing schemes.

    Parameters: 
      - <strong>storage_account</strong> (<em>str</em>) – The storage account name to which to log.
      - <strong>container</strong> (<em>str</em>) – The container (or ADLS2 filesystem) to which to log.
      - <strong>secret_credential</strong> (<em>Optional</em><em>[</em><em>dict</em><em>]</em>) – Secret credential for the storage account. This should be
      - <strong>access_key_or_sas_token</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) – Access key or SAS token for the storage account.
      - <strong>default_azure_credential</strong> (<em>Optional</em><em>[</em><em>dict</em><em>]</em>) – Use and configure DefaultAzureCredential.
      - <strong>local_dir</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) – Path to the local directory in which to stage logs. Default:
      - <strong>prefix</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) – Prefix for the log file keys.
      - <strong>upload_interval</strong> (<em>Optional</em><em>[</em><em>int</em><em>]</em>) – Interval in seconds to upload partial log files blob storage. By default, will only upload when the capture is complete.
      - <strong>show_url_only</strong> (<em>bool</em>) – Only show the URL of the log file in the UI, instead of fetching and displaying the full content. Default False.
      - <strong>inst_data</strong> (<em>Optional</em><em>[</em>[*ConfigurableClassData*](../internals.mdx#dagster._serdes.ConfigurableClassData)<em>]</em>) – Serializable representation of the compute


    Examples:
    Using an Azure Blob Storage account with an [AzureSecretCredential](https://learn.microsoft.com/en-us/python/api/azure-identity/azure.identity.clientsecretcredential?view=azure-python):

        ```YAML
        compute_logs:
          module: dagster_azure.blob.compute_log_manager
          class: AzureBlobComputeLogManager
          config:
            storage_account: my-storage-account
            container: my-container
            secret_credential:
              client_id: my-client-id
              client_secret: my-client-secret
              tenant_id: my-tenant-id
            prefix: "dagster-test-"
            local_dir: "/tmp/cool"
            upload_interval: 30
            show_url_only: false
        ```
    Using an Azure Blob Storage account with a [DefaultAzureCredential](https://learn.microsoft.com/en-us/python/api/azure-identity/azure.identity.defaultazurecredential?view=azure-python):

        ```YAML
        compute_logs:
          module: dagster_azure.blob.compute_log_manager
          class: AzureBlobComputeLogManager
          config:
            storage_account: my-storage-account
            container: my-container
            default_azure_credential:
              exclude_environment_credential: false
            prefix: "dagster-test-"
            local_dir: "/tmp/cool"
            upload_interval: 30
            show_url_only: false
        ```
    Using an Azure Blob Storage account with an access key:

        ```YAML
        compute_logs:
          module: dagster_azure.blob.compute_log_manager
          class: AzureBlobComputeLogManager
            config:
            storage_account: my-storage-account
            container: my-container
            access_key_or_sas_token: my-access-key
            prefix: "dagster-test-"
            local_dir: "/tmp/cool"
            upload_interval: 30
            show_url_only: false
        ```

    </dd>

</dl>
</div>


<div class="section" id="i-o-manager">


## I/O Manager

<dl>
    <dt><Link id='dagster_azure.adls2.ADLS2PickleIOManager'>dagster_azure.adls2.ADLS2PickleIOManager IOManagerDefinition</Link></dt>
    <dd>

        <div className='lineblock'> </div>

    Persistent IO manager using Azure Data Lake Storage Gen2 for storage.

    Serializes objects via pickling. Suitable for objects storage for distributed executors, so long
    as each execution node has network connectivity and credentials for ADLS and the backing
    container.

    Assigns each op output to a unique filepath containing run ID, step key, and output name.
    Assigns each asset to a single filesystem path, at “\<base_dir>/\<asset_key>”. If the asset key
    has multiple components, the final component is used as the name of the file, and the preceding
    components as parent directories under the base_dir.

    Subsequent materializations of an asset will overwrite previous materializations of that asset.
    With a base directory of “/my/base/path”, an asset with key
    <cite>AssetKey([“one”, “two”, “three”])</cite> would be stored in a file called “three” in a directory
    with path “/my/base/path/one/two/”.

    Example usage:

    1. Attach this IO manager to a set of assets.
        ```python
        from dagster import Definitions, asset
        from dagster_azure.adls2 import ADLS2PickleIOManager, ADLS2Resource, ADLS2SASToken

        @asset
        def asset1():
            # create df ...
            return df

        @asset
        def asset2(asset1):
            return df[:5]

        defs = Definitions(
            assets=[asset1, asset2],
            resources={
                "io_manager": ADLS2PickleIOManager(
                    adls2_file_system="my-cool-fs",
                    adls2_prefix="my-cool-prefix",
                    adls2=ADLS2Resource(
                        storage_account="my-storage-account",
                        credential=ADLS2SASToken(token="my-sas-token"),
                    ),
                ),
            },
        )
        ```
    2. Attach this IO manager to your job to make it available to your ops.
        ```python
        from dagster import job
        from dagster_azure.adls2 import ADLS2PickleIOManager, ADLS2Resource, ADLS2SASToken

        @job(
            resource_defs={
                "io_manager": ADLS2PickleIOManager(
                    adls2_file_system="my-cool-fs",
                    adls2_prefix="my-cool-prefix",
                    adls2=ADLS2Resource(
                        storage_account="my-storage-account",
                        credential=ADLS2SASToken(token="my-sas-token"),
                    ),
                ),
            },
        )
        def my_job():
            ...
        ```

    </dd>

</dl>
</div>


<div class="section" id="file-manager-experimental">


## File Manager (Experimental)

<dl>
    <dt><Link id='dagster_azure.adls2.adls2_file_manager'>dagster_azure.adls2.adls2_file_manager ResourceDefinition</Link></dt>
    <dd>

        <div className='lineblock'> </div>

    FileManager that provides abstract access to ADLS2.

    Implements the [`FileManager`](../internals.mdx#dagster._core.storage.file_manager.FileManager) API.


    </dd>

</dl>
<dl>
    <dt><Link id='dagster_azure.adls2.ADLS2FileHandle'>class dagster_azure.adls2.ADLS2FileHandle</Link></dt>
    <dd>
    A reference to a file on ADLS2.
    </dd>

</dl>
</div>


<div class="section" id="legacy">

## Legacy

<dl>
    <dt><Link id='dagster_azure.adls2.ConfigurablePickledObjectADLS2IOManager'>dagster_azure.adls2.ConfigurablePickledObjectADLS2IOManager IOManagerDefinition</Link></dt>
    <dd>

        <div className='lineblock'> </div>

        :::danger[deprecated]
        This API will be removed in version 2.0.
         Please use ADLS2PickleIOManager instead..

        :::

    Renamed to ADLS2PickleIOManager. See ADLS2PickleIOManager for documentation.


    </dd>

</dl>
<dl>
    <dt><Link id='dagster_azure.adls2.adls2_resource'>dagster_azure.adls2.adls2_resource ResourceDefinition</Link></dt>
    <dd>

        <div className='lineblock'> </div>

    Resource that gives ops access to Azure Data Lake Storage Gen2.

    The underlying client is a `DataLakeServiceClient`.

    Attach this resource definition to a [`JobDefinition`](../jobs.mdx#dagster.JobDefinition) in order to make it
    available to your ops.

    Example:

        ```python
        from dagster import job, op
        from dagster_azure.adls2 import adls2_resource

        @op(required_resource_keys={'adls2'})
        def example_adls2_op(context):
            return list(context.resources.adls2.adls2_client.list_file_systems())

        @job(resource_defs={"adls2": adls2_resource})
        def my_job():
            example_adls2_op()
        ```
    Note that your ops must also declare that they require this resource with
    <cite>required_resource_keys</cite>, or it will not be initialized for the execution of their compute
    functions.

    You may pass credentials to this resource using either a SAS token, a key or by passing the
    <cite>DefaultAzureCredential</cite> object.

        ```YAML
        resources:
          adls2:
            config:
              storage_account: my_storage_account
              # str: The storage account name.
              credential:
                sas: my_sas_token
                # str: the SAS token for the account.
                key:
                  env: AZURE_DATA_LAKE_STORAGE_KEY
                # str: The shared access key for the account.
                DefaultAzureCredential: {}
                # dict: The keyword arguments used for DefaultAzureCredential
                # or leave the object empty for no arguments
                DefaultAzureCredential:
                    exclude_environment_credential: true
        ```

    </dd>

</dl>
<dl>
    <dt><Link id='dagster_azure.adls2.adls2_pickle_io_manager'>dagster_azure.adls2.adls2_pickle_io_manager IOManagerDefinition</Link></dt>
    <dd>

        <div className='lineblock'> </div>

    Persistent IO manager using Azure Data Lake Storage Gen2 for storage.

    Serializes objects via pickling. Suitable for objects storage for distributed executors, so long
    as each execution node has network connectivity and credentials for ADLS and the backing
    container.

    Assigns each op output to a unique filepath containing run ID, step key, and output name.
    Assigns each asset to a single filesystem path, at “\<base_dir>/\<asset_key>”. If the asset key
    has multiple components, the final component is used as the name of the file, and the preceding
    components as parent directories under the base_dir.

    Subsequent materializations of an asset will overwrite previous materializations of that asset.
    With a base directory of “/my/base/path”, an asset with key
    <cite>AssetKey([“one”, “two”, “three”])</cite> would be stored in a file called “three” in a directory
    with path “/my/base/path/one/two/”.

    Example usage:

    Attach this IO manager to a set of assets.

        ```python
        from dagster import Definitions, asset
        from dagster_azure.adls2 import adls2_pickle_io_manager, adls2_resource

        @asset
        def asset1():
            # create df ...
            return df

        @asset
        def asset2(asset1):
            return df[:5]

        defs = Definitions(
            assets=[asset1, asset2],
            resources={
                "io_manager": adls2_pickle_io_manager.configured(
                    {"adls2_file_system": "my-cool-fs", "adls2_prefix": "my-cool-prefix"}
                ),
                "adls2": adls2_resource,
            },
        )
        ```
    Attach this IO manager to your job to make it available to your ops.

        ```python
        from dagster import job
        from dagster_azure.adls2 import adls2_pickle_io_manager, adls2_resource

        @job(
            resource_defs={
                "io_manager": adls2_pickle_io_manager.configured(
                    {"adls2_file_system": "my-cool-fs", "adls2_prefix": "my-cool-prefix"}
                ),
                "adls2": adls2_resource,
            },
        )
        def my_job():
            ...
        ```

    </dd>

</dl>
</div></div>
