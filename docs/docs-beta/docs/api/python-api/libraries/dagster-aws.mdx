---
title: 'aws (dagster-aws)'
title_meta: 'aws (dagster-aws) API Documentation - Build Better Data Pipelines | Python Reference Documentation for Dagster'
description: 'aws (dagster-aws) Dagster API | Comprehensive Python API documentation for Dagster, the data orchestration platform. Learn how to build, test, and maintain data pipelines with our detailed guides and examples.'
last_update:
  date: '2025-02-03'
---

<div class="section" id="aws-dagster-aws">


# AWS (dagster-aws)

Utilities for interfacing with AWS with Dagster.

<div class="section" id="s3">


## S3

<dl>
    <dt><Link id='dagster_aws.s3.S3Resource'>dagster_aws.s3.S3Resource ResourceDefinition</Link></dt>
    <dd>

        <div className='lineblock'> </div>

    Resource that gives access to S3.

    The underlying S3 session is created by calling
    `boto3.session.Session(profile_name)`.
    The returned resource object is an S3 client, an instance of <cite>botocore.client.S3</cite>.

    Example:

        ```python
        from dagster import job, op, Definitions
        from dagster_aws.s3 import S3Resource

        @op
        def example_s3_op(s3: S3Resource):
            return s3.get_client().list_objects_v2(
                Bucket='my-bucket',
                Prefix='some-key'
            )

        @job
        def example_job():
            example_s3_op()

        defs = Definitions(
            jobs=[example_job],
            resources={'s3': S3Resource(region_name='us-west-1')}
        )
        ```

    </dd>

</dl>
<dl>
    <dt><Link id='dagster_aws.s3.S3PickleIOManager'>dagster_aws.s3.S3PickleIOManager IOManagerDefinition</Link></dt>
    <dd>

        <div className='lineblock'> </div>

    Persistent IO manager using S3 for storage.

    Serializes objects via pickling. Suitable for objects storage for distributed executors, so long
    as each execution node has network connectivity and credentials for S3 and the backing bucket.

    Assigns each op output to a unique filepath containing run ID, step key, and output name.
    Assigns each asset to a single filesystem path, at “\<base_dir>/\<asset_key>”. If the asset key
    has multiple components, the final component is used as the name of the file, and the preceding
    components as parent directories under the base_dir.

    Subsequent materializations of an asset will overwrite previous materializations of that asset.
    With a base directory of “/my/base/path”, an asset with key
    <cite>AssetKey([“one”, “two”, “three”])</cite> would be stored in a file called “three” in a directory
    with path “/my/base/path/one/two/”.

    Example usage:

        ```python
        from dagster import asset, Definitions
        from dagster_aws.s3 import S3PickleIOManager, S3Resource


        @asset
        def asset1():
            # create df ...
            return df

        @asset
        def asset2(asset1):
            return asset1[:5]

        defs = Definitions(
            assets=[asset1, asset2],
            resources={
                "io_manager": S3PickleIOManager(
                    s3_resource=S3Resource(),
                    s3_bucket="my-cool-bucket",
                    s3_prefix="my-cool-prefix",
                )
            }
        )
        ```

    </dd>

</dl>
<dl>
    <dt><Link id='dagster_aws.s3.S3ComputeLogManager'>class dagster_aws.s3.S3ComputeLogManager</Link></dt>
    <dd>

    Logs compute function stdout and stderr to S3.

    Users should not instantiate this class directly. Instead, use a YAML block in `dagster.yaml`
    such as the following:

        ```YAML
        compute_logs:
          module: dagster_aws.s3.compute_log_manager
          class: S3ComputeLogManager
          config:
            bucket: "mycorp-dagster-compute-logs"
            local_dir: "/tmp/cool"
            prefix: "dagster-test-"
            use_ssl: true
            verify: true
            verify_cert_path: "/path/to/cert/bundle.pem"
            endpoint_url: "http://alternate-s3-host.io"
            skip_empty_files: true
            upload_interval: 30
            upload_extra_args:
              ServerSideEncryption: "AES256"
            show_url_only: false
            region: "us-west-1"
        ```
    Parameters: 
      - <strong>bucket</strong> (<em>str</em>) – The name of the s3 bucket to which to log.
      - <strong>local_dir</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) – Path to the local directory in which to stage logs. Default:
      - <strong>prefix</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) – Prefix for the log file keys.
      - <strong>use_ssl</strong> (<em>Optional</em><em>[</em><em>bool</em><em>]</em>) – Whether or not to use SSL. Default True.
      - <strong>verify</strong> (<em>Optional</em><em>[</em><em>bool</em><em>]</em>) – Whether or not to verify SSL certificates. Default True.
      - <strong>verify_cert_path</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) – A filename of the CA cert bundle to use. Only used if
      - <strong>endpoint_url</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) – Override for the S3 endpoint url.
      - <strong>skip_empty_files</strong> – (Optional[bool]): Skip upload of empty log files.
      - <strong>upload_interval</strong> – (Optional[int]): Interval in seconds to upload partial log files to S3. By default, will only upload when the capture is complete.
      - <strong>upload_extra_args</strong> – (Optional[dict]): Extra args for S3 file upload
      - <strong>show_url_only</strong> – (Optional[bool]): Only show the URL of the log file in the UI, instead of fetching and displaying the full content. Default False.
      - <strong>region</strong> – (Optional[str]): The region of the S3 bucket. If not specified, will use the default region of the AWS session.
      - <strong>inst_data</strong> (<em>Optional</em><em>[</em>[*ConfigurableClassData*](../internals.mdx#dagster._serdes.ConfigurableClassData)<em>]</em>) – Serializable representation of the compute



    </dd>

</dl>
<dl>
    <dt><Link id='dagster_aws.s3.S3Coordinate'>dagster_aws.s3.S3Coordinate DagsterType</Link></dt>
    <dd>

    A [`dagster.DagsterType`](../types.mdx#dagster.DagsterType) intended to make it easier to pass information about files on S3
    from op to op. Objects of this type should be dicts with `'bucket'` and `'key'` keys,
    and may be hydrated from config in the intuitive way, e.g., for an input with the name
    `s3_file`:

        ```YAML
        inputs:
          s3_file:
            value:
              bucket: my-bucket
              key: my-key
        ```

    </dd>

</dl>
<div class="section" id="file-manager-experimental">


### File Manager (Experimental)

<dl>
    <dt><Link id='dagster_aws.s3.S3FileHandle'>class dagster_aws.s3.S3FileHandle</Link></dt>
    <dd>
    A reference to a file on S3.
    </dd>

</dl>
<dl>
    <dt><Link id='dagster_aws.s3.S3FileManagerResource'>dagster_aws.s3.S3FileManagerResource ResourceDefinition</Link></dt>
    <dd>

        <div className='lineblock'> </div>

    Base class for Dagster resources that utilize structured config.

    This class is a subclass of both `ResourceDefinition` and `Config`.

    Example definition:

        ```python
        class WriterResource(ConfigurableResource):
            prefix: str

            def output(self, text: str) -> None:
                print(f"{self.prefix}{text}")
        ```
    Example usage:

        ```python
        @asset
        def asset_that_uses_writer(writer: WriterResource):
            writer.output("text")

        defs = Definitions(
            assets=[asset_that_uses_writer],
            resources={"writer": WriterResource(prefix="a_prefix")},
        )
        ```
    You can optionally use this class to model configuration only and vend an object
    of a different type for use at runtime. This is useful for those who wish to
    have a separate object that manages configuration and a separate object at runtime. Or
    where you want to directly use a third-party class that you do not control.

    To do this you override the <cite>create_resource</cite> methods to return a different object.

        ```python
        class WriterResource(ConfigurableResource):
            str: prefix

            def create_resource(self, context: InitResourceContext) -> Writer:
                # Writer is pre-existing class defined else
                return Writer(self.prefix)
        ```
    Example usage:

        ```python
        @asset
        def use_preexisting_writer_as_resource(writer: ResourceParam[Writer]):
            writer.output("text")

        defs = Definitions(
            assets=[use_preexisting_writer_as_resource],
            resources={"writer": WriterResource(prefix="a_prefix")},
        )
        ```

    </dd>

</dl>
</div></div>


<div class="section" id="ecs">


## ECS

<dl>
    <dt><Link id='dagster_aws.ecs.EcsRunLauncher'>dagster_aws.ecs.EcsRunLauncher RunLauncher</Link></dt>
    <dd>

        <div className='lineblock'> </div>

    RunLauncher that starts a task in ECS for each Dagster job run.


    </dd>

</dl>
<dl>
    <dt><Link id='dagster_aws.ecs.ecs_executor'>dagster_aws.ecs.ecs_executor ExecutorDefinition</Link></dt>
    <dd>

        <div className='lineblock'> </div>

        :::warning[experimental]
        This API may break in future versions, even between dot releases.


        :::

    Executor which launches steps as ECS tasks.

    To use the <cite>ecs_executor</cite>, set it as the <cite>executor_def</cite> when defining a job:

        ```python
        from dagster_aws.ecs import ecs_executor

        from dagster import job, op


        @op(
            tags={"ecs/cpu": "256", "ecs/memory": "512"},
        )
        def ecs_op():
            pass


        @job(executor_def=ecs_executor)
        def ecs_job():
            ecs_op()


        ```
    Then you can configure the executor with run config as follows:

        ```YAML
        execution:
          config:
            cpu: 1024
            memory: 2048
            ephemeral_storage: 10
            task_overrides:
            containerOverrides:
              - name: run
                environment:
                  - name: MY_ENV_VAR
                    value: "my_value"
        ```
    <cite>max_concurrent</cite> limits the number of ECS tasks that will execute concurrently for one run. By default
    there is no limit- it will maximally parallel as allowed by the DAG. Note that this is not a
    global limit.

    Configuration set on the ECS tasks created by the <cite>ECSRunLauncher</cite> will also be
    set on the tasks created by the <cite>ecs_executor</cite>.

    Configuration set using <cite>tags</cite> on a <cite>@job</cite> will only apply to the <cite>run</cite> level. For configuration
    to apply at each <cite>step</cite> it must be set using <cite>tags</cite> for each <cite>@op</cite>.


    </dd>

</dl>
</div>


<div class="section" id="redshift">


## Redshift

<dl>
    <dt><Link id='dagster_aws.redshift.RedshiftClientResource'>dagster_aws.redshift.RedshiftClientResource ResourceDefinition</Link></dt>
    <dd>

        <div className='lineblock'> </div>

    This resource enables connecting to a Redshift cluster and issuing queries against that
    cluster.

    Example:

        ```python
        from dagster import Definitions, asset, EnvVar
        from dagster_aws.redshift import RedshiftClientResource

        @asset
        def example_redshift_asset(context, redshift: RedshiftClientResource):
            redshift.get_client().execute_query('SELECT 1', fetch_results=True)

        redshift_configured = RedshiftClientResource(
            host='my-redshift-cluster.us-east-1.redshift.amazonaws.com',
            port=5439,
            user='dagster',
            password=EnvVar("DAGSTER_REDSHIFT_PASSWORD"),
            database='dev',
        )

        defs = Definitions(
            assets=[example_redshift_asset],
            resources={'redshift': redshift_configured},
        )
        ```

    </dd>

</dl>
<div class="section" id="testing">


### Testing

<dl>
    <dt><Link id='dagster_aws.redshift.FakeRedshiftClientResource'>dagster_aws.redshift.FakeRedshiftClientResource ResourceDefinition</Link></dt>
    <dd>

        <div className='lineblock'> </div>

    This resource enables connecting to a Redshift cluster and issuing queries against that
    cluster.

    Example:

        ```python
        from dagster import Definitions, asset, EnvVar
        from dagster_aws.redshift import RedshiftClientResource

        @asset
        def example_redshift_asset(context, redshift: RedshiftClientResource):
            redshift.get_client().execute_query('SELECT 1', fetch_results=True)

        redshift_configured = RedshiftClientResource(
            host='my-redshift-cluster.us-east-1.redshift.amazonaws.com',
            port=5439,
            user='dagster',
            password=EnvVar("DAGSTER_REDSHIFT_PASSWORD"),
            database='dev',
        )

        defs = Definitions(
            assets=[example_redshift_asset],
            resources={'redshift': redshift_configured},
        )
        ```

    </dd>

</dl>
</div></div>


<div class="section" id="emr">


## EMR

<dl>
    <dt><Link id='dagster_aws.emr.emr_pyspark_step_launcher'>dagster_aws.emr.emr_pyspark_step_launcher ResourceDefinition</Link></dt>
    <dd>

        <div className='lineblock'> </div>

        :::warning[superseded]
        This API has been superseded.
         While there is no plan to remove this functionality, for new projects, we recommend using Dagster Pipes. For more information, see https://docs.dagster.io/guides/build/external-pipelines/.

        :::

      - <strong>spark_config</strong>:
      - <strong>cluster_id</strong>: Name of the job flow (cluster) on which to execute.
      - <strong>region_name</strong>: The AWS region that the cluster is in.
      - <strong>action_on_failure</strong>: The EMR action to take when the cluster step fails: [https://docs.aws.amazon.com/emr/latest/APIReference/API_StepConfig.html](https://docs.aws.amazon.com/emr/latest/APIReference/API_StepConfig.html)
      - <strong>staging_bucket</strong>: S3 bucket to use for passing files between the plan process and EMR process.
      - <strong>staging_prefix</strong>: S3 key prefix inside the staging_bucket to use for files passed the plan process and EMR process
      - <strong>wait_for_logs</strong>: If set, the system will wait for EMR logs to appear on S3. Note that logs are copied every 5 minutes, so enabling this will add several minutes to the job runtime.
      - <strong>local_job_package_path</strong>: Absolute path to the package that contains the job definition(s) whose steps will execute remotely on EMR. This is a path on the local fileystem of the process executing the job. The expectation is that this package will also be available on the python path of the launched process running the Spark step on EMR, either deployed on step launch via the deploy_local_job_package option, referenced on s3 via the s3_job_package_path option, or installed on the cluster via bootstrap actions.
      - <strong>local_pipeline_package_path</strong>: (legacy) Absolute path to the package that contains the pipeline definition(s) whose steps will execute remotely on EMR. This is a path on the local fileystem of the process executing the pipeline. The expectation is that this package will also be available on the python path of the launched process running the Spark step on EMR, either deployed on step launch via the deploy_local_pipeline_package option, referenced on s3 via the s3_pipeline_package_path option, or installed on the cluster via bootstrap actions.
      - <strong>deploy_local_job_package</strong>: If set, before every step run, the launcher will zip up all the code in local_job_package_path, upload it to s3, and pass it to spark-submit’s –py-files option. This gives the remote process access to up-to-date user code. If not set, the assumption is that some other mechanism is used for distributing code to the EMR cluster. If this option is set to True, s3_job_package_path should not also be set.
      - <strong>deploy_local_pipeline_package</strong>: (legacy) If set, before every step run, the launcher will zip up all the code in local_job_package_path, upload it to s3, and pass it to spark-submit’s –py-files option. This gives the remote process access to up-to-date user code. If not set, the assumption is that some other mechanism is used for distributing code to the EMR cluster. If this option is set to True, s3_job_package_path should not also be set.
      - <strong>s3_job_package_path</strong>: If set, this path will be passed to the –py-files option of spark-submit. This should usually be a path to a zip file.  If this option is set, deploy_local_job_package should not be set to True.
      - <strong>s3_pipeline_package_path</strong>: If set, this path will be passed to the –py-files option of spark-submit. This should usually be a path to a zip file.  If this option is set, deploy_local_pipeline_package should not be set to True.



    </dd>

</dl>
<dl>
    <dt><Link id='dagster_aws.emr.EmrJobRunner'>class dagster_aws.emr.EmrJobRunner</Link></dt>
    <dd>

    </dd>

</dl>
<dl>
    <dt><Link id='dagster_aws.emr.EmrError'>class dagster_aws.emr.EmrError</Link></dt>
    <dd>

    </dd>

</dl>
<dl>
    <dt><Link id='dagster_aws.emr.EmrClusterState'>dagster_aws.emr.EmrClusterState `=` \<enum 'EmrClusterState'></Link></dt>
    <dd>
    Cluster state for EMR.
    </dd>

</dl>
<dl>
    <dt><Link id='dagster_aws.emr.EmrStepState'>dagster_aws.emr.EmrStepState `=` \<enum 'EmrStepState'></Link></dt>
    <dd>
    Step state for EMR.
    </dd>

</dl>
</div>


<div class="section" id="cloudwatch">


## CloudWatch

<dl>
    <dt><Link id='dagster_aws.cloudwatch.cloudwatch_logger'>dagster_aws.cloudwatch.cloudwatch_logger LoggerDefinition</Link></dt>
    <dd>

        <div className='lineblock'> </div>

    Core class for defining loggers.

    Loggers are job-scoped logging handlers, which will be automatically invoked whenever
    dagster messages are logged from within a job.

    Parameters: 
      - <strong>logger_fn</strong> (<em>Callable</em><em>[</em><em>[</em>[*InitLoggerContext*](../loggers.mdx#dagster.InitLoggerContext)<em>]</em><em>, </em><em>logging.Logger</em><em>]</em>) – User-provided function to
      - <strong>config_schema</strong> (<em>Optional</em><em>[</em>[*ConfigSchema*](../config.mdx#dagster.ConfigSchema)<em>]</em>) – The schema for the config. Configuration data available in
      - <strong>description</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) – A human-readable description of this logger.



    </dd>

</dl>
</div>


<div class="section" id="secretsmanager">


## SecretsManager

Resources which surface SecretsManager secrets for use in Dagster resources and jobs.

<dl>
    <dt><Link id='dagster_aws.secretsmanager.SecretsManagerResource'>dagster_aws.secretsmanager.SecretsManagerResource ResourceDefinition</Link></dt>
    <dd>

        <div className='lineblock'> </div>

    Resource that gives access to AWS SecretsManager.

    The underlying SecretsManager session is created by calling
    `boto3.session.Session(profile_name)`.
    The returned resource object is a SecretsManager client, an instance of <cite>botocore.client.SecretsManager</cite>.

    Example:

        ```python
        from dagster import build_op_context, job, op
        from dagster_aws.secretsmanager import SecretsManagerResource

        @op
        def example_secretsmanager_op(secretsmanager: SecretsManagerResource):
            return secretsmanager.get_client().get_secret_value(
                SecretId='arn:aws:secretsmanager:region:aws_account_id:secret:appauthexample-AbCdEf'
            )

        @job
        def example_job():
            example_secretsmanager_op()

        defs = Definitions(
            jobs=[example_job],
            resources={
                'secretsmanager': SecretsManagerResource(
                    region_name='us-west-1'
                )
            }
        )
        ```

    </dd>

</dl>
<dl>
    <dt><Link id='dagster_aws.secretsmanager.SecretsManagerSecretsResource'>dagster_aws.secretsmanager.SecretsManagerSecretsResource ResourceDefinition</Link></dt>
    <dd>

        <div className='lineblock'> </div>

    Resource that provides a dict which maps selected SecretsManager secrets to
    their string values. Also optionally sets chosen secrets as environment variables.

    Example:

        ```python
        import os
        from dagster import build_op_context, job, op, ResourceParam
        from dagster_aws.secretsmanager import SecretsManagerSecretsResource

        @op
        def example_secretsmanager_secrets_op(secrets: SecretsManagerSecretsResource):
            return secrets.fetch_secrets().get("my-secret-name")

        @op
        def example_secretsmanager_secrets_op_2(secrets: SecretsManagerSecretsResource):
            with secrets.secrets_in_environment():
                return os.getenv("my-other-secret-name")

        @job
        def example_job():
            example_secretsmanager_secrets_op()
            example_secretsmanager_secrets_op_2()

        defs = Definitions(
            jobs=[example_job],
            resources={
                'secrets': SecretsManagerSecretsResource(
                    region_name='us-west-1',
                    secrets_tag="dagster",
                    add_to_environment=True,
                )
            }
        )
        ```
    Note that your ops must also declare that they require this resource with or it will not be initialized
    for the execution of their compute functions.


    </dd>

</dl>
</div>


<div class="section" id="pipes">


## Pipes

<div class="section" id="context-injectors">


### Context Injectors

<dl>
    <dt><Link id='dagster_aws.pipes.PipesS3ContextInjector'>class dagster_aws.pipes.PipesS3ContextInjector</Link></dt>
    <dd>

    A context injector that injects context by writing to a temporary S3 location.

    Parameters: 
      - <strong>bucket</strong> (<em>str</em>) – The S3 bucket to write to.
      - <strong>client</strong> (<em>boto3.client</em>) – A boto3 client to use to write to S3.
      - <strong>key_prefix</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) – An optional prefix to use for the S3 key. Defaults to a random



    </dd>

</dl>
<dl>
    <dt><Link id='dagster_aws.pipes.PipesLambdaEventContextInjector'>class dagster_aws.pipes.PipesLambdaEventContextInjector</Link></dt>
    <dd>
    Injects context via AWS Lambda event input.
    Should be paired with :py:class`~dagster_pipes.PipesMappingParamsLoader` on the Lambda side.
    </dd>

</dl>
</div>


<div class="section" id="message-readers">


### Message Readers

<dl>
    <dt><Link id='dagster_aws.pipes.PipesS3MessageReader'>class dagster_aws.pipes.PipesS3MessageReader</Link></dt>
    <dd>

    Message reader that reads messages by periodically reading message chunks from a specified S3
    bucket.

    If <cite>log_readers</cite> is passed, this reader will also start the passed readers
    when the first message is received from the external process.

    Parameters: 
      - <strong>interval</strong> (<em>float</em>) – interval in seconds between attempts to download a chunk
      - <strong>bucket</strong> (<em>str</em>) – The S3 bucket to read from.
      - <strong>client</strong> (<em>WorkspaceClient</em>) – A boto3 client.
      - <strong>log_readers</strong> (<em>Optional</em><em>[</em><em>Sequence</em><em>[</em><em>PipesLogReader</em><em>]</em><em>]</em>) – A set of log readers for logs on S3.
      - <strong>include_stdio_in_messages</strong> (<em>bool</em>) – Whether to send stdout/stderr to Dagster via Pipes messages. Defaults to False.



    </dd>

</dl>
<dl>
    <dt><Link id='dagster_aws.pipes.PipesCloudWatchMessageReader'>class dagster_aws.pipes.PipesCloudWatchMessageReader</Link></dt>
    <dd>

        :::warning[experimental]
        This API may break in future versions, even between dot releases.


        :::

    Message reader that consumes AWS CloudWatch logs to read pipes messages.


    </dd>

</dl>
</div>


<div class="section" id="clients">


### Clients

<dl>
    <dt><Link id='dagster_aws.pipes.PipesLambdaClient'>class dagster_aws.pipes.PipesLambdaClient</Link></dt>
    <dd>

    A pipes client for invoking AWS lambda.

    By default context is injected via the lambda input event and messages are parsed out of the
    4k tail of logs.

    Parameters: 
      - <strong>client</strong> (<em>boto3.client</em>) – The boto lambda client used to call invoke.
      - <strong>context_injector</strong> (<em>Optional</em><em>[</em>[*PipesContextInjector*](../pipes.mdx#dagster.PipesContextInjector)<em>]</em>) – A context injector to use to inject
      - <strong>message_reader</strong> (<em>Optional</em><em>[</em>[*PipesMessageReader*](../pipes.mdx#dagster.PipesMessageReader)<em>]</em>) – A message reader to use to read messages


    <dl>
        <dt><Link id='dagster_aws.pipes.PipesLambdaClient.run'>run</Link></dt>
        <dd>

        Synchronously invoke a lambda function, enriched with the pipes protocol.

        Parameters: 
          - <strong>function_name</strong> (<em>str</em>) – The name of the function to use.
          - <strong>event</strong> (<em>Mapping</em><em>[</em><em>str</em><em>, </em><em>Any</em><em>]</em>) – A JSON serializable object to pass as input to the lambda.
          - <strong>context</strong> (<em>Union</em><em>[</em>[*OpExecutionContext*](../execution.mdx#dagster.OpExecutionContext)<em>, </em>[*AssetExecutionContext*](../execution.mdx#dagster.AssetExecutionContext)<em>]</em>) – The context of the currently executing Dagster op or asset.


        Returns: Wrapper containing results reported by the external
        process.Return type: PipesClientCompletedInvocation

        </dd>

    </dl>

    </dd>

</dl>
<dl>
    <dt><Link id='dagster_aws.pipes.PipesGlueClient'>class dagster_aws.pipes.PipesGlueClient</Link></dt>
    <dd>

        :::warning[experimental]
        This API may break in future versions, even between dot releases.


        :::

    A pipes client for invoking AWS Glue jobs.

    Parameters: 
      - <strong>context_injector</strong> (<em>Optional</em><em>[</em>[*PipesContextInjector*](../pipes.mdx#dagster.PipesContextInjector)<em>]</em>) – A context injector to use to inject
      - <strong>message_reader</strong> (<em>Optional</em><em>[</em>[*PipesMessageReader*](../pipes.mdx#dagster.PipesMessageReader)<em>]</em>) – A message reader to use to read messages
      - <strong>client</strong> (<em>Optional</em><em>[</em><em>boto3.client</em><em>]</em>) – The boto Glue client used to launch the Glue job
      - <strong>forward_termination</strong> (<em>bool</em>) – Whether to cancel the Glue job run when the Dagster process receives a termination signal.


    <dl>
        <dt><Link id='dagster_aws.pipes.PipesGlueClient.run'>run</Link></dt>
        <dd>

        Start a Glue job, enriched with the pipes protocol.

        See also: [AWS API Documentation](https://docs.aws.amazon.com/goto/WebAPI/glue-2017-03-31/StartJobRun)

        Parameters: 
          - <strong>context</strong> (<em>Union</em><em>[</em>[*OpExecutionContext*](../execution.mdx#dagster.OpExecutionContext)<em>, </em>[*AssetExecutionContext*](../execution.mdx#dagster.AssetExecutionContext)<em>]</em>) – The context of the currently executing Dagster op or asset.
          - <strong>start_job_run_params</strong> (<em>Dict</em>) – Parameters for the `start_job_run` boto3 Glue client call.
          - <strong>extras</strong> (<em>Optional</em><em>[</em><em>Dict</em><em>[</em><em>str</em><em>, </em><em>Any</em><em>]</em><em>]</em>) – Additional Dagster metadata to pass to the Glue job.


        Returns: Wrapper containing results reported by the external
        process.Return type: PipesClientCompletedInvocation

        </dd>

    </dl>

    </dd>

</dl>
<dl>
    <dt><Link id='dagster_aws.pipes.PipesECSClient'>class dagster_aws.pipes.PipesECSClient</Link></dt>
    <dd>

        :::warning[experimental]
        This API may break in future versions, even between dot releases.


        :::

    A pipes client for running AWS ECS tasks.

    Parameters: 
      - <strong>client</strong> (<em>Any</em>) – The boto ECS client used to launch the ECS task
      - <strong>context_injector</strong> (<em>Optional</em><em>[</em>[*PipesContextInjector*](../pipes.mdx#dagster.PipesContextInjector)<em>]</em>) – A context injector to use to inject
      - <strong>message_reader</strong> (<em>Optional</em><em>[</em>[*PipesMessageReader*](../pipes.mdx#dagster.PipesMessageReader)<em>]</em>) – A message reader to use to read messages
      - <strong>forward_termination</strong> (<em>bool</em>) – Whether to cancel the ECS task when the Dagster process receives a termination signal.


    <dl>
        <dt><Link id='dagster_aws.pipes.PipesECSClient.run'>run</Link></dt>
        <dd>

        Run ECS tasks, enriched with the pipes protocol.

        Parameters: 
          - <strong>context</strong> (<em>Union</em><em>[</em>[*OpExecutionContext*](../execution.mdx#dagster.OpExecutionContext)<em>, </em>[*AssetExecutionContext*](../execution.mdx#dagster.AssetExecutionContext)<em>]</em>) – The context of the currently executing Dagster op or asset.
          - <strong>run_task_params</strong> (<em>dict</em>) – Parameters for the `run_task` boto3 ECS client call.
          - <strong>extras</strong> (<em>Optional</em><em>[</em><em>Dict</em><em>[</em><em>str</em><em>, </em><em>Any</em><em>]</em><em>]</em>) – Additional information to pass to the Pipes session in the external process.
          - <strong>pipes_container_name</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) – If running more than one container in the task,
          - <strong>waiter_config</strong> (<em>Optional</em><em>[</em><em>WaiterConfig</em><em>]</em>) – Optional waiter configuration to use. Defaults to 70 days (Delay: 6, MaxAttempts: 1000000).


        Returns: Wrapper containing results reported by the external
        process.Return type: PipesClientCompletedInvocation

        </dd>

    </dl>

    </dd>

</dl>
<dl>
    <dt><Link id='dagster_aws.pipes.PipesEMRClient'>class dagster_aws.pipes.PipesEMRClient</Link></dt>
    <dd>

        :::warning[experimental]
        This API may break in future versions, even between dot releases.


        :::

    A pipes client for running jobs on AWS EMR.

    Parameters: 
      - <strong>message_reader</strong> (<em>Optional</em><em>[</em>[*PipesMessageReader*](../pipes.mdx#dagster.PipesMessageReader)<em>]</em>) – A message reader to use to read messages
      - <strong>client</strong> (<em>Optional</em><em>[</em><em>boto3.client</em><em>]</em>) – The boto3 EMR client used to interact with AWS EMR.
      - <strong>context_injector</strong> (<em>Optional</em><em>[</em>[*PipesContextInjector*](../pipes.mdx#dagster.PipesContextInjector)<em>]</em>) – A context injector to use to inject
      - <strong>forward_termination</strong> (<em>bool</em>) – Whether to cancel the EMR job if the Dagster process receives a termination signal.
      - <strong>wait_for_s3_logs_seconds</strong> (<em>int</em>) – The number of seconds to wait for S3 logs to be written after execution completes.
      - <strong>s3_application_logs_prefix</strong> (<em>str</em>) – The prefix to use when looking for application logs in S3.


    <dl>
        <dt><Link id='dagster_aws.pipes.PipesEMRClient.run'>run</Link></dt>
        <dd>

        Run a job on AWS EMR, enriched with the pipes protocol.

        Starts a new EMR cluster for each invocation.

        Parameters: 
          - <strong>context</strong> (<em>Union</em><em>[</em>[*OpExecutionContext*](../execution.mdx#dagster.OpExecutionContext)<em>, </em>[*AssetExecutionContext*](../execution.mdx#dagster.AssetExecutionContext)<em>]</em>) – The context of the currently executing Dagster op or asset.
          - <strong>run_job_flow_params</strong> (<em>Optional</em><em>[</em><em>dict</em><em>]</em>) – Parameters for the `run_job_flow` boto3 EMR client call.
          - <strong>extras</strong> (<em>Optional</em><em>[</em><em>Dict</em><em>[</em><em>str</em><em>, </em><em>Any</em><em>]</em><em>]</em>) – Additional information to pass to the Pipes session in the external process.


        Returns: Wrapper containing results reported by the external process.Return type: PipesClientCompletedInvocation

        </dd>

    </dl>

    </dd>

</dl>
<dl>
    <dt><Link id='dagster_aws.pipes.PipesEMRContainersClient'>class dagster_aws.pipes.PipesEMRContainersClient</Link></dt>
    <dd>

        :::warning[experimental]
        This API may break in future versions, even between dot releases.


        :::

    A pipes client for running workloads on AWS EMR Containers.

    Parameters: 
      - <strong>client</strong> (<em>Optional</em><em>[</em><em>boto3.client</em><em>]</em>) – The boto3 AWS EMR containers client used to interact with AWS EMR Containers.
      - <strong>context_injector</strong> (<em>Optional</em><em>[</em>[*PipesContextInjector*](../pipes.mdx#dagster.PipesContextInjector)<em>]</em>) – A context injector to use to inject
      - <strong>message_reader</strong> (<em>Optional</em><em>[</em>[*PipesMessageReader*](../pipes.mdx#dagster.PipesMessageReader)<em>]</em>) – A message reader to use to read messages
      - <strong>forward_termination</strong> (<em>bool</em>) – Whether to cancel the AWS EMR Containers workload if the Dagster process receives a termination signal.
      - <strong>pipes_params_bootstrap_method</strong> (<em>Literal</em><em>[</em><em>"args"</em><em>, </em><em>"env"</em><em>]</em>) – The method to use to inject parameters into the AWS EMR Containers workload. Defaults to “args”.
      - <strong>waiter_config</strong> (<em>Optional</em><em>[</em><em>WaiterConfig</em><em>]</em>) – Optional waiter configuration to use. Defaults to 70 days (Delay: 6, MaxAttempts: 1000000).


    <dl>
        <dt><Link id='dagster_aws.pipes.PipesEMRContainersClient.run'>run</Link></dt>
        <dd>

        Run a workload on AWS EMR Containers, enriched with the pipes protocol.

        Parameters: 
          - <strong>context</strong> (<em>Union</em><em>[</em>[*OpExecutionContext*](../execution.mdx#dagster.OpExecutionContext)<em>, </em>[*AssetExecutionContext*](../execution.mdx#dagster.AssetExecutionContext)<em>]</em>) – The context of the currently executing Dagster op or asset.
          - <strong>params</strong> (<em>dict</em>) – Parameters for the `start_job_run` boto3 AWS EMR Containers client call.
          - <strong>extras</strong> (<em>Optional</em><em>[</em><em>Dict</em><em>[</em><em>str</em><em>, </em><em>Any</em><em>]</em><em>]</em>) – Additional information to pass to the Pipes session in the external process.


        Returns: Wrapper containing results reported by the external
        process.Return type: PipesClientCompletedInvocation

        </dd>

    </dl>

    </dd>

</dl>
<dl>
    <dt><Link id='dagster_aws.pipes.PipesEMRServerlessClient'>class dagster_aws.pipes.PipesEMRServerlessClient</Link></dt>
    <dd>

        :::warning[experimental]
        This API may break in future versions, even between dot releases.


        :::

    A pipes client for running workloads on AWS EMR Serverless.

    Parameters: 
      - <strong>client</strong> (<em>Optional</em><em>[</em><em>boto3.client</em><em>]</em>) – The boto3 AWS EMR Serverless client used to interact with AWS EMR Serverless.
      - <strong>context_injector</strong> (<em>Optional</em><em>[</em>[*PipesContextInjector*](../pipes.mdx#dagster.PipesContextInjector)<em>]</em>) – A context injector to use to inject
      - <strong>message_reader</strong> (<em>Optional</em><em>[</em>[*PipesMessageReader*](../pipes.mdx#dagster.PipesMessageReader)<em>]</em>) – A message reader to use to read messages
      - <strong>forward_termination</strong> (<em>bool</em>) – Whether to cancel the AWS EMR Serverless workload if the Dagster process receives a termination signal.
      - <strong>poll_interval</strong> (<em>float</em>) – The interval in seconds to poll the AWS EMR Serverless workload for status updates. Defaults to 5 seconds.


    <dl>
        <dt><Link id='dagster_aws.pipes.PipesEMRServerlessClient.run'>run</Link></dt>
        <dd>

        Run a workload on AWS EMR Serverless, enriched with the pipes protocol.

        Parameters: 
          - <strong>context</strong> (<em>Union</em><em>[</em>[*OpExecutionContext*](../execution.mdx#dagster.OpExecutionContext)<em>, </em>[*AssetExecutionContext*](../execution.mdx#dagster.AssetExecutionContext)<em>]</em>) – The context of the currently executing Dagster op or asset.
          - <strong>params</strong> (<em>dict</em>) – Parameters for the `start_job_run` boto3 AWS EMR Serverless client call.
          - <strong>extras</strong> (<em>Optional</em><em>[</em><em>Dict</em><em>[</em><em>str</em><em>, </em><em>Any</em><em>]</em><em>]</em>) – Additional information to pass to the Pipes session in the external process.


        Returns: Wrapper containing results reported by the external
        process.Return type: PipesClientCompletedInvocation

        </dd>

    </dl>

    </dd>

</dl>
</div></div>


<div class="section" id="legacy">

## Legacy

<dl>
    <dt><Link id='dagster_aws.s3.ConfigurablePickledObjectS3IOManager'>dagster_aws.s3.ConfigurablePickledObjectS3IOManager IOManagerDefinition</Link></dt>
    <dd>

        <div className='lineblock'> </div>

        :::danger[deprecated]
        This API will be removed in version 2.0.
         Please use S3PickleIOManager instead..

        :::

    Renamed to S3PickleIOManager. See S3PickleIOManager for documentation.


    </dd>

</dl>
<dl>
    <dt><Link id='dagster_aws.s3.s3_resource'>dagster_aws.s3.s3_resource ResourceDefinition</Link></dt>
    <dd>

        <div className='lineblock'> </div>

    Resource that gives access to S3.

    The underlying S3 session is created by calling
    `boto3.session.Session(profile_name)`.
    The returned resource object is an S3 client, an instance of <cite>botocore.client.S3</cite>.

    Example:

        ```python
        from dagster import build_op_context, job, op
        from dagster_aws.s3 import s3_resource

        @op(required_resource_keys={'s3'})
        def example_s3_op(context):
            return context.resources.s3.list_objects_v2(
                Bucket='my-bucket',
                Prefix='some-key'
            )

        @job(resource_defs={'s3': s3_resource})
        def example_job():
            example_s3_op()

        example_job.execute_in_process(
            run_config={
                'resources': {
                    's3': {
                        'config': {
                            'region_name': 'us-west-1',
                        }
                    }
                }
            }
        )
        ```
    Note that your ops must also declare that they require this resource with
    <cite>required_resource_keys</cite>, or it will not be initialized for the execution of their compute
    functions.

    You may configure this resource as follows:

        ```YAML
        resources:
          s3:
            config:
              region_name: "us-west-1"
              # Optional[str]: Specifies a custom region for the S3 session. Default is chosen
              # through the ordinary boto credential chain.
              use_unsigned_session: false
              # Optional[bool]: Specifies whether to use an unsigned S3 session. Default: True
              endpoint_url: "http://localhost"
              # Optional[str]: Specifies a custom endpoint for the S3 session. Default is None.
              profile_name: "dev"
              # Optional[str]: Specifies a custom profile for S3 session. Default is default
              # profile as specified in ~/.aws/credentials file
              use_ssl: true
              # Optional[bool]: Whether or not to use SSL. By default, SSL is used.
              verify: None
              # Optional[str]: Whether or not to verify SSL certificates. By default SSL certificates are verified.
              # You can also specify this argument if you want to use a different CA cert bundle than the one used by botocore."
              aws_access_key_id: None
              # Optional[str]: The access key to use when creating the client.
              aws_secret_access_key: None
              # Optional[str]: The secret key to use when creating the client.
              aws_session_token: None
              # Optional[str]:  The session token to use when creating the client.
        ```

    </dd>

</dl>
<dl>
    <dt><Link id='dagster_aws.s3.s3_pickle_io_manager'>dagster_aws.s3.s3_pickle_io_manager IOManagerDefinition</Link></dt>
    <dd>

        <div className='lineblock'> </div>

    Persistent IO manager using S3 for storage.

    Serializes objects via pickling. Suitable for objects storage for distributed executors, so long
    as each execution node has network connectivity and credentials for S3 and the backing bucket.

    Assigns each op output to a unique filepath containing run ID, step key, and output name.
    Assigns each asset to a single filesystem path, at “\<base_dir>/\<asset_key>”. If the asset key
    has multiple components, the final component is used as the name of the file, and the preceding
    components as parent directories under the base_dir.

    Subsequent materializations of an asset will overwrite previous materializations of that asset.
    With a base directory of “/my/base/path”, an asset with key
    <cite>AssetKey([“one”, “two”, “three”])</cite> would be stored in a file called “three” in a directory
    with path “/my/base/path/one/two/”.

    Example usage:

    1. Attach this IO manager to a set of assets.
        ```python
        from dagster import Definitions, asset
        from dagster_aws.s3 import s3_pickle_io_manager, s3_resource


        @asset
        def asset1():
            # create df ...
            return df

        @asset
        def asset2(asset1):
            return asset1[:5]

        defs = Definitions(
            assets=[asset1, asset2],
            resources={
                "io_manager": s3_pickle_io_manager.configured(
                    {"s3_bucket": "my-cool-bucket", "s3_prefix": "my-cool-prefix"}
                ),
                "s3": s3_resource,
            },
        )
        ```
    2. Attach this IO manager to your job to make it available to your ops.
        ```python
        from dagster import job
        from dagster_aws.s3 import s3_pickle_io_manager, s3_resource

        @job(
            resource_defs={
                "io_manager": s3_pickle_io_manager.configured(
                    {"s3_bucket": "my-cool-bucket", "s3_prefix": "my-cool-prefix"}
                ),
                "s3": s3_resource,
            },
        )
        def my_job():
            ...
        ```

    </dd>

</dl>
<dl>
    <dt><Link id='dagster_aws.s3.s3_file_manager'>dagster_aws.s3.s3_file_manager ResourceDefinition</Link></dt>
    <dd>

        <div className='lineblock'> </div>

    FileManager that provides abstract access to S3.

    Implements the [`FileManager`](../internals.mdx#dagster._core.storage.file_manager.FileManager) API.


    </dd>

</dl>
<dl>
    <dt><Link id='dagster_aws.redshift.redshift_resource'>dagster_aws.redshift.redshift_resource ResourceDefinition</Link></dt>
    <dd>

        <div className='lineblock'> </div>

    This resource enables connecting to a Redshift cluster and issuing queries against that
    cluster.

    Example:

        ```python
        from dagster import build_op_context, op
        from dagster_aws.redshift import redshift_resource

        @op(required_resource_keys={'redshift'})
        def example_redshift_op(context):
            return context.resources.redshift.execute_query('SELECT 1', fetch_results=True)

        redshift_configured = redshift_resource.configured({
            'host': 'my-redshift-cluster.us-east-1.redshift.amazonaws.com',
            'port': 5439,
            'user': 'dagster',
            'password': 'dagster',
            'database': 'dev',
        })
        context = build_op_context(resources={'redshift': redshift_configured})
        assert example_redshift_op(context) == [(1,)]
        ```

    </dd>

</dl>
<dl>
    <dt><Link id='dagster_aws.redshift.fake_redshift_resource'>dagster_aws.redshift.fake_redshift_resource ResourceDefinition</Link></dt>
    <dd>

        <div className='lineblock'> </div>


    </dd>

</dl>
<dl>
    <dt><Link id='dagster_aws.secretsmanager.secretsmanager_resource'>dagster_aws.secretsmanager.secretsmanager_resource ResourceDefinition</Link></dt>
    <dd>

        <div className='lineblock'> </div>

    Resource that gives access to AWS SecretsManager.

    The underlying SecretsManager session is created by calling
    `boto3.session.Session(profile_name)`.
    The returned resource object is a SecretsManager client, an instance of <cite>botocore.client.SecretsManager</cite>.

    Example:

        ```python
        from dagster import build_op_context, job, op
        from dagster_aws.secretsmanager import secretsmanager_resource

        @op(required_resource_keys={'secretsmanager'})
        def example_secretsmanager_op(context):
            return context.resources.secretsmanager.get_secret_value(
                SecretId='arn:aws:secretsmanager:region:aws_account_id:secret:appauthexample-AbCdEf'
            )

        @job(resource_defs={'secretsmanager': secretsmanager_resource})
        def example_job():
            example_secretsmanager_op()

        example_job.execute_in_process(
            run_config={
                'resources': {
                    'secretsmanager': {
                        'config': {
                            'region_name': 'us-west-1',
                        }
                    }
                }
            }
        )
        ```
    You may configure this resource as follows:

        ```YAML
        resources:
          secretsmanager:
            config:
              region_name: "us-west-1"
              # Optional[str]: Specifies a custom region for the SecretsManager session. Default is chosen
              # through the ordinary boto credential chain.
              profile_name: "dev"
              # Optional[str]: Specifies a custom profile for SecretsManager session. Default is default
              # profile as specified in ~/.aws/credentials file
        ```

    </dd>

</dl>
<dl>
    <dt><Link id='dagster_aws.secretsmanager.secretsmanager_secrets_resource'>dagster_aws.secretsmanager.secretsmanager_secrets_resource ResourceDefinition</Link></dt>
    <dd>

        <div className='lineblock'> </div>

    Resource that provides a dict which maps selected SecretsManager secrets to
    their string values. Also optionally sets chosen secrets as environment variables.

    Example:

        ```python
        import os
        from dagster import build_op_context, job, op
        from dagster_aws.secretsmanager import secretsmanager_secrets_resource

        @op(required_resource_keys={'secrets'})
        def example_secretsmanager_secrets_op(context):
            return context.resources.secrets.get("my-secret-name")

        @op(required_resource_keys={'secrets'})
        def example_secretsmanager_secrets_op_2(context):
            return os.getenv("my-other-secret-name")

        @job(resource_defs={'secrets': secretsmanager_secrets_resource})
        def example_job():
            example_secretsmanager_secrets_op()
            example_secretsmanager_secrets_op_2()

        example_job.execute_in_process(
            run_config={
                'resources': {
                    'secrets': {
                        'config': {
                            'region_name': 'us-west-1',
                            'secrets_tag': 'dagster',
                            'add_to_environment': True,
                        }
                    }
                }
            }
        )
        ```
    Note that your ops must also declare that they require this resource with
    <cite>required_resource_keys</cite>, or it will not be initialized for the execution of their compute
    functions.

    You may configure this resource as follows:

        ```YAML
        resources:
          secretsmanager:
            config:
              region_name: "us-west-1"
              # Optional[str]: Specifies a custom region for the SecretsManager session. Default is chosen
              # through the ordinary boto credential chain.
              profile_name: "dev"
              # Optional[str]: Specifies a custom profile for SecretsManager session. Default is default
              # profile as specified in ~/.aws/credentials file
              secrets: ["arn:aws:secretsmanager:region:aws_account_id:secret:appauthexample-AbCdEf"]
              # Optional[List[str]]: Specifies a list of secret ARNs to pull from SecretsManager.
              secrets_tag: "dagster"
              # Optional[str]: Specifies a tag, all secrets which have the tag set will be pulled
              # from SecretsManager.
              add_to_environment: true
              # Optional[bool]: Whether to set the selected secrets as environment variables. Defaults
              # to false.
        ```

    </dd>

</dl>
</div></div>
