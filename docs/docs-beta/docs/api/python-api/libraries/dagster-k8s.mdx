---
title: 'kubernetes (dagster-k8s)'
title_meta: 'kubernetes (dagster-k8s) API Documentation - Build Better Data Pipelines | Python Reference Documentation for Dagster'
description: 'kubernetes (dagster-k8s) Dagster API | Comprehensive Python API documentation for Dagster, the data orchestration platform. Learn how to build, test, and maintain data pipelines with our detailed guides and examples.'
last_update:
  date: '2025-02-03'
---

<div class="section" id="kubernetes-dagster-k8s">


# Kubernetes (dagster-k8s)

See also the [Kubernetes deployment guide](https://docs.dagster.io/deploying/kubernetes/).

This library contains utilities for running Dagster with Kubernetes. This includes a Python API
allowing the webserver to launch runs as Kubernetes Jobs, as well as a Helm chart you can use as the basis
for a Dagster deployment on a Kubernetes cluster.

</div>


<div class="section" id="apis">


# APIs

<dl>
    <dt><Link id='dagster_k8s.K8sRunLauncher'>dagster_k8s.K8sRunLauncher RunLauncher</Link></dt>
    <dd>

        <div className='lineblock'> </div>

    RunLauncher that starts a Kubernetes Job for each Dagster job run.

    Encapsulates each run in a separate, isolated invocation of `dagster-graphql`.

    You can configure a Dagster instance to use this RunLauncher by adding a section to your
    `dagster.yaml` like the following:

        ```yaml
        run_launcher:
          module: dagster_k8s.launcher
          class: K8sRunLauncher
          config:
            service_account_name: your_service_account
            job_image: my_project/dagster_image:latest
            instance_config_map: dagster-instance
            postgres_password_secret: dagster-postgresql-secret
        ```

    </dd>

</dl>
<dl>
    <dt><Link id='dagster_k8s.k8s_job_executor'>dagster_k8s.k8s_job_executor ExecutorDefinition</Link></dt>
    <dd>

        <div className='lineblock'> </div>

    Executor which launches steps as Kubernetes Jobs.

    To use the <cite>k8s_job_executor</cite>, set it as the <cite>executor_def</cite> when defining a job:

        ```python
        from dagster_k8s import k8s_job_executor

        from dagster import job

        @job(executor_def=k8s_job_executor)
        def k8s_job():
            pass
        ```
    Then you can configure the executor with run config as follows:

        ```YAML
        execution:
          config:
            job_namespace: 'some-namespace'
            image_pull_policy: ...
            image_pull_secrets: ...
            service_account_name: ...
            env_config_maps: ...
            env_secrets: ...
            env_vars: ...
            job_image: ... # leave out if using userDeployments
            max_concurrent: ...
        ```
    <cite>max_concurrent</cite> limits the number of pods that will execute concurrently for one run. By default
    there is no limit- it will maximally parallel as allowed by the DAG. Note that this is not a
    global limit.

    Configuration set on the Kubernetes Jobs and Pods created by the <cite>K8sRunLauncher</cite> will also be
    set on Kubernetes Jobs and Pods created by the <cite>k8s_job_executor</cite>.

    Configuration set using <cite>tags</cite> on a <cite>@job</cite> will only apply to the <cite>run</cite> level. For configuration
    to apply at each <cite>step</cite> it must be set using <cite>tags</cite> for each <cite>@op</cite>.


    </dd>

</dl>
<div class="section" id="ops">


## Ops

<dl>
    <dt><Link id='dagster_k8s.k8s_job_op'>dagster_k8s.k8s_job_op `=` \<dagster._core.definitions.op_definition.OpDefinition object></Link></dt>
    <dd>

        <div className='lineblock'> </div>

        :::warning[experimental]
        This API may break in future versions, even between dot releases.


        :::

    An op that runs a Kubernetes job using the k8s API.

    Contrast with the <cite>k8s_job_executor</cite>, which runs each Dagster op in a Dagster job in its
    own k8s job.

    This op may be useful when:
          - You need to orchestrate a command that isn’t a Dagster op (or isn’t written in Python)
          - You want to run the rest of a Dagster job using a specific executor, and only a single


    For example:

        ```python
        from dagster_k8s import k8s_job_op

        from dagster import job

        first_op = k8s_job_op.configured(
            {
                "image": "busybox",
                "command": ["/bin/sh", "-c"],
                "args": ["echo HELLO"],
            },
            name="first_op",
        )
        second_op = k8s_job_op.configured(
            {
                "image": "busybox",
                "command": ["/bin/sh", "-c"],
                "args": ["echo GOODBYE"],
            },
            name="second_op",
        )

        @job
        def full_job():
            second_op(first_op())
        ```
    You can create your own op with the same implementation by calling the <cite>execute_k8s_job</cite> function
    inside your own op.

    The service account that is used to run this job should have the following RBAC permissions:

        ```YAML
        rules:
          - apiGroups: ["batch"]
              resources: ["jobs", "jobs/status"]
              verbs: ["*"]
          # The empty arg "" corresponds to the core API group
          - apiGroups: [""]
              resources: ["pods", "pods/log", "pods/status"]
              verbs: ["*"]'
        ```

    </dd>

</dl>
<dl>
    <dt><Link id='dagster_k8s.execute_k8s_job'>dagster_k8s.execute_k8s_job</Link></dt>
    <dd>

        :::warning[experimental]
        This API may break in future versions, even between dot releases.


        :::

    This function is a utility for executing a Kubernetes job from within a Dagster op.

    Parameters: 
      - <strong>image</strong> (<em>str</em>) – The image in which to launch the k8s job.
      - <strong>command</strong> (<em>Optional</em><em>[</em><em>List</em><em>[</em><em>str</em><em>]</em><em>]</em>) – The command to run in the container within the launched
      - <strong>args</strong> (<em>Optional</em><em>[</em><em>List</em><em>[</em><em>str</em><em>]</em><em>]</em>) – The args for the command for the container. Default: None.
      - <strong>namespace</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) – Override the kubernetes namespace in which to run the k8s job.
      - <strong>image_pull_policy</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) – Allows the image pull policy to be overridden, e.g. to
      - <strong>image_pull_secrets</strong> (<em>Optional</em><em>[</em><em>List</em><em>[</em><em>Dict</em><em>[</em><em>str</em><em>, </em><em>str</em><em>]</em><em>]</em><em>]</em>) – Optionally, a list of dicts, each of
      - <strong>service_account_name</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) – The name of the Kubernetes service account under which
      - <strong>env_secrets</strong> (<em>Optional</em><em>[</em><em>List</em><em>[</em><em>str</em><em>]</em><em>]</em>) – A list of custom Secret names from which to
      - <strong>env_vars</strong> (<em>Optional</em><em>[</em><em>List</em><em>[</em><em>str</em><em>]</em><em>]</em>) – A list of environment variables to inject into the Job.
      - <strong>volume_mounts</strong> (<em>Optional</em><em>[</em><em>List</em><em>[</em>[*Permissive*](../config.mdx#dagster.Permissive)<em>]</em><em>]</em>) – A list of volume mounts to include in the job’s
      - <strong>volumes</strong> (<em>Optional</em><em>[</em><em>List</em><em>[</em>[*Permissive*](../config.mdx#dagster.Permissive)<em>]</em><em>]</em>) – A list of volumes to include in the Job’s Pod. Default: `[]`. See:
      - <strong>labels</strong> (<em>Optional</em><em>[</em><em>Dict</em><em>[</em><em>str</em><em>, </em><em>str</em><em>]</em><em>]</em>) – Additional labels that should be included in the Job’s Pod. See:
      - <strong>resources</strong> (<em>Optional</em><em>[</em><em>Dict</em><em>[</em><em>str</em><em>, </em><em>Any</em><em>]</em><em>]</em>) – [https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/](https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/)
      - <strong>scheduler_name</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) – Use a custom Kubernetes scheduler for launched Pods. See:
      - <strong>load_incluster_config</strong> (<em>bool</em>) – Whether the op is running within a k8s cluster. If `True`,
      - <strong>kubeconfig_file</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) – The kubeconfig file from which to load config. Defaults to
      - <strong>timeout</strong> (<em>Optional</em><em>[</em><em>int</em><em>]</em>) – Raise an exception if the op takes longer than this timeout in
      - <strong>container_config</strong> (<em>Optional</em><em>[</em><em>Dict</em><em>[</em><em>str</em><em>, </em><em>Any</em><em>]</em><em>]</em>) – Raw k8s config for the k8s pod’s main container
      - <strong>pod_template_spec_metadata</strong> (<em>Optional</em><em>[</em><em>Dict</em><em>[</em><em>str</em><em>, </em><em>Any</em><em>]</em><em>]</em>) – Raw k8s config for the k8s pod’s
      - <strong>pod_spec_config</strong> (<em>Optional</em><em>[</em><em>Dict</em><em>[</em><em>str</em><em>, </em><em>Any</em><em>]</em><em>]</em>) – Raw k8s config for the k8s pod’s pod spec
      - <strong>job_metadata</strong> (<em>Optional</em><em>[</em><em>Dict</em><em>[</em><em>str</em><em>, </em><em>Any</em><em>]</em><em>]</em>) – Raw k8s config for the k8s job’s metadata
      - <strong>job_spec_config</strong> (<em>Optional</em><em>[</em><em>Dict</em><em>[</em><em>str</em><em>, </em><em>Any</em><em>]</em><em>]</em>) – Raw k8s config for the k8s job’s job spec
      - <strong>k8s_job_name</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) – Overrides the name of the k8s job. If not set, will be set
      - <strong>merge_behavior</strong> (<em>Optional</em><em>[</em><em>K8sConfigMergeBehavior</em><em>]</em>) – How raw k8s config set on this op should
      - <strong>delete_failed_k8s_jobs</strong> (<em>bool</em>) – Whether to immediately delete failed Kubernetes jobs. If False,



    </dd>

</dl>
<div class="section" id="python-api">


### Python API

The `K8sRunLauncher` allows webserver instances to be configured to launch new runs by starting
per-run Kubernetes Jobs. To configure the `K8sRunLauncher`, your `dagster.yaml` should
include a section like:

    ```yaml
    run_launcher:
      module: dagster_k8s.launcher
      class: K8sRunLauncher
      config:
        image_pull_secrets:
        service_account_name: dagster
        job_image: "my-company.com/image:latest"
        dagster_home: "/opt/dagster/dagster_home"
        postgres_password_secret: "dagster-postgresql-secret"
        image_pull_policy: "IfNotPresent"
        job_namespace: "dagster"
        instance_config_map: "dagster-instance"
        env_config_maps:
          - "dagster-k8s-job-runner-env"
        env_secrets:
          - "dagster-k8s-some-secret"
    ```
</div>


<div class="section" id="helm-chart">


### Helm chart

For local dev (e.g., on kind or minikube):

    ```shell
    helm install \
        --set dagsterWebserver.image.repository="dagster.io/buildkite-test-image" \
        --set dagsterWebserver.image.tag="py310-latest" \
        --set job_runner.image.repository="dagster.io/buildkite-test-image" \
        --set job_runner.image.tag="py310-latest" \
        --set imagePullPolicy="IfNotPresent" \
        dagster \
        helm/dagster/
    ```
Upon installation, the Helm chart will provide instructions for port forwarding
the Dagster webserver and Flower (if configured).

</div>


<div class="section" id="running-tests">


### Running tests

To run the unit tests:

    ```default
    pytest -m "not integration"
    ```
To run the integration tests, you must have [Docker](https://docs.docker.com/install/),
[kind](https://kind.sigs.k8s.io/docs/user/quick-start#installation),
and [helm](https://helm.sh/docs/intro/install/) installed.

On macOS:

    ```default
    brew install kind
    brew install helm
    ```
Docker must be running.

You may experience slow first test runs thanks to image pulls (run `pytest -svv --fulltrace` for
visibility). Building images and loading them to the kind cluster is slow, and there is
no visibility into the progress of the load.

<strong>NOTE:</strong> This process is quite slow, as it requires bootstrapping a local `kind` cluster with
Docker images and the `dagster-k8s` Helm chart. For faster development, you can either:

1. Keep a warm kind cluster
2. Use a remote K8s cluster, e.g. via AWS EKS or GCP GKE
Instructions are below.

<div class="section" id="faster-local-development-with-kind">


#### Faster local development (with kind)

You may find that the kind cluster creation, image loading, and kind cluster creation loop
is too slow for effective local dev.

You may bypass cluster creation and image loading in the following way. First add the `--no-cleanup`
flag to your pytest invocation:

    ```shell
    pytest --no-cleanup -s -vvv -m "not integration"
    ```
The tests will run as before, but the kind cluster will be left running after the tests are completed.

For subsequent test runs, you can run:

    ```shell
    pytest --kind-cluster="cluster-d9971c84d44d47f382a2928c8c161faa" --existing-helm-namespace="dagster-test-95590a" -s -vvv -m "not integration"
    ```
This will bypass cluster creation, image loading, and Helm chart installation, for much faster tests.

The kind cluster name and Helm namespace for this command can be found in the logs, or retrieved
via the respective CLIs, using `kind get clusters` and `kubectl get namespaces`. Note that
for `kubectl` and `helm` to work correctly with a kind cluster, you should override your
kubeconfig file location with:

    ```shell
    kind get kubeconfig --name kind-test > /tmp/kubeconfig
    export KUBECONFIG=/tmp/kubeconfig
    ```
</div>


<div class="section" id="manual-kind-cluster-setup">


#### Manual kind cluster setup

The test fixtures provided by `dagster-k8s` automate the process described below, but sometimes
it’s useful to manually configure a kind cluster and load images onto it.

First, ensure you have a Docker image appropriate for your Python version. Run, from the root of
the repo:

    ```shell
    ./python_modules/dagster-test/dagster_test/test_project/build.sh 3.7.6 \
        dagster.io.priv/buildkite-test-image:py310-latest
    ```
In the above invocation, the Python majmin version should be appropriate for your desired tests.

Then run the following commands to create the cluster and load the image. Note that there is no
feedback from the loading process.

    ```shell
    kind create cluster --name kind-test
    kind load docker-image --name kind-test dagster.io/dagster-docker-buildkite:py310-latest
    ```
If you are deploying the Helm chart with an in-cluster Postgres (rather than an external database),
and/or with dagster-celery workers (and a RabbitMQ), you’ll also want to have images present for
rabbitmq and postgresql:

    ```shell
    docker pull docker.io/bitnami/rabbitmq
    docker pull docker.io/bitnami/postgresql

    kind load docker-image --name kind-test docker.io/bitnami/rabbitmq:latest
    kind load docker-image --name kind-test docker.io/bitnami/postgresql:latest
    ```
Then you can run pytest as follows:

    ```shell
    pytest --kind-cluster=kind-test
    ```
</div></div>


<div class="section" id="faster-local-development-with-an-existing-k8s-cluster">


### Faster local development (with an existing K8s cluster)

If you already have a development K8s cluster available, you can run tests on that cluster vs.
running locally in `kind`.

For this to work, first build and deploy the test image to a registry available to your cluster.
For example, with a private ECR repository:

    ```default
    ./python_modules/dagster-test/dagster_test/test_project/build.sh 3.7.6
    docker tag dagster-docker-buildkite:latest $AWS_ACCOUNT_ID.dkr.ecr.us-west-2.amazonaws.com/dagster-k8s-tests:2020-04-21T21-04-06

    aws ecr get-login --no-include-email --region us-west-1 | sh
    docker push $AWS_ACCOUNT_ID.dkr.ecr.us-west-1.amazonaws.com/dagster-k8s-tests:2020-04-21T21-04-06
    ```
Then, you can run tests on EKS with:

    ```default
    export DAGSTER_DOCKER_IMAGE_TAG="2020-04-21T21-04-06"
    export DAGSTER_DOCKER_REPOSITORY="$AWS_ACCOUNT_ID.dkr.ecr.us-west-2.amazonaws.com"
    export DAGSTER_DOCKER_IMAGE="dagster-k8s-tests"

    # First run with --no-cleanup to leave Helm chart in place
    pytest --cluster-provider="kubeconfig" --no-cleanup -s -vvv

    # Subsequent runs against existing Helm chart
    pytest --cluster-provider="kubeconfig" --existing-helm-namespace="dagster-test-<some id>" -s -vvv
    ```
</div>


<div class="section" id="validating-helm-charts">


### Validating Helm charts

To test / validate Helm charts, you can run:

    ```shell
    helm install dagster --dry-run --debug helm/dagster
    helm lint
    ```
</div>


<div class="section" id="enabling-gcr-access-from-minikube">


### Enabling GCR access from Minikube

To enable GCR access from Minikube:

    ```shell
    kubectl create secret docker-registry element-dev-key \
        --docker-server=https://gcr.io \
        --docker-username=oauth2accesstoken \
        --docker-password="$(gcloud auth print-access-token)" \
        --docker-email=my@email.com
    ```
</div>


<div class="section" id="a-note-about-pvcs">


### A note about PVCs

Both the Postgres and the RabbitMQ Helm charts will store credentials using Persistent Volume
Claims, which will outlive test invocations and calls to `helm uninstall`. These must be deleted if
you want to change credentials. To view your pvcs, run:

    ```default
    kubectl get pvc
    ```
</div>


<div class="section" id="testing-redis">


### Testing Redis

The Redis Helm chart installs w/ a randomly-generated password by default; turn this off:

    ```default
    helm install dagredis stable/redis --set usePassword=false
    ```
Then, to connect to your database from outside the cluster execute the following commands:

    ```default
    kubectl port-forward --namespace default svc/dagredis-master 6379:6379
    redis-cli -h 127.0.0.1 -p 6379
    ```
</div></div>


<div class="section" id="pipes">

## Pipes

<dl>
    <dt><Link id='dagster_k8s.PipesK8sClient'>class dagster_k8s.PipesK8sClient</Link></dt>
    <dd>

    A pipes client for launching kubernetes pods.

    By default context is injected via environment variables and messages are parsed out of
    the pod logs, with other logs forwarded to stdout of the orchestration process.

    The first container within the containers list of the pod spec is expected (or set) to be
    the container prepared for pipes protocol communication.

    Parameters: 
      - <strong>env</strong> (<em>Optional</em><em>[</em><em>Mapping</em><em>[</em><em>str</em><em>, </em><em>str</em><em>]</em><em>]</em>) – An optional dict of environment variables to pass to the
      - <strong>context_injector</strong> (<em>Optional</em><em>[</em>[*PipesContextInjector*](../pipes.mdx#dagster.PipesContextInjector)<em>]</em>) – A context injector to use to inject
      - <strong>message_reader</strong> (<em>Optional</em><em>[</em>[*PipesMessageReader*](../pipes.mdx#dagster.PipesMessageReader)<em>]</em>) – A message reader to use to read messages
      - <strong>load_incluster_config</strong> (<em>Optional</em><em>[</em><em>bool</em><em>]</em>) – Whether this client is expected to be running from inside
      - <strong>kubeconfig_file</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) – The value to pass as the config_file argument to
      - <strong>kube_context</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) – The value to pass as the context argument to
      - <strong>poll_interval</strong> (<em>Optional</em><em>[</em><em>float</em><em>]</em>) – How many seconds to wait between requests when


    <dl>
        <dt><Link id='dagster_k8s.PipesK8sClient.run'>run</Link></dt>
        <dd>

        Publish a kubernetes pod and wait for it to complete, enriched with the pipes protocol.

        Parameters: 
          - <strong>context</strong> (<em>Union</em><em>[</em>[*OpExecutionContext*](../execution.mdx#dagster.OpExecutionContext)<em>, </em>[*AssetExecutionContext*](../execution.mdx#dagster.AssetExecutionContext)<em>]</em>) – The execution context.
          - <strong>image</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) – The image to set the first container in the pod spec to use.
          - <strong>command</strong> (<em>Optional</em><em>[</em><em>Union</em><em>[</em><em>str</em><em>, </em><em>Sequence</em><em>[</em><em>str</em><em>]</em><em>]</em><em>]</em>) – The command to set the first container in the pod spec to use.
          - <strong>namespace</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) – Which kubernetes namespace to use, defaults to the current namespace if
          - <strong>env</strong> (<em>Optional</em><em>[</em><em>Mapping</em><em>[</em><em>str</em><em>,</em><em>str</em><em>]</em><em>]</em>) – A mapping of environment variable names to values to set on the first
          - <strong>base_pod_meta</strong> (<em>Optional</em><em>[</em><em>Mapping</em><em>[</em><em>str</em><em>, </em><em>Any</em><em>]</em><em>]</em>) – Raw k8s config for the k8s pod’s metadata
          - <strong>base_pod_spec</strong> (<em>Optional</em><em>[</em><em>Mapping</em><em>[</em><em>str</em><em>, </em><em>Any</em><em>]</em><em>]</em>) – Raw k8s config for the k8s pod’s pod spec
          - <strong>extras</strong> (<em>Optional</em><em>[</em><em>PipesExtras</em><em>]</em>) – Extra values to pass along as part of the ext protocol.
          - <strong>context_injector</strong> (<em>Optional</em><em>[</em>[*PipesContextInjector*](../pipes.mdx#dagster.PipesContextInjector)<em>]</em>) – Override the default ext protocol context injection.
          - <strong>message_reader</strong> (<em>Optional</em><em>[</em>[*PipesMessageReader*](../pipes.mdx#dagster.PipesMessageReader)<em>]</em>) – Override the default ext protocol message reader.
          - <strong>ignore_containers</strong> (<em>Optional</em><em>[</em><em>Set</em><em>]</em>) – Ignore certain containers from waiting for termination. Defaults to
          - <strong>enable_multi_container_logs</strong> (<em>bool</em>) – Whether or not to enable multi-container log consumption.


        Returns: 
        Wrapper containing results reported by the external
            process.

        Return type: PipesClientCompletedInvocation

        </dd>

    </dl>

    </dd>

</dl>
<dl>
    <dt><Link id='dagster_k8s.PipesK8sPodLogsMessageReader'>class dagster_k8s.PipesK8sPodLogsMessageReader</Link></dt>
    <dd>
    Message reader that reads messages from kubernetes pod logs.
    </dd>

</dl>
</div></div>
