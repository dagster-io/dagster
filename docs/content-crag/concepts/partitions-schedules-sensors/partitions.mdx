---
title: Partitions | Dagster
description: Dagster provides the Partition Set abstraction for pipelines where each run deals with a subset of data.
---

# Partitions

Dagster provides the _Partition Set_ abstraction for pipelines where each run deals with a subset of data.

## Relevant APIs

| Name                                         | Description                                                                                         |
| -------------------------------------------- | --------------------------------------------------------------------------------------------------- |
| <PyObject object="Partition" />              | Class defining a logical slice of pipeline configuration.                                           |
| <PyObject object="PartitionSetDefinition" /> | Class defining a set of <PyObject object="Partition" /> objects.                                    |
| <PyObject object="validate_run_config" />    | A function used to verify that a run config dictionary is valid against a pipeline's config schema. |

## Overview

Partition sets let you define a set of logical "partitions", usually time windows, along with a scheme for building pipeline run config for a partition. Having defined a partition set, you can kick off a pipeline run or set of pipeline runs by simply selecting partitions in the set.

Partitions have two main uses:

- Partitioned [Schedules](/concepts/partitions-schedules-sensors/schedules): You can construct a schedule that targets a single partition for each run it launches. For example, a pipeline might run each day and process the data that arrived during the previous day.
- [Backfills](/concepts/partitions-schedules-sensors/backfills): You can launch a set of pipeline runs all at once, each run targeting one of the partitions in the set. For example, after making a code change, you might want to re-run your pipeline on every date that it has run on in the past.

---

## Defining a Partition Set

Here's a pipeline that computes some data for a given date.

```python file=/concepts/partitions_schedules_sensors/pipeline.py startafter=start_pipeline_marker_1 endbefore=end_pipeline_marker_1
@solid(config_schema={"date": str})
def process_data_for_date(context):
    date = context.solid_config["date"]
    context.log.info(f"processing data for {date}")


@solid
def post_slack_message(context):
    context.log.info("posting slack message")


@pipeline
def my_data_pipeline():
    process_data_for_date()
    post_slack_message()
```

The solid `process_data_for_date` takes, as config, a string `date`. This piece of config will define which date to compute data for. For example, if we wanted to compute for May 5th, 2020, we would execute the pipeline with the following config:

```python file=/concepts/partitions_schedules_sensors/config.yaml
solids:
  process_data_for_date:
    config:
      date: "2020-05-05"
```

You can define a <PyObject module="dagster" object="PartitionSetDefinition"/> that defines the full set of partitions and how to define the run config for a given partition.

```python file=/concepts/partitions_schedules_sensors/partition_definition.py startafter=start_def endbefore=end_def
def get_date_partitions():
    """Every day in the month of May, 2020"""
    return [Partition(f"2020-05-{str(day).zfill(2)}") for day in range(1, 32)]


def run_config_for_date_partition(partition):
    date = partition.value
    return {"solids": {"process_data_for_date": {"config": {"date": date}}}}


date_partition_set = PartitionSetDefinition(
    name="date_partition_set",
    pipeline_name="my_data_pipeline",
    partition_fn=get_date_partitions,
    run_config_fn_for_partition=run_config_for_date_partition,
)
```

To add the partition set to the repository and view it in Dagit, you must first include it in the repository definition.

```python file=/concepts/partitions_schedules_sensors/partition_definition.py startafter=start_repo_includ endbefore=end_repo_include
@repository
def my_repository():
    return [
        my_data_pipeline,
        date_partition_set,
    ]
```

## Creating Schedules from Partition Sets

To create a schedule from a partition set, you must first define a partition selector. A partition selector is a function which takes a <PyObject object="ScheduleEvaluationContext" /> and a <PyObject object="PartitionSetDefinition" /> and returns a <PyObject object="Partition" /> object or a list of <PyObject object="Partition" /> objects. Every time the schedule fires, a pipeline run will be submitted for each partition returned by this partition selector.

Once you have created your partition selector, you can manually create a schedule using the method <PyObject object="PartitionSetDefinition" method="create_schedule_definition" />.

In the following example, we first define a static partition set with partitions representing days of the week. We then create a partition selector, which maps the schedule execution time to the appropriate partition. Finally, we create a schedule that kicks off a pipeline run for the selected partition.

```python file=/concepts/partitions_schedules_sensors/partition_definition.py startafter=start_manual_partition_schedule endbefore=end_manual_partition_schedule
weekday_partition_set = PartitionSetDefinition(
    name="weekday_partition_set",
    pipeline_name="my_data_pipeline",
    partition_fn=lambda: [
        Partition("Monday"),
        Partition("Tuesday"),
        Partition("Wednesday"),
        Partition("Thursday"),
        Partition("Friday"),
        Partition("Saturday"),
        Partition("Sunday"),
    ],
    run_config_fn_for_partition=_weekday_run_config_for_partition,
)


def weekday_partition_selector(
    ctx: ScheduleEvaluationContext, partition_set: PartitionSetDefinition
) -> Union[Partition, List[Partition]]:
    """Maps a schedule execution time to the corresponding partition or list of partitions that
    should be executed at that time"""
    partitions = partition_set.get_partitions(ctx.scheduled_execution_time)
    weekday = ctx.scheduled_execution_time.weekday() if ctx.scheduled_execution_time else 0
    return partitions[weekday]


my_schedule = weekday_partition_set.create_schedule_definition(
    "my_schedule",
    "5 0 * * *",
    partition_selector=weekday_partition_selector,
    execution_timezone="US/Eastern",
)


@repository
def my_repository_with_partitioned_schedule():
    return [
        my_data_pipeline,
        weekday_partition_set,
        my_schedule,
    ]
```

For partition sets consisting of `datetime` objects, the <PyObject object="create_offset_partition_selector" /> is a useful factory function for selecting partitions derived from the execution time. For example, you might want to fire your schedule every night and select the previous day's partition. The <PyObject object="identity_partition_selector" /> is also a useful partition selector, where the selected partition's value is the schedule execution time.

Dagster provides a number of decorators (<PyObject object="hourly_schedule" decorator />, <PyObject object="daily_schedule" decorator />, <PyObject object="weekly_schedule" decorator />, <PyObject object="monthly_schedule" decorator />) that produce [partitioned schedules](/concepts/partitions-schedules-sensors/schedules#partition-based-schedules). These schedule decorators construct both the partition set that defines the configuration space of a specific pipeline as well as the schedule that it runs against. See our [Schedules overview](/concepts/partitions-schedules-sensors/schedules) for more information.

## Testing a Partition Set

You can test a partition set by initializing the set of partitions, and then evaluating the run config produced by your partition set against your pipeline using the <PyObject object="validate_run_config" /> function.

```python file=/concepts/partitions_schedules_sensors/partition_definition.py startafter=start_test_partition_set endbefore=end_test_partition_set
from dagster import validate_run_config


def test_my_partition_set():
    for partition in date_partition_set.get_partitions():
        run_config = date_partition_set.run_config_for_partition(partition)
        assert validate_run_config(my_data_pipeline, run_config)
```

## Partitions in Dagit

### The Partitions Tab

In Dagit, you can view runs by partition in the Partitions tab of a Pipeline page.

In the "Run Matrix", each column corresponds to one of the partitions in the partition set. Each row corresponds to one of the steps in the pipeline.

<!-- This was generated with:
    * `dagit -f repo.py` inside docs_snippet/concepts/partitions_schedules_sensors/ directory
    * Navigating to the partitions page for `my_data_pipeline`
-->

<Image
alt="Partitions Tab"
src="/images/concepts/partitions-schedules-sensors/partitions-page.png"
width={3808}
height={2414}
/>

You can click on individual boxes to see the history of runs for that step and partition.

<Image
alt="Partition Step Modal"
src="/images/concepts/partitions-schedules-sensors/partitions-step-modal.png"
width={3808}
height={2414}
/>

### Launching Partitioned Runs from the Playground

You can view and use partitions in the Dagit playground view for a pipeline. In the top bar, you can select from the list of all available partition sets, then choose a specific partition. Within the config editor, the config for the selected partition will be populated.

In the screenshot below, we select the `date_partition_set` and the `2020-05-01` partition, and we can see that the correct run config for the partition has been populated in the editor.

<Image
alt="Partitions in Dagit Playground"
src="/images/concepts/partitions-schedules-sensors/partitions-playground.png"
width={3808}
height={2414}
/>
