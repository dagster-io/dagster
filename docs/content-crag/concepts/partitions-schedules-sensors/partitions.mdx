---
title: Partitioned Jobs | Dagster
description: Dagster "Partitioned Jobs" enable launching backfills, where each partition processes a subset of data.
---

# Partitioned Jobs

## Relevant APIs

| Name                                                       | Description                                                                                         |
| ---------------------------------------------------------- | --------------------------------------------------------------------------------------------------- |
| <PyObject object="PartitionedConfig" />                    | Determines a set of partitions and how to generate run config for a partition.                      |
| <PyObject object="daily_partitioned_config" decorator />   | Decorator for constructing partitioned config where each partition is a date.                       |
| <PyObject object="hourly_partitioned_config" decorator />  | Decorator for constructing partitioned config where each partition is an hour of a date.            |
| <PyObject object="weekly_partitioned_config" decorator />  | Decorator for constructing partitioned config where each partition is a week.                       |
| <PyObject object="monthly_partitioned_config" decorator /> | Decorator for constructing partitioned config where each partition is a month.                      |
| <PyObject object="static_partitioned_config" decorator />  | Decorator for constructing partitioned config for a static set of partition keys.                   |
| <PyObject object="dynamic_partitioned_config" decorator /> | Decorator for constructing partitioned config for a set of partition keys that can grow over time.  |
| <PyObject object="build_schedule_from_partitioned_job" />  | A function that constructs a schedule whose interval matches the partitioning of a partitioned job. |

## Overview

A _partitioned job_ is a job where each run corresponds to a "partition". The choice of partition determines the run's config. Most commonly, each partition is a time window, so, when a job executes, it processes data within one of the time windows.

Having defined a partitioned job, you can:

- View runs by partition in Dagit.
- Define a [schedule](/concepts/partitions-schedules-sensors/schedules) that fills in a partition each time it runs. For example, a job might run each day and process the data that arrived during the previous day.
- Launch [backfills](/concepts/partitions-schedules-sensors/backfills), which are sets of runs that each process a different partition. For example, after making a code change, you might want to re-run your job on every date that it has run on in the past.

---

## Defining Partitioned Jobs

You define a partitioned job by constructing a <PyObject object="PartitionedConfig" /> object and supplying it when you construct your job.

### Defining a Job with Time Partitions

The most common kind of partitioned job is a time-partitioned job - each partition is a time window, and each run for a partition processes data within that time window.

#### Non-Partitioned Job with Date Config

Before we define a partitioned job, let's look at a non-partitioned job that computes some data for a given date:

```python file=/concepts/partitions_schedules_sensors/date_config_job.py
from dagster import job, op


@op(config_schema={"date": str})
def process_data_for_date(context):
    date = context.op_config["date"]
    context.log.info(f"processing data for {date}")


@job
def do_stuff():
    process_data_for_date()
```

It takes, as config, a string `date`. This piece of config defines which date to compute data for. For example, if you wanted to compute for May 5th, 2020, you would execute the graph with the following config:

```python file=/concepts/partitions_schedules_sensors/config.yaml
graph:
  process_data_for_date:
    config:
      date: "2020-05-05"
```

#### Date-Partitioned Job

With the job above, it's possible to supply any value for the `date` param, which means that, if you wanted to launch a backfill, Dagster wouldn't know what values to run it on. You can instead build a partitioned job that operates on a defined set of dates.

First, you define the <PyObject object="PartitionedConfig"/>. In this case, because each partition is a date, you can use the <PyObject object="daily_partitioned_config" decorator /> decorator. It defines the full set of partitions - every date between the start date and the current date, as well as how to determine the run config for a given partition.

```python file=/concepts/partitions_schedules_sensors/partitioned_job.py startafter=start_partitioned_config endbefore=end_partitioned_config
from dagster import daily_partitioned_config
from datetime import datetime


@daily_partitioned_config(start_date=datetime(2020, 1, 1))
def my_partitioned_config(start: datetime, _end: datetime):
    return {"ops": {"process_data_for_date": {"config": {"date": start.strftime("%Y-%m-%d")}}}}
```

Then you can build a job that uses the `PartitionedConfig` by supplying it to the `config` argument when you construct the job:

```python file=/concepts/partitions_schedules_sensors/partitioned_job.py startafter=start_partitioned_job endbefore=end_partitioned_job
@job(config=my_partitioned_config)
def do_stuff_partitioned():
    process_data_for_date()
```

In addition to the <PyObject object="daily_partitioned_config" decorator /> decorator, Dagster also provides <PyObject object="monthly_partitioned_config" decorator />, <PyObject object="weekly_partitioned_config" decorator />, <PyObject object="hourly_partitioned_config" decorator />.

### Defining a Job with Static Partitions

Not all jobs are partitioned by time. Here's a partitioned job where the partitions are continents:

```python file=/concepts/partitions_schedules_sensors/static_partitioned_job.py
from dagster import job, op, static_partitioned_config

CONTINENTS = ["Africa", "Antarctica", "Asia", "Europe", "North America", "Oceania", "South America"]


@static_partitioned_config(partition_keys=CONTINENTS)
def continent_config(partition_key: str):
    return {"ops": {"continent_op": {"config": {"continent_name": partition_key}}}}


@op
def continent_op(context):
    context.log.info(context.op_config["continent_name"])


@job(config=continent_config)
def continent_job():
    continent_op()
```

## Creating Schedules from Partitioned Jobs

It's common that, when you have a partitioned job, you want to run it on a schedule. For example, if your job has a partition for each date, you likely want to run that job every day, on the partition for that day.

The <PyObject object="build_schedule_from_partitioned_job"/> function allows you to construct a schedule from a partitioned job. The [Schedules concept page](/concepts/partitions-schedules-sensors/schedules#a-schedule-from-a-partitioned-job) describes how to use it.

## Testing

### Testing Partitioned Config

You see what config a <PyObject object="PartitionedConfig" /> will generate for a partition by invoking its <PyObject object="PartitionedConfig" method="get_run_config" /> method.

If you want to check whether the generated run config is valid for the config of job, you can use the <PyObject object="validate_run_config" /> function.

```python file=/concepts/partitions_schedules_sensors/partitioned_config_test.py startafter=start endbefore=end
from dagster import validate_run_config


def test_my_partitioned_config():
    first_partition_key = my_partitioned_config.get_partition_keys()[0]
    run_config = my_partitioned_config.get_run_config(first_partition_key)
    assert run_config == {"ops": {"process_data_for_date": {"config": {"date": "2020-01-01"}}}}
    assert validate_run_config(do_stuff_partitioned, run_config)
```

### Testing Partitioned Jobs

To run a partitioned job in-process on a particular partition, you can supply a value for the `partition_key` argument of <PyObject object="JobDefinition" method="execute_in_process" />

```python file=/concepts/partitions_schedules_sensors/partitioned_job_test.py startafter=start endbefore=end
def test_do_stuff_partitioned():
    assert do_stuff_partitioned.execute_in_process(partition_key="2020-01-01").success
```

## Partitions in Dagit

### The Partitions Tab

In Dagit, you can view runs by partition in the Partitions tab of a Job page.

In the "Run Matrix", each column corresponds to one of the partitions in the job. Each row corresponds to one of the steps in the job.

<!-- This was generated from go/prod -->

<Image
alt="Partitions Tab"
src="/images/concepts/partitions-schedules-sensors/partitions-page.png"
width={3808}
height={2414}
/>

You can click on individual boxes to see the history of runs for that step and partition.

<Image
alt="Partition Step Modal"
src="/images/concepts/partitions-schedules-sensors/partitions-step-modal.png"
width={3808}
height={2414}
/>

### Launching Partitioned Runs from the Playground

You can view and use partitions in the Dagit Playground tab for a job. In the top bar, you can select from the list of all available partitions. Within the config editor, the config for the selected partition will be populated.

In the screenshot below, we select the `2020-05-01` partition, and we can see that the run config for the partition has been populated in the editor.

<Image
alt="Partitions in Dagit Playground"
src="/images/concepts/partitions-schedules-sensors/partitions-playground.png"
width={3808}
height={2414}
/>
