import PyObject from 'components/PyObject';
import { DynamicMetaTags } from 'components/MetaTags';

<DynamicMetaTags
  title="Deploying on Kubernetes | Dagster"
  description="Dagster is a system for building modern data applications."
/>

# Deploying on Kubernetes

The `dagster-k8s` package includes a template [Helm](https://helm.sh/)
chart that you can use to get up and running quickly on a Kubernetes
cluster. It's still early days for this chart, so it's not as yet
available on the Helm Hub.

Even if you don't use Helm, you may find the Helm charts useful as a
reference for all the components you will probably want as part of a
Kubernetes-based deployment of Dagster.

## Helm chart

If you've cloned the source for the Dagster repo, you can find the Helm
chart at (from the root of the repo)
`python_modules/libraries/dagster-k8s/helm/dagster`.

The Helm chart sets up a simple but complete deployment with the
following components (strangely capitalized nouns are Kubernetes
objects):

- Dagit running as a Deployment behind a Service. Dagit is configured
  with a ConfigMap (from which the instance config/`dagster.yaml` is
  read) to use a Postgres database of your choice for run and event
  log storage, and to use the `K8sRunLauncher` to launch each pipeline
  run in a separate Job. Each Jobs also gets its instance config from
  the ConfigMap.
- Optionally, a Postgres database for run and event log storage.
- Optionally, a RabbitMQ as a dagster-celery broker.
- Optionally, a set of dagster-celery workers running as a Deployment.
  Each workers also gets its instance config from the ConfigMap.
- An additional ConfigMap from which the dagster-celery workers, if
  configured, can read the celery config (i.e., the broker URL,
  backend, etc.; `celery.yaml`).
- An additional ConfigMap which can be used to inject environment
  variables into the dagster-k8s Jobs.
- A ServiceAccount (to launch the Jobs) bound to a properly scoped
  Role.

All of the parameters for this chart live in the `values.yaml` file.

## K8sRunLauncher

We use the **`RunLauncher`** to
express the difference between asking Dagit to execute a pipeline
directly (on the same node) and asking Dagit to launch execution
"somewhere else" -- perhaps just on a remote Dagit instance, or
perhaps in another environment such as a Kubernetes Job.

Note that **where** execution is launched is independent from **how** it
is executed, that is, which executor is used. The **`RunLauncher`** abstraction is
layered on top of the executor abstraction. You might choose to launch
execution in a Kubernetes Job so that execution is isolated from your
instance of Dagit, but users may still run their pipelines using the
single-process executor, the multiprocess executor, or the
dagster-celery executor.

A concrete **`RunLauncher`**, in
this case the <PyObject module="dagster_k8s" object="K8sRunLauncher" />,
can be configured on the Dagster instance using a config block such as the following:

```yaml
run_launcher:
  module: dagster_k8s.launcher
  class: K8sRunLauncher
  config:
    service_account_name: cool-service-account
    job_image: cool-docker-image
    instance_config_map: my-cool-instance-config-map
    job_namespace: cool-namespace
    #... more config options -- see the class for complete documentation
```

The most important thing to note here is the `job_image` config
parameter. As currently written, the <PyObject module="dagster_k8s" object="K8sRunLauncher" />
requires an image that can run CLI invocations of `dagster-graphql`
corresponding to the execution of your pipeline runs. That means that
your Dagster code (as well as dagster-graphql, and any system or Python
dependencies) must be present on the `job_image`. We plan to explore
other implementation options, perhaps allowing Dagster code to be
brought in using a persistent volume, and would appreciate feedback.
