import { DynamicMetaTags } from 'components/MetaTags';

<DynamicMetaTags
  title="Advanced Tutorials | Dagster"
  description="Dagster is a system for building modern data applications."
/>

import AnchorHeading from 'components/AnchorHeading';
import PyObject from 'components/PyObject';

# Advanced Tutorials

This section will introduce some advanced features and give you deeper insight into Dagster. It's worth reading if you have needs including things like

- Reusing the same solids over and over again
- Configuring pipeline-wide facilities to avoiding repeated code or config with unchanged business logic
- Persisting artifacts
- Scheduling data pipelines
- Better organizing multiple pipelines

## More about Solids

Abstracting business logic into reusable, configurable solids is one
important step towards making data applications like other software
applications.

> - You can find the tutorial code on [Github](https://github.com/dagster-io/dagster/tree/master/examples/dagster_examples/intro_tutorial/advanced/solids/)
> - If you’ve cloned the dagster git repository, you’ll find this example at `examples/dagster_examples/intro_tutorial/advanced/solids/`

### Reusable Solids

Solids are intended to abstract chunks of business logic, but
abstractions aren't very meaningful unless they can be reused.

Our conditional outputs pipeline included a lot of repeated code --
`sort_hot_cereals_by_calories` and `sort_cold_cereals_by_calories`, for
instance. In general, it's preferable to build pipelines out of a
relatively restricted set of well-tested library solids, using config
liberally to parametrize them. (You'll certainly have your own version
of `read_csv`, for instance, and Dagster includes libraries like
`dagster_aws` and `dagster_spark` to wrap and abstract interfaces with
common third party tools.)

Let's replace `sort_hot_cereals_by_calories` and
`sort_cold_cereals_by_calories` by two aliases of the same library
solid:

```python literalinclude showLines startLine=56 emphasize-lines=17-18 caption=reusable_solids.py
file:/dagster_examples/intro_tutorial/advanced/solids/reusable_solids.py
lines:56-75
```

You'll see that Dagit distinguishes between the two invocations of the
single library solid and the solid's definition. The invocation is
named and bound via a dependency graph to other invocations of other
solids. The definition is the generic, resuable piece of logic that is
invoked many times within this pipeline.

![reusable_solids.png](/assets/images/tutorial/reusable_solids.png)

Configuring solids also uses the aliases, as in the following YAML:

```YAML literalinclude emphasize-lines=6,8 caption=reusable_solids.yaml
file:/dagster_examples/intro_tutorial/advanced/solids/reusable_solids.yaml
```

<br />

### Composite Solids

The other basic facility that we expect from software in
other domains is composability -- the ability to combine building
blocks into larger functional units.

Composite solids can be used to organize and refactor large or
complicated pipelines, abstracting away complexity, as well as to wrap
reusable general-purpose solids together with domain-specific logic.

As an example, let's compose two instances of a complex,
general-purpose `read_csv` solid along with some domain-specific logic
for the specific purpose of joining our cereal dataset with a lookup
table providing human-readable names for the cereal manufacturers.

```python literalinclude showLines startLine=128 caption=composite_solids.py
file:/dagster_examples/intro_tutorial/advanced/solids/composite_solids.py
lines:128-132
```

Defining a composite solid is similar to defining a pipeline, except
that we use the <PyObject module="dagster" object="composite_solid" displayText="@composite_solid" /> decorator
instead of <PyObject module="dagster" object="pipeline" displayText="@pipeline" />.

Dagit has sophisticated facilities for visualizing composite solids:

![composite_solids.png](/assets/images/tutorial/composite_solids.png)

All of the complexity of the composite solid is hidden by default, but
we can expand it at will by clicking into the solid (or on the
"Expand" button in the right-hand pane):

![composite_solids_expanded.png](/assets/images/tutorial/composite_solids_expanded.png)

Note the line indicating that the output of `join_cereal` is returned as
the output of the composite solid as a whole.

Config for the individual solids making up the composite is nested, as
follows:

```YAML literalinclude emphasize-lines=1-3 caption=composite_solids.yaml
file:/dagster_examples/intro_tutorial/advanced/solids/composite_solids.yaml
```

When we execute this pipeline, Dagit includes information about the
nesting of individual execution steps within the composite:

![composite_solids_results.png](/assets/images/tutorial/composite_solids_results.png)

<br />

## More about Pipelines

Pipelines often interact with external resources like Hadoop/Spark clusters or data warehouses like Snowflake or BigQuery. Dagster provides various facilities to avoid hard-coding interactions with such systems, so that your business logic can remain the same across local/test, dev, prod, etc. environments. Resources represent these external systems, and modes/presets support swapping resource implementations across different environments.

> - You can find the tutorial code on [Github](https://github.com/dagster-io/dagster/tree/master/examples/dagster_examples/intro_tutorial/advanced/pipelines/)
> - If you’ve cloned the dagster git repository, you’ll find this example at `examples/dagster_examples/intro_tutorial/advanced/pipelines/`

### Parametrizing Pipelines with Resources

Dagster models interactions with features of the external environment
like these as resources (and library modules such as `dagster_aws`,
`dagster_gcp`, and even `dagster_slack` provide out-of-the-box
implementations for many common external services).

Typically, your data processing pipelines will want to store their
results in a data warehouse somewhere separate from the raw data
sources. We'll adjust our toy pipeline so that it does a little more
work on our cereal dataset, stores the finished product in a swappable
data warehouse, and lets the team know when we're finished.

You might have noticed that our cereal dataset isn't normalized --
that is, the serving sizes for some cereals are as small as a quarter of
a cup, and for others are as large as a cup and a half. This grossly
understates the nutritional difference between our different cereals.

Let's transform our dataset and then store it in a normalized table in
the warehouse:

```python literalinclude showLines startLine=59 emphasize-lines=18 caption=resources.py
file:/dagster_examples/intro_tutorial/advanced/pipelines/resources.py
lines:59-82
```

Resources are another facility that Dagster makes available on the
`context` object passed to solid logic. Note that we've completely
encapsulated access to the database behind the call to
`context.resources.warehouse.update_normalized_cereals`. This means that
we can easily swap resource implementations -- for instance, to test
against a local SQLite database instead of a production Snowflake
database; to abstract software changes, such as swapping raw SQL for
SQLAlchemy; or to accomodate changes in business logic, like moving from
an overwriting scheme to append-only, date-partitioned tables.

To implement a resource and specify its config schema, we use the <PyObject module="dagster" object="resource" displayText="@resource" /> decorator. The decorated function should return whatever object you wish to make available under the specific resource's slot in
`context.resources`. Resource constructor functions have access to their
own `context` argument, which gives access to resource-specific config.
(Unlike the contexts we've seen so far, which are instances of <PyObject module="dagster" object="SystemComputeExecutionContext" />, this
context is an instance of <PyObject module="dagster" object="InitResourceContext" />.)

```python literalinclude showLines startLine=17 emphasize-lines=28-30 caption=resources.py
file:/dagster_examples/intro_tutorial/advanced/pipelines/resources.py
lines:17-46
```

The last thing we need to do is to attach the resource to our pipeline,
so that it's properly initialized when the pipeline run begins and made
available to our solid logic as `context.resources.warehouse`.

```python literalinclude showLines startLine=85 emphasize-lines=2-6 caption=resources.py
file:/dagster_examples/intro_tutorial/advanced/pipelines/resources.py
lines:85-94
```

All resources are associated with a <PyObject module="dagster" object="ModeDefinition" /> So far,
all of our pipelines have had only a single, system default mode, so we haven't had to tell Dagster what mode to run
them in. Even in this case, where we provide a single anonymous mode to
the <PyObject module="dagster" object="pipeline" displayText="@pipeline" /> decorator,
we won't have to specify which mode to use (it will take the place of the `default` mode).

We can put it all together with the following config:

```YAML literalinclude emphasize-lines=1-4 caption=resources.yaml
file:/dagster_examples/intro_tutorial/advanced/pipelines/resources.yaml
```

(Here, we pass the special string `":memory:"` in config as the
connection string for our database -- this is how SQLite designates an
in-memory database.)

<br />

#### Expressing Resource Dependencies

We've provided a `warehouse` resource to our pipeline, but we're still
manually managing our pipeline's dependency on this resource. Dagster
also provides a way for solids to advertise their resource requirements,
to make it easier to keep track of which resources need to be provided
for a pipeline.

```python literalinclude showLines startLine=57 emphasize-lines=1 caption=required_resources.py
file:/dagster_examples/intro_tutorial/advanced/pipelines/required_resources.py
lines:57-80
```

Now, the Dagster machinery knows that this solid requires a resource
called `warehouse` to be present on its mode definitions, and will
complain if that resource is not present.

<br />

### Pipeline Modes

By attaching different sets of resources with the same APIs to different
modes, we can support running pipelines -- with unchanged business
logic -- in different environments. So you might have a "unittest"
mode that runs against an in-memory SQLite database, a "dev" mode that
runs against Postgres, and a "prod" mode that runs against Snowflake.

Separating the resource definition from the business logic makes
pipelines testable. As long as the APIs of the resources agree, and the
fundamental operations they expose are tested in each environment, we
can test business logic independent of environments that may be very
costly or difficult to test against.

```python literalinclude showLines startLine=75 caption=modes.py
file:/dagster_examples/intro_tutorial/advanced/pipelines/modes.py
lines:75-84
```

Even if you're not familiar with SQLAlchemy, it's enough to note that
this is a very different implementation of the `warehouse` resource. To
make this implementation available to Dagster, we attach it to a <PyObject module="dagster" object="ModeDefinition" />.

```python literalinclude showLines startLine=130 caption=modes.py
file:/dagster_examples/intro_tutorial/advanced/pipelines/modes.py
lines:130-145
```

Each of the ways we can invoke a Dagster pipeline lets us select which
mode we'd like to run it in.

From the command line, we can set `-d` or `--mode` and select the name
of the mode:

```bash
$ dagster pipeline execute -f modes.py -e resources.yaml -d unittest
```

Or, from the Python API:

```python literalinclude showLines startLine=146 emphasize-lines=12 caption=modes.py
file:/dagster_examples/intro_tutorial/advanced/pipelines/modes.py
lines:146-159
```

And in Dagit, we can use the "Mode" selector to pick the mode in which
we'd like to execute.

![modes.png](/assets/images/tutorial/modes.png)

The config editor is Dagit is mode-aware, so when you switch modes and
introduce a resource that requires additional config, the editor will
prompt you.

<br />

### Pipeline Config Presets

Useful as the Dagit config editor and the ability to stitch together
YAML fragments is, once pipelines have been productionized and config is
unlikely to change, it's often useful to distribute pipelines with
embedded config. For example, you might point solids at different S3
buckets in different environments, or want to pull database credentials
from different environment variables.

Dagster calls this a config preset:

```python literalinclude showLines startLine=129 emphasize-lines=14-40 caption=presets.py
file:/dagster_examples/intro_tutorial/advanced/pipelines/presets.py
lines:129-170
```

We illustrate two ways of defining a preset.

The first is to pass an `environment_dict` literal to the constructor.
Because this dict is defined in Python, you can do arbitrary computation
to construct it -- for instance, picking up environment variables,
making a call to a secrets store like Hashicorp Vault, etc.

The second is to use the `from_files` static constructor, and pass a
list of file globs from which to read YAML fragments. Order matters in
this case, and keys from later files will overwrite keys from earlier
files.

To select a preset for execution, we can use the CLI, the Python API, or
Dagit.

From the CLI, use `-p` or `--preset`:

```bash
$ dagster pipeline execute -f presets.py --preset unittest
```

From Python, you can use <PyObject module="dagster" object="execute_pipeline" />

```python literalinclude showLines startLine=174 caption=presets.py
file:/dagster_examples/intro_tutorial/advanced/pipelines/presets.py
lines:174
dedent:4
```

And in Dagit, we can use the "Presets" selector.

![presets.png](/assets/images/tutorial/presets.png)

<br />

<br />

## Materializations

> - You can find the tutorial code on [Github](https://github.com/dagster-io/dagster/tree/master/examples/dagster_examples/intro_tutorial/advanced/materializations/)
> - If you’ve cloned the dagster git repository, you’ll find this example at `examples/dagster_examples/intro_tutorial/advanced/materializations/`

Steps in a data pipeline often produce persistent artifacts, for instance,
graphs or tables describing the result of some computation. Typically these
artifacts are saved to disk (or to cloud storage) with a [name](https://xkcd.com/1459/) that
has something to do with their origin. But it can be hard to organize and cross-reference
artifacts produced by many different runs of a pipeline, or to identify all of the files that
might have been created by some pipeline's logic.

Dagster solids can describe their persistent artifacts to the system by
yielding <PyObject module="dagster" object="Materialization" /> events. Like <PyObject module="dagster" object="TypeCheck" /> and <PyObject module="dagster" object="ExpectationResult" />,
materializations are side-channels for metadata -- they don't get passed
to downstream solids and they aren't used to define the data dependencies that
structure a pipeline's DAG.

Suppose that we rewrite our `sort_calories` solid so that it saves
the newly sorted data frame to disk.

```python literalinclude showLines startLine=25 emphasize-lines=20-23 caption=materializations.py
file:/dagster_examples/intro_tutorial/advanced/materializations/materializations.py
lines:25-47
```

We've taken the basic precaution of ensuring that the saved csv file has a
different filename for each run of the pipeline. But there's no way for Dagit
to know about this persistent artifact. So we'll add the following lines:

```python literalinclude showLines startLine=24 emphasize-lines=25-33 caption=materializations.py
file:/dagster_examples/intro_tutorial/advanced/materializations/materializations.py
lines:24-57
```

Note that we've had to add the last line, yielding an <PyObject module="dagster" object="Output" />.
Until now, all of our solids have relied on Dagster's implicit conversion
of the return value of a solid's compute function into its output. When we explicitly
yield other types of events from solid logic, we need to also explicitly yield
the output so that the framework can recognize them.

Now, if we run this pipeline in Dagit:

![materializations.png](/assets/images/tutorial/materializations.png)

### Custom Materializing Data Types

Data types can also be configured so that outputs materialize themselves,
obviating the need to explicitly yield a <PyObject module="dagster" object="Materialization" /> from solid logic.
Dagster calls this facility the <PyObject module="dagster" object="output_materialization_config" displayText="@output_materialization_config" />.

Suppose we would like to be able to configure outputs of our toy custom type,
the `SimpleDataFrame`, to be automatically materialized to disk as
both as a pickle and as a .csv. (This is a reasonable idea, since .csv files
are human-readable and manipulable by a wide variety of third party tools,
while pickle is a binary format.)

```python literalinclude showLines startLine=29 caption=output_materialization.py
file:/dagster_examples/intro_tutorial/advanced/materializations/output_materialization.py
lines:29-70
```

We set the output materialization config on the type:

```python literalinclude showLines startLine=93 emphasize-lines=5 caption=output_materialization.py
file:/dagster_examples/intro_tutorial/advanced/materializations/output_materialization.py
lines:93-98
```

Now we can tell Dagster to materialize intermediate outputs of this type by
providing config:

```YAML literalinclude emphasize-lines=6-10 caption=output_materialization.yaml
file:/dagster_examples/intro_tutorial/advanced/materializations/output_materialization.yaml
lines:1-10
```

When we run this pipeline, we'll see that materializations are yielded (and
visible in the structured logs in Dagit), and that files are created on disk
(with the semicolon separator we specified).

![output_materializations.png](/assets/images/tutorial/output_materializations.png)

<br />

<br />

## Intermediates

> - You can find the tutorial code on [Github](https://github.com/dagster-io/dagster/tree/master/examples/dagster_examples/intro_tutorial/advanced/intermediates/)
> - If you’ve cloned the dagster git repository, you’ll find this example at `examples/dagster_examples/intro_tutorial/advanced/intermediates/`

We've already seen how solids can describe their persistent artifacts to the
system using [materializations](#materializations).

Dagster also has a facility for automatically materializing the intermediate
values that actually pass between solids.

This can be very useful for debugging, when you want to inspect the value
output by a solid and ensure that it is as you expect; for audit, when you
want to understand how a particular downstream output was created; and for
re-executing downstream solids with cached results from expensive upstream
computations.

To turn intermediate storage on, just set another key in the pipeline config:

```YAML literalinclude caption=intermediates.yaml emphasize-lines:6-7
file:/dagster_examples/intro_tutorial/intermediates.yaml
```

When you execute the pipeline using this config, you'll see new structured
entries in the Dagit log viewer indicating that intermediates have been stored
on the filesystem.

![intermediates.png](/assets/images/tutorial/intermediates.png)

### Intermediate Storage for Custom Data Types

By default, Dagster will try to pickle intermediate values to store them on
the filesystem. Some custom data types cannot be pickled (for instance, a
Spark RDD), so you will need to tell Dagster how to serialize them.

Our toy `LessSimpleDataFrame` is, of course, pickleable, but
supposing it was not, let's set a custom **`SerializationStrategy`** on
it to tell Dagster how to store intermediates of this type.

```python literalinclude showLines startLine=16 caption=serialization_strategy.py
file:/dagster_examples/intro_tutorial/advanced/intermediates/serialization_strategy.py
lines:16-39
```

Now, when we set the `storage` key in pipeline config and run this
pipeline, we'll see that our intermediate is automatically persisted as a
human-readable .csv:

![serialization_strategy.png](/assets/images/tutorial/serialization_strategy.png)

### Re-execution

Once intermediates are being stored, Dagit makes it possible to individually
execute solids whose outputs are satisfied by previously materialized
intermediates.

Click on the `sort_by_calories.compute` execution step, and you'll
see the option appear to reexecute only this step, using the automatically
materialized intermediate output of the previous solid.

![reexecution.png](/assets/images/tutorial/reexecution.png)

![reexecution_results.png](/assets/images/tutorial/reexecution_results.png)

Reexecuting individual solids can be very helpful while you're writing solids,
or while you're actively debugging them.

You can also manually specify intermediates from previous runs as inputs to
solids. Recall the syntax we used to set input values using the config system:

```YAML literalinclude caption=inputs_env.yaml
file:/dagster_examples/intro_tutorial/basics/e02_solids/inputs_env.yaml
```

Instead of setting the key `value` (i.e., providing a ), we can
also set `pickle`, as follows:

```YAML literalinclude caption=reexecution_env.yaml
file:/dagster_examples/intro_tutorial//advanced/intermediates/reexecution_env.yaml
```

(Of course, you'll need to use the path to an intermediate that is actually
present on your filesystem.)

If you directly substitute this config into Dagit, you'll see an error,
because the system still expects the input to `sort_by_calories` to
be satisfied by the output from `read_csv`.

![reexecution_errors.png](/assets/images/tutorial/reexecution_errors.png)

To make this config valid, we'll need to tell Dagit to execute only a subset
of the pipeline --just the `sort_by_calories` solid. Click on the
subset-selector button in the top left of the playground, to the left of the
Mode selector (which, when no subset has been specified, will read "\*"):

![subset_selection.png](/assets/images/tutorial/subset_selection.png)

Hit "Apply", and this config will now pass validation, and the individual
solid can be reexecuted:

![subset_config.png](/assets/images/tutorial/subset_config.png)

This facility is especially valuable during test, since it allows you to
validate newly written solids against values generated during previous runs of
a known good pipeline.

<br />

<br />

## Organizing Pipelines in Repositories

> - You can find the tutorial code on [Github](https://github.com/dagster-io/dagster/tree/master/examples/dagster_examples/intro_tutorial/advanced/repositories/)
> - If you’ve cloned the dagster git repository, you’ll find this example at `examples/dagster_examples/intro_tutorial/advanced/repositories/`

In all of the examples we've seen so far, we've specified a file (`-f`) or a module
(`-m`) and named a pipeline definition function (`-n`) in order to tell the
CLI tools how to load a pipeline, e.g.:

```bash
$ dagit -f hello_cereal.py
$ dagster pipeline execute -f hello_cereal.py
```

But most of the time, especially when working on long-running projects with
other people, we will want to be able to target many pipelines at once with
our tools.

Dagster provides the concept of a repository, a collection of pipelines that
the Dagster tools can target as a whole. Repositories are declared using the <PyObject module="dagster" object="RepositoryDefinition" /> API
as follows:

```python literalinclude showLines startLine=14 caption=repos.py
file:/dagster_examples/intro_tutorial/advanced/repositories/repos.py
lines:14-24
```

Note that the name of the pipeline in the `RepositoryDefinition`
must match the name we declared for it in its `pipeline` (the
default is the function name). Don't worry, if these names don't match, you'll
see a helpful error message.

If you save this file as `repos.py`, you can then run the command
line tools on it. Try running:

```bash
$ dagit -f repos.py -n define_repo
```

Now you can see the list of all pipelines in the repo via the dropdown at the
top:

![repos.png](/assets/images/tutorial/repos.png)

Typing the name of the file and function defining the repository gets tiresome
and repetitive, so let's create a declarative config file with this
information to make using the command line tools easier. Save this file as
`repository.yaml`. This is the default name for a repository config
file, although you can tell the CLI tools to use any file you like with the
`-y` flag.

```YAML literalinclude caption=repository.yaml
file:/dagster_examples/intro_tutorial/advanced/repositories/repository.yaml
```

Now you should be able to list the pipelines in this repo without all the
typing:

```bash
$ dagit
```

<br />

<br />

## Scheduling Pipeline Runs

> - You can find the tutorial code on [Github](https://github.com/dagster-io/dagster/tree/master/examples/dagster_examples/intro_tutorial/advanced/scheduling/)
> - If you’ve cloned the dagster git repository, you’ll find this example at `examples/dagster_examples/intro_tutorial/advanced/scheduling/`

Dagster includes a simple built-in scheduler that works with Dagit for
control and monitoring. Suppose that we need to run our simple cereal
pipeline every morning before breakfast, at 6:45 AM.

### Requirements

You'll need to install the `dagster-cron` library.

```bash
$ pip install dagster-cron
```

You must also ensure that `cron` is installed on the machine you're
running the scheduler on.

### Pipeline

```python literalinclude showLines caption=scheduler.py
file:/dagster_examples/intro_tutorial/advanced/scheduling/scheduler.py
lines:1-34
```

As before, we've defined some solids, a pipeline, and a repository.

### Defining the scheduler

We first need to define the Scheduler on our <PyObject module="dagster.core.instance" object="DagsterInstance" />. For
now, the only implemented scheduler is <PyObject module="dagster_cron" object="SystemCronScheduler"/>,
but this is pluggable (and you can write your own). To use the scheduler, add the following lines to your
`$DAGSTER_HOME/dagster.yaml`:

```yaml
scheduler:
  module: dagster_cron.cron_scheduler
  class: SystemCronScheduler
```

### Defining schedules

Now we'll write a <PyObject module="dagster" object="ScheduleDefinition" /> to
define the schedule we want. We pass the `cron_schedule` parameter to this class to define when the pipeline
should run using the standard cron syntax; the other parameters
determine other familiar aspects of how the pipeline will run, such as
its config.

We wrap the schedule definition in a function decorated with <PyObject module="dagster" object="schedules" displayText="@schedules" />

```python literalinclude showLines startLine=36 caption=scheduler.py
file:/dagster_examples/intro_tutorial/advanced/scheduling/scheduler.py
lines:36-45
```

To complete the picture, we'll need to extend the `repository.yaml`
structure we've met before with a new key, `scheduler`.

```YAML literalinclude caption=scheduler.yaml
file:/dagster_examples/intro_tutorial/advanced/scheduling/scheduler.yaml
```

### Starting schedules

Whenever we make changes to schedule definitions using the
`SystemCronScheduler`, we need to run `dagster schedule up`. This
utility will create, update, or remove schedules in the underlying
system cron file as appropriate to assure it is consistent with the
schedule definitions in code.

To preview the changes, first run:

```bash
$ dagster schedule up --preview -y scheduler.yaml
Planned Changes:
  + good_morning (add)
```

After confirming schedule changes are as expected, run:

```bash
$ dagster schedule up -y scheduler.yaml
Changes:
  + good_morning (add)
```

Verify that the `good_morning` scheduled job has been added to `cron`:

```bash
$ crontab -l
```

If the `good_morning` job is not listed, you may have to start it with:

```bash
$ dagster schedule start good_morning
```

Now, we can load dagit to view the schedule and monitor runs:

```bash
$ dagit -y scheduler.yaml
```

### Cron filters

If you need to define a more specific schedule than cron allows, you can
pass a function in the `should_execute` argument to <PyObject module="dagster" object="ScheduleDefinition" />.

For example, we can define a filter that only returns _`True`_
on weekdays:

```python
import datetime

def weekday_filter():
    weekno = datetime.datetime.today().weekday()
    # Returns true if current day is a weekday
    return weekno < 5
```

If we combine this _should_execute_ filter with a
_cron_schedule_ that runs at 6:45am every day, then we'll
have a schedule that runs at 6:45am only on weekdays.

```python
good_weekday_morning = ScheduleDefinition(
    name="good_weekday_morning",
    cron_schedule="45 6 * * *",
    pipeline_name="hello_cereal_pipeline",
    environment_dict={"storage": {"filesystem": {}}},
    should_execute=weekday_filter,
)
```

<br />

<br />
