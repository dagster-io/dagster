import { DynamicMetaTags } from 'components/MetaTags';

<DynamicMetaTags
  title="ETL with Dagster | Dagster"
  description="Dagster is a system for building modern data applications."
/>

import AnchorHeading from 'components/AnchorHeading';
import PyObject from 'components/PyObject';

# ETL with Dagster

We now have the basic building blocks for our cereal pipeline. To construct and execute a data
pipeline using Dagster, we will need solids as our computational units to contain business logics,
a pipeline to connect solids, and a way to execute the pipeline.

## Execute Our First Pipeline

> - You can find the tutorial code on [Github](https://github.com/dagster-io/dagster/tree/master/examples/dagster_examples/intro_tutorial/basics/e01_first_pipeline/)
> - If you’ve cloned the dagster git repository, you’ll find this example at `examples/dagster_examples/intro_tutorial/basics/e01_first_pipeline/`

<br />

Let's write our first Dagster pipeline.

### Hello, Solid!

Let's write our first Dagster solid and save it as `hello_cereal.py`.

A solid is a unit of computation in a data pipeline. Typically, you'll define
solids by annotating ordinary Python functions with the <PyObject module="dagster" object="solid" displayText="@solid" /> decorator.

The logic in our first solid is very straightforward: it just reads in the csv
from a hardcoded path and logs the number of rows it finds.

```python literalinclude showLines caption=hello_cereal.py
file:/dagster_examples/intro_tutorial/basics/e01_first_pipeline/hello_cereal.py
lines:1-20
```

In this simplest case, our solid takes no inputs except for the <PyObject module="dagster" object="SystemComputeExecutionContext" displayText="context" /> in
which it executes (provided by the Dagster framework as the first argument
to every solid), and also returns no outputs. Don't worry, we'll soon encounter
solids that are much more dynamic.

<br />

### Hello, Pipeline!

To execute our solid, we'll embed it in an equally simple pipeline.
A pipeline is a set of solids arranged into a DAG (or href [directed acyclic graph](https://en.wikipedia.org/wiki/Directed_acyclic_graph)) of
computation. You'll typically define pipelines by annotating ordinary Python functions with the <PyObject module="dagster" object="pipeline" displayText="@pipeline" /> decorator.

```python literalinclude showLines startLine=22 caption=hello_cereal.py
file:/dagster_examples/intro_tutorial/basics/e01_first_pipeline/hello_cereal.py
lines:22-24
```

Here you'll see that we call `hello_cereal()`. This call doesn't
actually execute the solid -- within the body of functions decorated with <PyObject module="dagster" object="pipeline" displayText="@pipeline" />, we use
function calls to indicate the dependency structure of the solids making up the pipeline.
Here, we indicate that the execution of
`hello_cereal` doesn't depend on any other solids by calling it
with no arguments.

<br />

### Executing Our First Pipeline

Assuming you’ve saved this pipeline as `hello_cereal.py`, we can execute it via any of three different
mechanisms:

<details className="markdown">
  <summary>From arbitrary Python scripts, use Dagster’s Python API</summary>

<br />

From the directory in which you've saved the pipeline file, just run:

```bash
$ dagster pipeline execute -f hello_cereal.py
```

You'll see the full stream of events emitted by dagster appear in the console,
including our call to the logging machinery, which will look like:

```bash
2019-10-10 11:46:50 - dagster - INFO - system - a91a4cc4-d218-4c2b-800c-aac50fced1a5
- Found 77 cereals
    solid             = "hello_cereal"
    solid_definition  = "hello_cereal"
    step_key          = "hello_cereal.compute"
```

Success!

</details>

<details className="markdown">
  <summary>From the command line, use the Dagster CLI</summary>

<br />

If you'd rather execute your pipelines as a script, you can do that without
using the dagster CLI at all. Just add a few lines to
`hello_cereal.py`

```python literalinclude showLines startLine=27 caption=hello_cereal.py
file:/dagster_examples/intro_tutorial/basics/e01_first_pipeline/hello_cereal.py
lines:27-29
```

Now you can just run:

```bash
$ python hello_cereal.py
```

The <PyObject module="dagster" object="execute_pipeline" displayText="execute_pipeline()" /> function
called here is the core Python API for executing Dagster pipelines from code.

</details>

<details className="markdown">
  <summary>From a GUI, use the Dagit tool</summary>

<br />

To visualize your pipeline (which only has one node) in dagit, from the
directory in which you've saved the pipeline file, just run:

```bash
$ dagit -f hello_cereal.py
```

You'll see output like

```bash
Loading repository... Serving on http://127.0.0.1:3000
```

You should be able to navigate to [http://127.0.0.1:3000/pipeline/hello_cereal_pipeline/](http://127.0.0.1:3000/pipeline/hello_cereal_pipeline/) in your web browser and view your pipeline.
It isn't very interesting yet, because it only has one node.

![hello_cereal_figure_one.png](/assets/images/tutorial/hello_cereal_figure_one.png)

Click on the "Playground" tab and you'll see the two-paned view below.

![hello_cereal_figure_two.png](/assets/images/tutorial/hello_cereal_figure_two.png)

The top pane is empty here, but in more complicated pipelines, this is where
you'll be able to edit pipeline configuration on the fly.

The bottom pane shows the concrete execution plan corresponding to the logical
structure of the pipeline -- which also only has one node, `hello_cereal.compute`.

Click the "Start Execution" button to execute this plan directly from dagit. A
new window should open, and you'll see a much more structured view of the
stream of Dagster events start to appear in the left-hand pane.

(If you have pop-up blocking enabled, you may need to tell your browser to
allow pop-ups from 127.0.0.1 -- or, just navigate to the "Runs" tab to see
this, and every run of your pipeline.)

![hello_cereal_figure_three.png](/assets/images/tutorial/hello_cereal_figure_three.png)

In this view, you can filter and search through the logs corresponding to your
pipeline run.

</details>

<br />

<br />

## Basics of Solids

> - You can find the tutorial code on [Github](https://github.com/dagster-io/dagster/tree/master/examples/dagster_examples/intro_tutorial/basics/e02_solids/)
> - If you’ve cloned the dagster git repository, you’ll find this example at `examples/dagster_examples/intro_tutorial/basics/e02_solids/`

### Parametrizing Solids with Inputs

So far, we've only seen solids whose behavior is the same every time
they're run:

```python literalinclude showLines startLine=7 emphasize-lines=4 caption=hello_cereal.py
file:/dagster_examples/intro_tutorial/basics/e01_first_pipeline/hello_cereal.py
lines:7-20
```

In general, though, rather than relying on hardcoded values like
`dataset_path`, we'd like to be able to parametrize our solid logic.
Appropriately parameterized solids are more testable, and also more
reusable. Consider the following more generic solid:

```python literalinclude showLines startLine=7 caption=inputs.py
file:/dagster_examples/intro_tutorial/basics/e02_solids/inputs.py
lines:7-15
```

Here, rather than hard-coding the value of `dataset_path`, we use an
input, `csv_path`. It's easy to see why this is better. We can reuse
the same solid in all the different places we might need to read in a
.csv from a filepath. We can test the solid by pointing it at some known
test csv file. And we can use the output of another upstream solid to
determine which file to load.

Let's rebuild a pipeline we've seen before, but this time using our
newly parameterized solid.

```python literalinclude showLines startLine=1 emphasize-lines=38 caption=inputs.py
file:/dagster_examples/intro_tutorial/basics/e02_solids/inputs.py
lines:1-38
```

<br />

### Specifying Config for Pipeline Execution

As you can see above, what's missing from this setup is a way to specify the `csv_path` input to our
new `read_csv` solid in the absence of any upstream solids whose outputs we can rely on. Dagster
provides the ability to stub inputs to solids that aren't satisfied by the pipeline topology as part
of its flexible configuration facility.

We can specify config for a pipeline execution regardless of
which modality we use to execute the pipeline — the Python API, the
Dagit GUI, or the command line:

<details className="markdown">
  <summary>Specifying config in the Python API</summary>

<br />

We previously encountered the <PyObject module="dagster" object="execute_pipeline" displayText="execute_pipeline()" />function.
Pipeline configuration is specified by the second argument to this function,
which must be a dict (the "run config").

This dict contains all of the user-provided configuration with which to
execute a pipeline. As such, it can have [a lot of sections](/docs/apidocs/execution####config_schema), but
we'll only use one of them here: per-solid configuration, which is
specified under the key `solids`:

```python literalinclude showLines startLine=41 caption=inputs.py
file:/dagster_examples/intro_tutorial/basics/e02_solids/inputs.py
lines:41-48
dedent:4
```

The `solids` dict is keyed by solid name, and each solid is configured
by a dict that may itself have several sections. In this case we are
only interested in the `inputs` section, so that we can specify the
value of the input `csv_path`.

Now you can pass this run config to <PyObject module="dagster" object="execute_pipeline" displayText="execute_pipeline()" />:

```python literalinclude showLines startLine=49 caption=inputs.py
file:/dagster_examples/intro_tutorial/basics/e02_solids/inputs.py
lines:49-51
dedent:4
```

</details>

<details className="markdown">
  <summary>Specifying config using YAML fragments and the dagster CLI</summary>

<br />

When executing pipelines with the dagster CLI, we'll need to provide the environment
dict in a config file. We use YAML for the file-based representation of an environment
dict, but the values are the same as before:

```YAML literalinclude showLines caption=inputs_env.yaml
file:/dagster_examples/intro_tutorial/basics/e02_solids/inputs_env.yaml
dedent:4
```

We can pass config files in this format to the dagster CLI tool with the
`-e` flag.

```bash
$ dagster pipeline execute -f inputs.py -c inputs_env.yaml
```

In practice, you might have different sections of your run config
in different yaml files —if, for instance, some sections change more
often (e.g. in test and prod) while other are more static. In this case,
you can set multiple instances of the `-e` flag on CLI invocations, and
the CLI tools will assemble the YAML fragments into a single environment
dict.

</details>

<details className="markdown">
  <summary>Using the Dagit config editor</summary>

<br />

Dagit provides a powerful, schema-aware, typeahead-enabled config editor to enable
rapid experimentation with and debugging of parameterized pipeline executions. As
always, run:

```bash
$ dagit -f inputs.py
```

Notice that no execution plan appears in the bottom pane of the Playground.

![inputs_figure_one.png](/assets/images/tutorial/inputs_figure_one.png)

Because Dagit is schema-aware, it knows that this pipeline now requires
configuration in order to run without errors. In this case, since the pipeline
is relatively trivial, it wouldn't be especially costly to run the pipeline
and watch it fail. But when pipelines are complex and slow, it's invaluable to
get this kind of feedback up front rather than have an unexpected failure deep
inside a pipeline.

Recall that the execution plan, which you will ordinarily see above the log
viewer in the **Execute** tab, is the concrete pipeline that
Dagster will actually execute. Without a valid config, Dagster can't construct
a parametrization of the logical pipeline — so no execution plan is available
for us to preview.

Press _CTRL-Space_ in order to bring up the typeahead assistant.

![inputs_figure_two.png](/assets/images/tutorial/inputs_figure_two.png)

Here you can see all of the sections available in the run config. Don't
worry, we'll get to them all later.

Let's enter the config we need in order to execute our pipeline.

![inputs_figure_three.png](/assets/images/tutorial/inputs_figure_three.png)

Note that as you type and edit the config, the config minimap hovering on the
right side of the editor pane changes to provide context — so you always know
where in the nested config schema you are while making changes.

</details>

<br />

### Parametrizing Solids with Config

Solids often depend in predictable ways on features of the external world or
the pipeline in which they're invoked. For example, consider an extended
version of our csv-reading solid that implements more of the options available
in the underlying Python API:

```python literalinclude showLines startLine=7 emphasize-lines=9-15 caption=config_bad_1.py
file:/dagster_examples/intro_tutorial/basics/e02_solids/config_bad_1.py
lines:7-29
```

We obviously don't want to have to write a separate solid for each permutation
of these parameters that we use in our pipelines — especially because, in
more realistic cases like configuring a Spark job or even parametrizing the
`read_csv` function from a popular package like [Pandas](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html#pandas.read_csv), we
might have dozens or hundreds of parameters like these.

But hoisting all of these parameters into the signature of the solid function
as inputs isn't the right answer either:

```python literalinclude showLines startLine=7 emphasize-lines=5-11 caption=config_bad_2.py
file:/dagster_examples/intro_tutorial/basics/e02_solids/config_bad_2.py
lines:7-39
```

Defaults are often sufficient for configuration values like these, and sets of
parameters are often reusable. And it's unlikely that values like this will be
provided dynamically by the outputs of other solids in a pipeline.

Inputs, on the other hand, will usually be provided by the outputs of other
solids in a pipeline, even though we might sometimes want to stub them using
the config facility.

For all these reasons, it's bad practice to mix configuration values like
these with true input values.

The solution is to define a config schema for our solid:

```python literalinclude showLines startLine=1 emphasize-lines=16,34-42,89 caption=config.py
file:/dagster_examples/intro_tutorial/basics/e02_solids/config.py
lines:1-100
```

First, we pass the `config` argument to the <PyObject module="dagster" object="solid" displayText="@solid" /> decorator.
This tells Dagster to give our solid a config field structured
as a dictionary, whose keys are the keys of this argument, and the types of whose
values are defined by the values of this argument (instances of <PyObject module="dagster" object="Field" />).

Then, we define one of these fields, `escapechar`, to be a string,
setting a default value, making it optional, and setting a human-readable
description.

Finally, inside the body of the solid function, we access the config value set
by the user using the `solid_config` field on the familiar <PyObject module="dagster" object="SystemComputeExecutionContext" displayText="context" /> object. When Dagster
executes our pipeline, the framework will make validated config for each solid available on this object.

Let's see how all of this looks in dagit. As usual, run:

```bash
$ dagit -f config.py
```

![config_figure_one.png](/assets/images/tutorial/config_figure_one.png)

As you may by now expect, Dagit provides a fully type-aware and schema-aware
config editing environment with a typeahead. The human-readable descriptions
we provided on our config fields appear in the config context minimap, as well
as in typeahead tooltips and in the Explore pane when clicking into the
individual solid definition.

![config_figure_two.png](/assets/images/tutorial/config_figure_two.png)

You can see that we've added a new section to the solid config. In addition to
the `inputs` section, which we'll still use to set the `csv_path` input, we now have a `config`
section, where we can set values defined in the `config` argument to <PyObject module="dagster" object="solid" displayText="@solid" />.

```YAML literalinclude caption=config_env_bad.yaml
file:/dagster_examples/intro_tutorial/basics/e02_solids/config_env_bad.yaml
```

Of course, this config won't give us the results we're expecting. The values
in `cereal.csv` are comma-separated, not semicolon-separated, as
they might be if this were a .csv from Europe, where commas are frequently
used in place of the decimal point.

We'll see later how we can use Dagster's facilities for automatic data quality
checks to guard against semantic issues like this, which won't be caught by
the type system.

<br />

### Multiple and Conditional Outputs

Solids can have arbitrarily many outputs, and downstream solids can
depends on any number of these.

What's more, outputs don't necessarily have to be yielded by solids,
which lets us write pipelines where some solids conditionally execute
based on the presence of an upstream output.

Suppose we're interested in splitting hot and cold cereals into
separate datasets and processing them separately, based on config.

```python literalinclude showLines startLine=35 emphasize-lines=7-12,21,26 caption=multiple_outputs.py
file:/dagster_examples/intro_tutorial/basics/e02_solids/multiple_outputs.py
lines:35-66
```

Solids that yield multiple outputs must declare, and name, their outputs
(passing `output_defs` to the <PyObject module="dagster" object="solid" displayText="@solid" /> decorator).
Output names must be unique and each <PyObject module="dagster" object="Output" /> yielded by a
solid's compute function must have a name that corresponds to one of these declared outputs.

We'll define two downstream solids and hook them up to the multiple
outputs from `split_cereals`.

```python literalinclude showLines startLine=58 emphasize-lines=23-25 caption=multiple_outputs.py
file:/dagster_examples/intro_tutorial/basics/e02_solids/multiple_outputs.py
lines:58-84
```

As usual, we can visualize this in Dagit:

![multiple_outputs.png](/assets/images/tutorial/multiple_outputs.png)

Notice that the logical DAG corresponding to the pipeline definition
includes both dependencies -- we won't know about the conditionality in
the pipeline until runtime, when one of the outputs is not yielded by
`split_cereal`.

![multiple_outputs_zoom.png](/assets/images/tutorial/multiple_outputs_zoom.png)

Zooming in, Dagit shows us the details of the multiple outputs from
`split_cereals` and their downstream dependencies.

When we execute this pipeline with the following config, we'll see that
the cold cereals output is omitted and that the execution step
corresponding to the downstream solid is marked skipped in the right
hand pane:

```YAML literalinclude showLines caption=multiple_outputs.yaml
file:/dagster_examples/intro_tutorial/basics/e02_solids/multiple_outputs.yaml
```

![conditional_outputs.png](/assets/images/tutorial/conditional_outputs.png)

<br />

<br />

## Basics of Pipelines

> - You can find the tutorial code on [Github](https://github.com/dagster-io/dagster/tree/master/examples/dagster_examples/intro_tutorial/basics/e03_pipelines/)
> - If you’ve cloned the dagster git repository, you’ll find this example at `examples/dagster_examples/intro_tutorial/basics/e03_pipelines/`

<br />

Our pipelines wouldn't be very interesting if they were limited to solids acting in isolation from each other.
Pipelines are useful because they let us connect solids into arbitrary DAGs ([directed acyclic graphs](https://en.wikipedia.org/wiki/Directed_acyclic_graph))
of computation.

### Let's Get Serial

We'll add a second solid to the pipeline we worked with in the first
section of the tutorial.

This new solid will consume the output of the first solid, which read
the cereal dataset in from disk, and in turn will sort the list of
cereals by their calorie content per serving.

```python literalinclude emphasize-lines=16,20,38
file:/dagster_examples/intro_tutorial/basics/e03_pipelines/serial_pipeline.py
lines:1-40
```

You'll see that we've modified our existing `load_cereals` solid to
return an output, in this case the list of dicts into which
`csv.DictReader <python:csv.DictReader>` reads the cereals dataset.

We've defined our new solid, `sort_by_calories`, to take a user-defined
input, `cereals`, in addition to the system-provided <PyObject module="dagster" object="SystemComputeExecutionContext" displayText="context" /> object.

We can use inputs and outputs to connect solids to each other. Here we
tell Dagster that although `load_cereals` doesn't depend on the output
of any other solid, `sort_by_calories` does -- it depends on the output
of `load_cereals`.

Let's visualize the DAG we've just defined in dagit.

```bash
$ dagit -f serial_pipeline.py
```

Navigate to [http://127.0.0.1:3000/pipeline/serial_pipeline/](http://127.0.0.1:3000/pipeline/serial_pipeline/) or choose
"serial_pipeline" from the dropdown:

![serial_pipeline_figure_one.png](/assets/images/tutorial/serial_pipeline_figure_one.png)

<br />

### A More Complex DAG

Solids don't need to be wired together serially. The output of one
solid can be consumed by any number of other solids, and the outputs of
several different solids can be consumed by a single solid.

```python literalinclude showLines emphasize-lines=55-59 startLine=7 caption=complex_pipeline.py
file:/dagster_examples/intro_tutorial/basics/e03_pipelines/complex_pipeline.py
lines:7-66
```

First we introduce the intermediate variable `cereals` into our pipeline
definition to represent the output of the `load_cereals` solid. Then we
make both `sort_by_calories` and `sort_by_protein` consume this output.
Their outputs are in turn both consumed by `display_results`.

Let's visualize this pipeline in Dagit
(`dagit -f complex_pipeline.py -n complex_pipeline`):

![complex_pipeline_figure_one.png](/assets/images/tutorial/complex_pipeline_figure_one.png)

When you execute this example from Dagit, you'll see that
`load_cereals` executes first, followed by `sort_by_calories` and
`sort_by_protein` -- in any order -- and that `display_results`
executes last, only after `sort_by_calories` and `sort_by_protein` have
both executed.

In more sophisticated execution environments, `sort_by_calories` and
`sort_by_protein` could execute not just in any order, but at the same
time, since they don't depend on each other's outputs -- but both
would still have to execute after `load_cereals` (because they depend on
its output) and before `display_results` (because `display_results`
depends on both of their outputs).

We'll write a simple test for this pipeline showing how we can assert
that all four of its solids executed successfully.

```python literalinclude showLines startLine=73 caption=complex_pipeline.py
file:/dagster_examples/intro_tutorial/basics/e03_pipelines/complex_pipeline.py
lines:73-80
```

<br />

<br />

## Making Your Pipelines Testable and Maintainable

The reality of building data applications is that they are notoriously difficult to test and are
therefore typically un- or under-tested. Besides, pipeline authors generally do not have control
over their input data, which led to an even more unfortunate reality — even if data pipelines are
covered by sophisticated tests, pipeline breakage could still happen.

Creating testable and verifiable data applications is one of the focuses of Dagster. We believe
ensuring data quality is critical for managing the complexity of data systems. This section will
talk about how you can build maintainable and testable data applications with Dagster.

> - You can find the tutorial code on [Github](https://github.com/dagster-io/dagster/tree/master/examples/dagster_examples/intro_tutorial/basics/e04_quality/)
> - If you’ve cloned the dagster git repository, you’ll find this example at `examples/dagster_examples/intro_tutorial/basics/e04_quality/`

### Testing solids and pipelines

Let's go back to our first solid and pipeline [`hello_cereal.py`](#execute-our-first-pipeline).

It wouldn't be complete without some tests to ensure they're working as expected. We'll use <PyObject module="dagster" object="execute_pipeline" displayText="execute_pipeline()" />
to test our pipeline, as well as <PyObject module="dagster" object="execute_solid" displayText="execute_solid()" />
to test our solid in isolation.

These functions synchronously execute a pipeline or solid and return results
objects (the <PyObject module="dagster" object="SolidExecutionResult" /> and <PyObject module="dagster" object="PipelineExecutionResult" />)
whose methods let us investigate, in detail, the success or failure of execution,
the outputs produced by solids, and (as we'll see later) other events associated
with execution.

```python literalinclude showLines startLine=32 caption=hello_cereal.py
file:/dagster_examples/intro_tutorial/basics/e04_quality/hello_cereal_with_tests.py
lines:32-43
```

Now you can use pytest, or your test runner of choice, to run unit tests as
you develop your data applications.

```bash
$ pytest hello_cereal_with_tests.py
```

Note: by convention, pytest tests are typically kept in separate files
prefixed with `test_`. We've put them in the same file just to
simplify the tutorial code.

Obviously, in production we'll often execute pipelines in a parallel,
streaming way that doesn't admit this kind of API, which is intended to enable
local tests like this.

Dagster is written to make testing easy in a domain where it has historically been very difficult. Throughout the rest of this tutorial, we'll explore the writing of unit tests for each piece of the framework as we learn about it. You can learn more about Testing in Dagster by reading [Testing Guide](/docs/learn/guides/testing/testing).

<br />

### Type-checking Inputs with Python Type Annotation

Note that this section requires Python 3.

If you zoom in on the **Explore** tab in Dagit and click on one
of our pipeline solids, you'll see that its inputs and outputs are annotated
with types.

![inputs_figure_four.png](/assets/images/tutorial/inputs_figure_four.png)

By default, every untyped value in Dagster is assigned the catch-all type <PyObject module="dagster" object="Any" />.
This means that any errors in the config won't be surfaced until the pipeline
is executed.

For example, when we execute our pipeline with this config, it'll fail at
runtime:

```YAML literalinclude caption=inputs_env_bad.yaml
file:/dagster_examples/intro_tutorial/basics/e04_quality/inputs_env_bad.yaml
```

When we enter this mistyped config in Dagit and execute our pipeline, you'll
see that an error appears in the structured log viewer pane of the **Execute**
tab:

![inputs_figure_five.png](/assets/images/tutorial/inputs_figure_five.png)

Click on "View Full Message" or on the red dot on the execution step that
failed and a detailed stacktrace will pop up.

![inputs_figure_six.png](/assets/images/tutorial/inputs_figure_six.png)

It would be better if we could catch this error earlier, when we specify the
config. So let's make the inputs typed.

A user can apply types to inputs and outputs using Python 3's type annotation
syntax. In this case, we just want to type the input as the built-in `str`.

```python literalinclude showLines startLine=7 emphasize-lines=2 caption=inputs_typed.py
file:/dagster_examples/intro_tutorial/basics/e04_quality/inputs_typed.py
lines:7-15
```

By using typed input instead we can catch this error prior to execution, and
reduce the surface area we need to test and guard against in user code.

![inputs_figure_seven.png](/assets/images/tutorial/inputs_figure_seven.png)

<br />

### Dagster Types

We've seen how we can type the inputs and outputs of solids using
Python 3's typing system, and how to use Dagster's built-in config
types, such as <PyObject module="dagster" object="String" displayText="dagster.String" />, to
define config schemas for our solids.

But what about when you want to define your own types?

This section will talk about the basics of Dagster's user defined types in the following order. You
can also learn more about Typing in Dagster by reading [Dagster Types Guide](/docs/learn/guides/dagster_types).

- [Complex Type Checks](#complex-type-checks)
- [Providing Input Values for Custom Types in Config](#providing-input-values-for-custom-types-in-config)
- [Testing Custom Types](#testing-custom-types)
- [MyPy Compliance](#mypy-compliance)
- [Metadata and Custom Type Checks](#Metadata-and-custom-type-checks)

<br />

Let's look back at our simple `read_csv` solid.

```python literalinclude showLines startLine=7 caption=inputs_typed.py
file:/dagster_examples/intro_tutorial/basics/e04_quality/inputs_typed.py
lines:7-15
```

The `lines` object returned by Python's built-in `csv.DictReader` is a
list of `collections.OrderedDict`, each of which represents one row of
the dataset:

```python
[
    OrderedDict([
        ('name', '100% Bran'), ('mfr', 'N'), ('type', 'C'), ('calories', '70'), ('protein', '4'),
        ('fat', '1'), ('sodium', '130'), ('carbo', '5'), ('sugars', '6'), ('potass', '280'),
        ('vitamins', '25'), ('shelf', '3'), ('weight', '1'), ('cups', '0.33'),
        ('rating', '68.402973')
    ]),
    OrderedDict([
        ('name', '100% Natural Bran'), ('mfr', 'Q'), ('type', 'C'), ('calories', '120'),
        ('protein', '3'), ('fat', '5'), ('sodium', '15'), ('fiber', '2'), ('carbo', '8'),
        ('sugars', '8'), ('potass', '135'), ('vitamins', '0'), ('shelf', '3'), ('weight', '1'),
        ('cups', '1'), ('rating', '33.983679')
    ]),
    ...
]
```

This is a simple representation of a "data frame", or a table of data.
We'd like to be able to use Dagster's type system to type the output
of `read_csv`, so that we can do type checking when we construct the
pipeline, ensuring that any solid consuming the output of `read_csv`
expects to receive a data frame.

To do this, we'll use the <PyObject module="dagster" object="DagsterType" displayText="DagsterType" /> class:

```python literalinclude  showLines startLine=6 caption=custom_types.py
file:/dagster_examples/intro_tutorial/basics/e04_quality/custom_types.py
lines:6-10
```

Now we can annotate the rest of our pipeline with our new type:

```python literalinclude showLines startLine=13 emphasize-lines=2,12 caption=custom_types.py
file:/dagster_examples/intro_tutorial/basics/e04_quality/custom_types.py
lines:13-36
```

The type metadata now appears in dagit and the system will ensure the
input and output to this solid are indeed instances of SimpleDataFrame.
As usual, run:

```bash
$ dagit -f custom_types.py
```

![custom_types_figure_one.png](/assets/images/tutorial/custom_types_figure_one.png)

You can see that the output of `read_csv` (which by default has the name
`result`) is marked to be of type `SimpleDataFrame`.

<br />

#### Complex Type Checks

The Dagster framework will fail type checks when a value isn't an
instance of the type we're expecting, e.g., if `read_csv` were to
return a `str` rather than a `SimpleDataFrame`.

Sometimes we know more about the types of our values, and we'd like to
do deeper type checks. For example, in the case of the
`SimpleDataFrame`, we expect to see a list of OrderedDicts, and for each
of these OrderedDicts to have the same fields, in the same order.

The type check function allows us to do this.

```python literalinclude showLines startLine=7 emphasize-lines=1,21 caption=custom_types_2.py
file:/dagster_examples/intro_tutorial/basics/e04_quality/custom_types_2.py
lines:7-30
```

Now, if our solid logic fails to return the right type, we'll see a
type check failure. Let's replace our `read_csv` solid with the
following bad logic:

```python literalinclude showLines startLine=32 caption=custom_types_2.py
file:/dagster_examples/intro_tutorial/basics/e04_quality/custom_types_2.py
lines:32-39
```

When we run the pipeline with this solid, we'll see an error like:

```bash
2019-10-12 13:19:19 - dagster - ERROR - custom_type_pipeline - 266c6a93-75e2-46dc-8bd7-d684ce91d0d1 - STEP_FAILURE
- Execution of step "bad_read_csv.compute" failed.
            cls_name = "Failure"
            solid = "bad_read_csv"
    solid_definition = "bad_read_csv"
            step_key = "bad_read_csv.compute"
user_failure_data = {"description": "LessSimpleDataFrame should be a list of OrderedDicts, got <class 'str'>
for row 1", "label": "intentional-failure", "metadata_entries": []}
```

<br />

#### Providing Input Values for Custom Types in Config

We saw earlier how, when a solid doesn't receive all of its inputs from
other solids further upstream in the pipeline, we can specify its input
values in config:

```YAML literalinclude caption=inputs_env.yaml
file:/dagster_examples/intro_tutorial/basics/e02_solids/inputs_env.yaml
```

The Dagster framework knows how to interpret values provided via config
as scalar inputs. In this case, `read_csv` just takes the string
representation of the filepath from which it'll read a csv. But for
more complex, custom types, we need to tell Dagster how to interpret
config values.

Consider our LessSimpleDataFrame. It might be convenient if Dagster knew
automatically how to read a data frame in from a csv file, without us
needing to separate that logic into the `read_csv` solid -- especially
if we knew the provenance and format of that csv file (e.g., if we were
using standard csvs as an internal interchange format) and didn't need
the full configuration surface of a general purpose `read_csv` solid.

What we want to be able to do is write:

```YAML literalinclude caption=custom_type_input.yaml
file:/dagster_examples/intro_tutorial/basics/e04_quality/custom_type_input.yaml
```

In order for the Dagster machinery to be able to decode the config value
`{'csv': 'cereal.csv'}` into an input of the correct
`LessSimpleDataFrame` value, we need to write what we call an input
hydration config.

```python literalinclude showLines startLine=32 caption=custom_types_3.py
file:/dagster_examples/intro_tutorial/basics/e04_quality/custom_types_3.py
lines:32-39
```

A function decorated with <PyObject module="dagster" object="input_hydration_config" displayText="@input_hydration_config" /> should
take the context object, as usual, and a parameter
representing the parsed config field. The schema for this field is
defined by the argument to the <PyObject module="dagster" object="input_hydration_config" displayText="@input_hydration_config" /> decorator.

Here, we introduce the <PyObject module="dagster" object="Selector" /> type,
which lets you specify mutually exclusive options in config schemas.
Here, there's only one option, `csv`, but you can imagine a more
sophisticated data frame type that might also know how to hydrate its
inputs from other formats and sources, and might have a selector with
fields like `parquet`, `xlsx`, `sql`, etc.

Then insert this into the original declaration:

```python literalinclude showLines startLine=42 emphasize-lines=5 caption=custom_types_3.py
file:/dagster_examples/intro_tutorial/basics/e04_quality/custom_types_3.py
lines:42-47
```

Now if you run a pipeline with this solid from dagit you will be able to
provide sources for these inputs via config:

```python literalinclude showLines startLine=70 caption=custom_types_3.py
file:/dagster_examples/intro_tutorial/basics/e04_quality/custom_types_3.py
lines:70-80
```

<br />

#### Testing Custom Types

As you write your own custom types, you'll also want to set up unit
tests that ensure your types are doing what you expect them to. Dagster
includes a utility function, <PyObject module="dagster" object="check_dagster_type" />,
that lets you type check any Dagster type against any value.

```python literalinclude showLines startLine=102 caption=custom_types_test.py
file:/dagster_examples/intro_tutorial/basics/e04_quality/custom_types_test.py
lines:102-114
```

Well tested library types can be reused across solids and pipelines to
provide standardized type checking within your organization's data
applications.

<br />

#### MyPy Compliance

In cases where DagsterTypes are created that do not have corresponding
usable Python types, and the user wishes to remain mypy compliant, there
are two options.

One is using `InputDefinition` and `OutputDefinition` exclusively for
dagster types, and reserving type annotations for naked Python types
_only_. This is verbose, but is explicit and clear.

```python literalinclude showLines startLine=21 emphasize-lines=1-5,14-15 caption=custom_types_mypy_verbose.py
file:/dagster_examples/intro_tutorial/basics/e04_quality/custom_types_mypy_verbose.py
lines:21-47
```

If one wishes to use type annotations exclusively but still use dagster
types without a 1:1 python type counterpart, the type checking behavior
must be modified. For this we recommend using the `typing.TYPE_CHECKING`
property in the python typing module.

While inelegant, this centralizes boilerplate to the type instantiation,
rather than have it on all places where the type is referenced.

```python literalinclude showLines startLine=7 emphasize-lines=1-8 caption=custom_types_mypy_typing_trick.py
file:/dagster_examples/intro_tutorial/basics/e04_quality/custom_types_mypy_typing_trick.py
lines:7-25
```

<br />

#### Metadata and Custom Type Checks

Custom types can also yield metadata about the type check. For example, in the
case of our data frame, we might want to record the number of rows and columns
in the dataset when our type checks succeed, and provide more information
about why type checks failed when they fail.
User-defined type check functions can optionally return a <PyObject module="dagster" object="TypeCheck" /> object
that contains metadata about the success or failure of the type check.
Let's see how to use this to emit some summary statistics about our DataFrame
type:

```python literalinclude showLines startLine=18 emphasize-lines=3-9,33-53 caption=custom_types_4.py
file:/dagster_examples/intro_tutorial/basics/e04_quality/custom_types_4.py
lines:18-71
```

A <PyObject module="dagster" object="TypeCheck" displayText="TypeCheck" /> must include a
`success` argument describing whether the check
passed or failed, and may include a description and/or a list of <PyObject module="dagster" object="EventMetadataEntry" /> objects.
You should use the static constructors on <PyObject module="dagster" object="EventMetadataEntry" /> to
construct these objects, which are flexible enough to support arbitrary metadata in JSON or Markdown format.

Dagit knows how to display and archive structured metadata of this kind for
future review:

![custom_types_figure_two.png](/assets/images/tutorial/custom_types_figure_two.png)

<br />

### Expectations

Custom type checks and metadata are appropriate for checking that a value will behave as we expect, and for collecting summary information about values.

But sometimes we want to make more specific, data- and business logic-dependent assertions about the semantics of values. It typically isn't appropriate to embed assertions like these into data types directly.

For one, they will usually vary substantially between instantiations — for example, we don't expect all data frames to have the same number of columns, and over-specifying data types (e.g., `SixColumnedDataFrame`) makes it difficult for generic logic to work generically (e.g., over all
data frames).

What's more, these additional, deeper semantic assertions are often non-stationary. Typically, you'll start running a pipeline with certain expectations about the data that you'll see; but over time, you'll learn more about your data (making your expectations more precise), and the process in the world that generates your data will shift (making some of your expectations invalid.)

We've already encountered the <PyObject module="dagster" object="TypeCheck" /> event, which is typically yielded by the type machinery (but can also be yielded manually from the body of a solid's compute function); <PyObject module="dagster" object="ExpectationResult" /> is another kind of structured side-channel result that a solid can yield. These extra events don't get passed to downstream solids and they aren't used to define the data dependencies of a pipeline DAG.

```python literalinclude showLines startLine=93 emphasize-lines=1-3,31 caption=custom_types_bad_5.py
file:/dagster_examples/intro_tutorial/basics/e04_quality/custom_types_bad_5.py
lines:93-133
```

Until now, every solid we've encountered has returned its result value, or `None`. But solids can also yield events of various types for side-channel communication about the results of their computations.

Running this pipeline yields an <PyObject module="dagster" object="ExpectationResult" /> with `success` set to `False` since we expect entries in the `calories` column to be of type `int` but they are of type `string`. We note that this precedes our incorrect result that the least caloric cereal is Corn Flakes (100 calories per serving) and the most caloric cereal Strawberry Fruit Wheats (90 calories per serving).

![custom_types_bad_data.png](/assets/images/tutorial/custom_types_bad_data.png)

To fix this, we can cast `calories` to `int` during the hydration process:

```python literalinclude showLines startLine=75 emphasize-lines=7 caption=custom_types_5.py
file:/dagster_examples/intro_tutorial/basics/e04_quality/custom_types_5.py
lines:75-86
```

Running this pipeline yields an <PyObject module="dagster" object="ExpectationResult" /> with
`success` set to `True` and the correct result that
the least caloric cereal is All-Bran with Extra Fiber (50 calories per
serving) and the most caloric cereal is Mueslix Crispy Blend (160
calories per serving).

This part of this system remains relatively immature, but yielding structured expectation results from your solid logic means that in future, tools like Dagit will be able to aggregate and track expectation results, as well as implement sophisticated policy engines to drive alerting and exception handling on a deep semantic basis. You can learn more about it by reading [Data Quality Tests Guide](/docs/learn/guides/testing/expectations).

<br />

Congratulations! Having reached this far, you now have a working, testable, and maintainable data pipeline. You’ve also learned the basics of Dagster, and you should now be able to build your own data applications using Dagster!

<br />

<br />
