import { DynamicMetaTags } from 'components/MetaTags';
import { CodeReferenceLink } from 'components/CodeReference';
import AnchorHeading from 'components/AnchorHeading';
import PyObject from 'components/PyObject';

<DynamicMetaTags
  title="Building pipelines with Dagster | Dagster"
  description="Tutoiral guide to executing data pipelines using Dagster."
/>
;

# Building Pipelines with Dagster

We now have the basic building blocks for our cereal pipeline. To construct and execute a data
pipeline using Dagster, we will need solids as our computational units containing business logic,
a pipeline to connect solids, and a way to execute the pipeline.

If you haven't already, grab the dataset for this tutorial from Github:

```shell
curl -O https://raw.githubusercontent.com/dagster-io/dagster/master/examples/docs_snippets/docs_snippets/intro_tutorial/cereal.csv
```

## Executing Your First Pipeline

<CodeReferenceLink filePath="examples/docs_snippets/docs_snippets/intro_tutorial/basics/e01_first_pipeline/" />

## Hello, Solid!

Let's write our first Dagster solid and save it as `hello_cereal.py`.

A solid is a unit of computation in a data pipeline. Typically, you'll define solids by annotating
ordinary Python functions with the <PyObject module="dagster" object="solid" displayText="@solid" />
decorator.

The logic in our first solid is very straightforward: it just reads in the csv from a hardcoded path
and logs the number of rows it finds.

```python literalinclude showLines caption=hello_cereal.py
file:/docs_snippets/docs_snippets/intro_tutorial/basics/e01_first_pipeline/hello_cereal.py
lines:1-20
```

In this simple case, our solid takes no inputs except for the <PyObject module="dagster"
object="SystemComputeExecutionContext" displayText="context" /> in which it executes (provided by
the Dagster framework as the first argument to every solid), and also returns no outputs. Don't
worry, we'll soon encounter solids that are much more dynamic.

<br />

## Hello, Pipeline!

To execute our solid, we'll embed it in an equally simple pipeline. A pipeline is a set of solids
arranged into a [DAG](https://en.wikipedia.org/wiki/Directed_acyclic_graph) of computation. You'll
typically define pipelines by annotating ordinary Python functions with the <PyObject
module="dagster" object="pipeline" displayText="@pipeline" /> decorator.

```python literalinclude showLines startLine=22 caption=hello_cereal.py
file:/docs_snippets/docs_snippets/intro_tutorial/basics/e01_first_pipeline/hello_cereal.py
lines:22-24
```

Here you'll see that we call `hello_cereal()`. This call doesn't actually execute the
solid&mdash;within the body of functions decorated with <PyObject module="dagster" object="pipeline"
displayText="@pipeline" />, we use function calls to indicate the dependency structure of the solids
making up the pipeline. Here, we indicate that the execution of `hello_cereal` doesn't depend on any
other solids by calling it with no arguments.

<br />

## Executing Our First Pipeline

Assuming you’ve saved this pipeline as `hello_cereal.py`, we can execute it via any of three
different mechanisms:

<details className="markdown">
  <summary>From the command line, use the Dagster CLI</summary>

<br />

From the directory in which you've saved the pipeline file, just run:

```bash
dagster pipeline execute -f hello_cereal.py
```

You'll see the full stream of events emitted by Dagster appear in the console,
including our call to the logging machinery, which will look like:

```bash
2019-10-10 11:46:50 - dagster - INFO - system - a91a4cc4-d218-4c2b-800c-aac50fced1a5
- Found 77 cereals
    solid             = "hello_cereal"
    solid_definition  = "hello_cereal"
    step_key          = "hello_cereal.compute"
```

Success!

</details>

<details className="markdown">
  <summary>From Python, use Dagster’s Python API</summary>

<br />

If you'd rather execute your pipelines as a script, you can do that without
using the Dagster CLI at all. Just add a few lines to
`hello_cereal.py`

```python literalinclude showLines startLine=27 caption=hello_cereal.py
file:/docs_snippets/docs_snippets/intro_tutorial/basics/e01_first_pipeline/hello_cereal.py
lines:27-29
```

Now you can just run:

```bash
python hello_cereal.py
```

The <PyObject module="dagster" object="execute_pipeline" displayText="execute_pipeline()" /> function
called here is the core Python API for executing Dagster pipelines from code.

</details>

<details className="markdown">
  <summary>From a GUI, use the Dagit tool</summary>

<br />

To visualize your pipeline (which only has one node) in Dagit, from the
directory in which you've saved the pipeline file, just run:

```bash
dagit -f hello_cereal.py
```

You'll see output like

```bash
Loading repository... Serving on http://127.0.0.1:3000
```

You should be able to navigate to
[http://127.0.0.1:3000/pipeline/hello_cereal_pipeline/](http://127.0.0.1:3000/pipeline/hello_cereal_pipeline/)
in your web browser and view your pipeline. It isn't very interesting yet, because it only has one
node.

![hello_cereal_figure_one.png](/assets/images/tutorial/hello_cereal_figure_one.png)

Click on the "Playground" tab and you'll see the view below.

![hello_cereal_figure_two.png](/assets/images/tutorial/hello_cereal_figure_two.png)

The top pane is empty here, but in more complicated pipelines, this is where you'll be able to edit
pipeline configuration on the fly.

The bottom pane shows the concrete execution plan corresponding to the logical structure of the
pipeline&mdash;which also only has one node, `hello_cereal.compute`.

Click the "Launch Execution" button to execute this plan directly from Dagit. A new window should
open, and you'll see a much more structured view of the stream of Dagster events start to appear in
the left-hand pane.

If you have pop-up blocking enabled, you may need to tell your browser to allow pop-ups from
127.0.0.1&mdash;or, just navigate to the "Runs" tab to see this, and every run of your pipeline.

![hello_cereal_figure_three.png](/assets/images/tutorial/hello_cereal_figure_three.png)

In this view, you can filter and search through the logs corresponding to your pipeline run.

</details>

<br />

<br />
