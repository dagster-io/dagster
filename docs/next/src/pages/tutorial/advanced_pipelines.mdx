import { DynamicMetaTags } from 'components/MetaTags';
import { CodeReferenceLink } from 'components/CodeReference';

<DynamicMetaTags
  title="Advanced: Pipelines | Dagster"
  description="Dagster provides various pipeline-level facilities to avoid hard-coding interactions
  and therefore improve the development experience."
/>

import AnchorHeading from 'components/AnchorHeading';
import PyObject from 'components/PyObject';

# Advanced: Pipelines

<CodeReferenceLink filePath="examples/docs_snippets/docs_snippets/intro_tutorial/advanced/pipelines/" />

Pipelines often interact with external resources like Hadoop/Spark clusters or data warehouses like
Snowflake or BigQuery. Dagster provides various facilities to avoid hard-coding interactions with
such systems, so that your business logic can remain the same across different environments
(local/test, dev, prod, etc.) Resources represent these external systems, and modes/presets support swapping
resource implementations across different environments.

## Parametrizing Pipelines with Resources

Dagster models interactions with features of the external environment as **resources**.
Dagster's library modules such as [`dagster_aws`](/_apidocs/libraries/dagster_aws),
[`dagster_gcp`](/_apidocs/libraries/dagster_gcp), and
[`dagster_slack`](/_apidocs/libraries/dagster_slack) provide out-of-the-box implementations for
many common external services.

Typically, your data processing pipelines will want to store their results in a data warehouse
somewhere separate from the raw data sources. We'll adjust our toy pipeline so that it does a little
more work on our cereal dataset, stores the finished product in a swappable data warehouse, and lets
the team know when we're finished.

You might have noticed that our cereal dataset isn't normalized&mdash;that is, the serving sizes for
some cereals are as small as a quarter of a cup, and for others are as large as a cup and a half.
This grossly understates the nutritional difference between our different cereals.

Let's transform our dataset and then store it in a normalized table in the warehouse:

```python literalinclude showLines startLine=59 emphasize-lines=18 caption=resources.py
file:/docs_snippets/docs_snippets/intro_tutorial/advanced/pipelines/resources.py
lines:59-82
```

Resources are another facility that Dagster makes available on the `context` object passed to solid
logic. Note that we've completely encapsulated access to the database behind the call to
`context.resources.warehouse.update_normalized_cereals`. This means that we can easily swap resource
implementations&mdash;for instance, to test against a local SQLite database instead of a production
Snowflake database; to abstract software changes, such as swapping raw SQL for SQLAlchemy; or to
accommodate changes in business logic, like moving from an overwriting scheme to append-only,
date-partitioned tables.

To implement a resource and specify its config schema, we use the <PyObject module="dagster"
object="resource" displayText="@resource" /> decorator. The decorated function should return
whatever object you wish to make available under the specific resource's slot in
`context.resources`. Resource constructor functions have access to their own `context` argument,
which gives access to resource-specific config. (Unlike the contexts we've seen so far, which are
instances of <PyObject module="dagster" object="SystemComputeExecutionContext" />, this context is
an instance of <PyObject module="dagster" object="InitResourceContext" />.)

```python literalinclude showLines startLine=17 emphasize-lines=28-30 caption=resources.py
file:/docs_snippets/docs_snippets/intro_tutorial/advanced/pipelines/resources.py
lines:17-46
```

The last thing we need to do is to attach the resource to our pipeline, so that it's properly
initialized when the pipeline run begins and made available to our solid logic as
`context.resources.warehouse`.

```python literalinclude showLines startLine=85 emphasize-lines=2-6 caption=resources.py
file:/docs_snippets/docs_snippets/intro_tutorial/advanced/pipelines/resources.py
lines:85-94
```

All resources are associated with a <PyObject module="dagster" object="ModeDefinition" /> So far,
all of our pipelines have had only a single, system default mode, so we haven't had to tell Dagster
what mode to run them in. Even in this case, where we provide a single anonymous mode to
the <PyObject module="dagster" object="pipeline" displayText="@pipeline" /> decorator,
we won't have to specify which mode to use (it will take the place of the `default` mode).

We can put it all together with the following config:

```YAML literalinclude emphasize-lines=1-4 caption=resources.yaml
file:/docs_snippets/docs_snippets/intro_tutorial/advanced/pipelines/resources.yaml
```

Here, we pass the special string `":memory:"` in config as the connection string for our
database&mdash;this is how SQLite designates an in-memory database.

<br />

### Expressing Resource Dependencies

We've provided a `warehouse` resource to our pipeline, but we're still manually managing our
pipeline's dependency on this resource. Dagster also provides a way for solids to advertise their
resource requirements, to make it easier to keep track of which resources need to be provided for a
pipeline.

```python literalinclude showLines startLine=57 emphasize-lines=1 caption=required_resources.py
file:/docs_snippets/docs_snippets/intro_tutorial/advanced/pipelines/required_resources.py
lines:57-80
```

Now, the Dagster machinery knows that this solid requires a resource called `warehouse` to be
present on its mode definitions, and will complain if that resource is not present.

<br />

## Pipeline Modes

By attaching different sets of resources with the same APIs to different modes, we can support
running pipelines&mdash;with unchanged business logic&mdash;in different environments. You might
have a "unittest" mode that runs against an in-memory SQLite database, a "dev" mode that runs
against Postgres, and a "prod" mode that runs against Snowflake.

Separating the resource definition from the business logic makes pipelines testable. As long as the
APIs of the resources agree, and the fundamental operations they expose are tested in each
environment, we can test business logic independent of environments that may be very costly or
difficult to test against.

```python literalinclude showLines startLine=77 caption=modes.py
file:/docs_snippets/docs_snippets/intro_tutorial/advanced/pipelines/modes.py
lines:77-86
```

Even if you're not familiar with SQLAlchemy, it's enough to note that this is a very different
implementation of the `warehouse` resource. To make this implementation available to Dagster, we
attach it to a <PyObject module="dagster" object="ModeDefinition" />.

```python literalinclude showLines startLine=130 caption=modes.py
file:/docs_snippets/docs_snippets/intro_tutorial/advanced/pipelines/modes.py
lines:130-145
```

Each of the ways we can invoke a Dagster pipeline lets us select which mode we'd like to run it in.

From the command line, we can set `-d` or `--mode` and select the name of the mode:

```bash
dagster pipeline execute -f modes.py -e resources.yaml -d unittest
```

Or, from the Python API:

```python literalinclude showLines startLine=149 emphasize-lines=12 caption=modes.py
file:/docs_snippets/docs_snippets/intro_tutorial/advanced/pipelines/modes.py
lines:149-161
dedent:4
```

And in Dagit, we can use the "Mode" selector to pick the mode in which we'd like to execute.

![modes.png](/assets/images/tutorial/modes.png)

The config editor is Dagit is mode-aware, so when you switch modes and introduce a resource that
requires additional config, the editor will prompt you.

<br />

## Pipeline Config Presets

Useful as the Dagit config editor and the ability to stitch together YAML fragments is, once
pipelines have been have been deployed and config is unlikely to change, it's often useful to
distribute pipelines with embedded config. For example, you might point solids at different S3
buckets in different environments, or want to pull database credentials from different environment
variables.

Dagster calls this a config preset:

```python literalinclude showLines startLine=131 emphasize-lines=14-40 caption=presets.py
file:/docs_snippets/docs_snippets/intro_tutorial/advanced/pipelines/presets.py
lines:131-172
```

The config above illustrates two ways of defining a preset.

The first is to pass an `run_config` literal to the constructor. Because this dict is defined in
Python, you can do arbitrary computation to construct it&mdash;for instance, picking up environment
variables, making a call to a secrets store like Hashicorp Vault, etc.

The second is to use the `from_files` static constructor, and pass a list of file globs from which
to read YAML fragments. Order matters in this case, and keys from later files will overwrite keys
from earlier files.

To select a preset for execution, we can use the CLI, the Python API, or Dagit. From the CLI, use
`-p` or `--preset`:

```bash
dagster pipeline execute -f presets.py --preset unittest
```

From Python, you can use <PyObject module="dagster" object="execute_pipeline" />

```python literalinclude showLines startLine=177 caption=presets.py
file:/docs_snippets/docs_snippets/intro_tutorial/advanced/pipelines/presets.py
lines:177
dedent:4
```

And in Dagit, we can use the "Presets" selector.

![presets.png](/assets/images/tutorial/presets.png)

<br />

<br />
