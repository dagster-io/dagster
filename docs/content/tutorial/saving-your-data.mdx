---
title: "Tutorial, part six: Saving your data | Dagster Docs"
description: Learn how to use I/O managers to save your data.
---

# Tutorial, part six: Saving your data

You've written a data pipeline using assets that runs every hour. Currently, your assets are stored in a temporary folder that is deleted after closing the `dagster` process. **Note**: If you set up Dagster without the tutorial, you may have set a `DAGSTER_HOME` environment variable. If so, then your assets will be stored in that directory.

You likely want to store your assets in places such as tables in your database or files in cloud storage. To do this, Dagster has I/O managers that enable reading and writing data to storage systems.

By the end of this section, you will:

- Store your files in a file system
- Store your data frames as tables in a database
- Learn about I/O managers

---

## What are I/O managers?

I/O stands for **input** and **output.** I/O managers are Dagster objects that control how Dagster reads and writes data to specific external services, such as Snowflake or Amazon Web Services (AWS) S3.

They manage **input** by reading an asset from where itâ€™s stored and loading it into memory to be used by a dependent asset. For example, `topstories_word_cloud` needs `topstories`. Therefore, the I/O manager that `topstories` uses will load the data into memory so the `topstories_word_cloud` asset can use it.

I/O managers control \*_output_ by writing the asset's to the location configured. By the end of this section, youâ€™ll configure an I/O manager to write the `hackernews_topstories` DataFrame into a database as a table.

---

## Step 1: Writing files to storage

Now, youâ€™ll write your files to a more permanent location. For this tutorial, youâ€™ll be writing to another directory in your file system by using the `fs_io_manager`, which is bundled with the core `dagster` package.

In `__init__.py`, add the following snippet anywhere above the `def = Definitions(...)` line:

```python file=/tutorial/saving/add_fs_io_manager.py startafter=start_imports_and_definitions endbefore=end_imports_and_definitions
from dagster import (
    AssetSelection,
    Definitions,
    ScheduleDefinition,
    define_asset_job,
    load_assets_from_modules,
    fs_io_manager,  # Update the imports at the top of the file to also include this
)

hackernews_job = define_asset_job("hackernews_job", selection=AssetSelection.all())

hackernews_schedule = ScheduleDefinition(
    job=hackernews_job, cron_schedule="0 * * * *"  # every hour
)

io_manager = fs_io_manager.configured(
    {
        "base_dir": "data",  # Path is built relative to where `dagster dev` is run
    }
)
```

Next, update your `Definitions` (shown below) to attach the I/O manager to your code location by defining it as a **resource**.

```python file=/tutorial/saving/add_fs_io_manager.py startafter=start_update_defs endbefore=end_update_defs
defs = Definitions(
    assets=all_assets,
    schedules=[hackernews_schedule],
    resources={
        "io_manager": io_manager,
    },
)
```

<Note>
  <strong>About resources</strong>
  <br />
  I/O managers are a special type of resource, a Dagster concept for{" "}
  <strong>connecting</strong> to external services. I/O managers are resources
  made specifically to control reading and writing to external services. Refer
  to the <a href="/concepts/resources">Resources documentation</a> for more
  information.
</Note>

To see the effect of this change, reload your code location in the Dagster UI and materialize your assets. When you click on an asset, youâ€™ll see the `path` metadata pointing to the `data` directory on your computer.

Look at your Dagster project in your file system (via terminal, Finder, or Windows Explorer) and youâ€™ll find a new directory called `data` that contains your materialized assets.

This is a short example of the value of I/O managers. In a few lines of code, assets previously stored in ephemeral memory can now be saved to many locations. With I/O managers, you can save your assets to places like AWS S3, Google Cloud Storage, or Azure Blob Storage.

In addition to file storage systems, I/O managers can connect to any service from which data can be read or written, such as databases.

---

## Step 2: Writing to databases

In data engineering, a common asset is tabular data. These are often seen as database tables or DataFrames in-memory. In Dagster, I/O managers define the relationship between how data is saved as a table in the database and what in-memory data format to represent the data as.

Dagster includes out-of-the-box I/O managers for common databases, such as DuckDB. When a DataFrame is returned in an assetâ€™s definition, it is translated into a database-compatible format. The I/O manager then runs a SQL query to write the data into the database.

Letâ€™s modify our pipeline to store our `topstories` DataFrame as a table in DuckDB. Weâ€™ll use Dagsterâ€™s out-of-the-box I/O manager for DuckDB, an easy-to-set-up database that runs on your computer without any setup on your end.

### Setting up I/O managers

Adding an I/O manager for a database is similar to connecting it to file storage. The biggest difference is you must specify what **type** of DataFrame to use when the data is loaded into Dagsterâ€™s memory. In this tutorial, youâ€™ll use DuckDB to store the data and Pandas DataFrames when transforming with the data.

Update your `__init__.py` to reflect the changes below:

```python file=/tutorial/saving/add_db_io_manager.py startafter=start_imports_and_definitions endbefore=end_imports_and_definitions
from dagster_duckdb_pandas import duckdb_pandas_io_manager

# Add the imports to the top
# These imports let you define how Dagster communicates with DuckDB

# Insert this section anywhere above your `defs = Definitions(...)`
database_io_manager = duckdb_pandas_io_manager.configured(
    {"database": "analytics.hackernews"}
)

# Update your Definitions
defs = Definitions(
    assets=all_assets,
    schedules=[hackernews_schedule],
    resources={
        "io_manager": io_manager,
        "database_io_manager": database_io_manager,  # Define the I/O manager here
    },
)
```

There are three changes in this snippet:

- `duckdb_pandas_io_manager` is imported
- A DuckDB+Pandas I/O manager is defined
- The new I/O manager is added as a resource under the key `database_io_manager`

### Choosing an I/O manager for each asset

You now have two I/O managers configured within a code location. By default, all assets will use the I/O manager under the `io_manager` key. Because you updated the `io_manager` key, the `fs_io_manager` could plug and play without any additional changes. Overriding and priority are given to the I/O manager key that is most specific to the asset.

To have the `topstories` asset store its data in DuckDB, modify its asset function in `assets.py` by specifying the assetâ€™s `io_manager_key` to be `"database_io_manager"`, as shown below:

```python file=/tutorial/saving/assets.py startafter=start_update_asset endbefore=end_update_asset
@asset(
    group_name="hackernews",
    io_manager_key="database_io_manager",  # Addition: `io_manager_key` specified
)
def topstories(topstory_ids):
    logger = get_dagster_logger()

    results = []
    for item_id in topstory_ids:
        item = requests.get(
            f"https://hacker-news.firebaseio.com/v0/item/{item_id}.json"
        ).json()
        results.append(item)

        if len(results) % 20 == 0:
            logger.info(f"Got {len(results)} items so far.")

    df = pd.DataFrame(results).drop(["kids"], axis=1)

    return Output(
        value=df,
        metadata={
            "num_records": len(df),
            "preview": MetadataValue.md(df.head().to_markdown()),
        },
    )
```

To validate that this worked, materialize your entire asset graph. The I/O manager should store the DataFrame in a table called `analytics.hackernews.topstories` and continue to store the IDs and word cloud in a directory called `data`.

Despite the changes in where the data is stored, the developer experience hasnâ€™t changed. I/O managers deal with reading/writing data from storage for you so you can focus on the core logic and computation.

---

## Next steps

Congratulations! ðŸŽ‰ You've written your first pipeline in Dagster!

This section of the tutorial shows some of the simplest possible uses of I/O managers. However, this is the tip of the iceberg of what can be done with I/O managers.

Once youâ€™ve outgrown being able to load all your data into memory in one chunk, you should learn about [partitions](/concepts/partitions-schedules-sensors/partitions) and how they are used to load only slices of your data into memory.

Assets donâ€™t need to use I/O managers. Sometimes itâ€™s impossible to separate business logic from I/O, or youâ€™ll never want to load the data into memory. In situations like these, youâ€™re able to not return any data, and dependencies can be defined without loading data in the dependent assetâ€™s function. Refer to the [Assets without I/O](/guides/dagster/non-argument-deps) for more information.

We also briefly mentioned resources. A property of resources is that they can be configured depending on the use case, such as using DuckDB as your data warehouse during development and Databricks during production.

Now that you've completed the tutorial, find out what else you can do in with our [Next steps](/tutorial/next-steps).
