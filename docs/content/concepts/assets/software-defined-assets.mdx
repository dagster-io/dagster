---
title: Software-Defined Assets | Dagster
description: A software-defined asset is a description of how to compute the contents of a particular data asset.
---

# Software-Defined Assets

An **asset** is an object in persistent storage, such as a table, file, or persisted machine learning model. A **software-defined asset** is a Dagster object that couples an asset to the function and upstream assets that are used to produce its contents.

Software-defined assets enable a declarative approach to data management, in which code is the source of truth on what data assets should exist and how those assets are computed.

A software-defined asset includes the following:

- An <PyObject object="AssetKey" />, which is a handle for referring to the asset.
- A set of upstream asset keys, which refer to assets that the contents of the software-defined asset are derived from.
- An [op](/concepts/ops-jobs-graphs/ops), which is a function responsible for computing the contents of the asset from its upstream dependencies.

  **Note**: A crucial distinction between software-defined assets and [ops](/concepts/ops-jobs-graphs/ops) is that software-defined assets know about their dependencies, while ops do not. Ops aren't connected to dependencies until they're placed inside a [graph](/concepts/ops-jobs-graphs/jobs-graphs).

**Materializing** an asset is the act of running its op and saving the results to persistent storage. You can initiate materializations from [Dagit](/concepts/dagit/dagit) or by invoking Python APIs. By default, assets are materialized to pickle files on your local filesystem, but materialization behavior is [fully customizable](#customizing-how-assets-are-materialized-with-io-managers). It's possible to materialize an asset in multiple storage environments, such as production and staging.

---

## Relevant APIs

| Name                                  | Description                                                                                                                                                                                                                                           |
| ------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| <PyObject object="asset" decorator /> | A decorator used to define assets.                                                                                                                                                                                                                    |
| <PyObject object="AssetGroup" />      | A group of software-defined assets.                                                                                                                                                                                                                   |
| <PyObject object="SourceAsset" />     | A class that describes an asset, but doesn't define how to compute it. Within an <PyObject object="AssetGroup" />, <PyObject object="SourceAsset" />s are used to represent assets that other assets depend on, but can't be materialized themselves. |

---

## Defining assets

### A basic software-defined asset

The easiest way to create a software-defined asset is with the <PyObject object="asset" decorator /> decorator.

```python file=/concepts/assets/basic_asset_definition.py
from dagster import asset


@asset
def my_asset():
    return [1, 2, 3]
```

By default, the name of the decorated function, `my_asset`, is used as the asset key. The decorated function forms the asset's op: it's responsible for producing the asset's contents. This asset doesn't depend on any other assets.

### Assets with dependencies

Software-defined assets can depend on other software-defined assets. The easiest way to define an asset dependency is to include an upstream asset name as an argument to the decorated function. In the following example, `downstream_asset` depends on `upstream_asset`. That means that the contents of `upstream_asset` are provided to the function that computes the contents of `downstream_asset`.

```python file=/concepts/assets/asset_dependency.py startafter=start_marker endbefore=end_marker
@asset
def upstream_asset():
    return [1, 2, 3]


@asset
def downstream_asset(upstream_asset):
    return upstream_asset + [4]
```

The [explicit dependencies](#explicit-dependencies) example covers an alternative way to specify asset dependencies without needing to match argument names to upstream asset names.

#### Non-argument dependencies

Alternatively, you can define dependencies where data from an upstream asset doesn’t need to be loaded by Dagster to compute the output of a downstream asset. When used, `non_argument_deps` defines the dependency between assets but doesn’t pass data through Dagster.

Consider the following example:

1. `upstream_asset` creates a new table (`sugary_cereals`) by selecting records from the `cereals` table
2. `downstream_asset` then creates a new table (`shopping_list`) by selecting records from `sugary_cereals`

```python file=/concepts/assets/non_argument_deps.py startafter=start_marker endbefore=end_marker
from dagster import asset


@asset
def upstream_asset():
    execute_query("CREATE TABLE sugary_cereals AS SELECT * FROM cereals")


@asset(non_argument_deps={"upstream_asset"})
def downstream_asset():
    execute_query("CREATE TABLE shopping_list AS SELECT * FROM sugary_cereals")
```

In this example, Dagster doesn’t need to load data from `upstream_asset` to successfully compute the `downstream_asset`. While `downstream_asset` does depend on `upstream_asset`, the key difference with `non_argument_deps` is that data isn’t being passed between the functions. Specifically, the data from the `sugary_cereals` table isn't being passed as an argument to `downstream_asset`.

### Asset context

Since a software-defined asset contains an op, all the typical functionality of an op - like the use of [resources](/concepts/resources) and [configuration](#asset-configuration) - is available to an asset. Supplying the `context` parameter provides access to system information for the op, for example:

```python file=/concepts/assets/asset_w_context.py startafter=start_w_context endbefore=end_w_context
@asset(required_resource_keys={"api"})
def my_asset(context):
    # fetches contents of an asset
    return context.resources.api.fetch_table("my_asset")
```

### Asset configuration

Like ops, configuration is also supported for assets. Configuration is accessible through the asset context at runtime and can be used to specify behavior. Note that asset configuration behaves the same as configuration for ops.

For example, the following asset queries an API endpoint defined through configuration:

```python file=/concepts/assets/asset_config.py startafter=start_example endbefore=end_example
@asset(config_schema={"api_endpoint": str})
def my_configurable_asset(context):
    api_endpoint = context.op_config["api_endpoint"]
    data = requests.get(f"{api_endpoint}/data").json()
    return data
```

Refer to the [Config schema documentation](/concepts/configuration/config-schema) for more configuration info and examples.

## Combining assets in groups

To materialize assets or load them in Dagit, you first need to combine them into an <PyObject object="AssetGroup" />, which is a set of assets with no unsatisfied dependencies. For example:

```python file=/concepts/assets/asset_group.py startafter=start_marker endbefore=end_marker
from dagster import AssetGroup, asset


@asset
def upstream_asset():
    return [1, 2, 3]


@asset
def downstream_asset(upstream_asset):
    return upstream_asset + [4]


asset_group = AssetGroup([upstream_asset, downstream_asset])
```

To save you from needing to add each asset individually to the group, Dagster provides the <PyObject object="AssetGroup" method="from_package_name" /> method for building <PyObject object="AssetGroup" />s out of all the assets within all Python modules underneath a Python package:

```python file=/concepts/assets/from_package_name.py
from dagster import AssetGroup

asset_group = AssetGroup.from_package_name(
    "docs_snippets.concepts.assets.package_with_assets"
)
```

Once assets are bundled into an asset group, you can:

- [View them in Dagit](#viewing-assets-in-dagit).
- [Materialize an ad-hoc set of them in Dagit](#materializing-assets-in-dagit).
- [Build a job](#building-jobs-that-materialize-assets), which materializes a fixed selection of the assets in the group, and can be put on a schedule or sensor.

## Viewing and materializing assets in Dagit

### Loading assets into Dagit

To view and materialize assets in Dagit, you can point it at a module that contains an <PyObject object="AssetGroup" />. E.g.

    dagit -m module_with_asset_group

If you want Dagit to contain both an asset group and a set of [jobs](/concepts/ops-jobs-graphs/jobs-graphs) that target the assets, you can place the <PyObject object="AssetGroup" /> and jobs together inside a [repository](/concepts/repositories-workspaces/repositories).

### Viewing assets in Dagit

- [All assets](#all-assets)
- [Details for an asset](#asset-details)
- [Dependency graph](#dependency-graph)
- [Upstream changes](#upstream-changed-indicator)

#### All assets

To view a list of all your assets, click **Assets** in the top-right corner of the page. This opens the Asset Catalog:

<img
alt="Asset Catalog"
src="/images/concepts/assets/software-defined-assets/catalog.png"
/>

#### Asset details

View the Asset Details page for an asset by clicking on its name:

<img
alt="Asset Details"
src="/images/concepts/assets/software-defined-assets/details.png"
/>

#### Dependency graph

To view a graph of all assets and their dependencies, you can:

- Click the graph icon to the upper-left of the Asset Catalog
- Click **View in Graph** on any asset

<img
alt="Asset Graph"
src="/images/concepts/assets/software-defined-assets/graph.png"
/>

#### Upstream changed indicator

On occasion, you might see an **upstream changed** indicator on an asset in the dependency graph or on the Asset Details page:

<img
alt="Asset Graph with an upstream changed indicator"
src="/images/concepts/assets/software-defined-assets/upstream-changed.png"
/>

This occurs when a downstream asset's last materialization took place **earlier than the asset it depends on.** Dagit displays this alert to notify you that the contents of an asset may be stale. For example:

- `comments` is upstream of `comment_stories`
- `comment_stories` depends on `comments`
- `comment_stories` was last materialized on February 25 at **5:30PM**
- `comments` was last materialized on February 25 at **7:05PM**

In this case, the contents of `comment_stories` may be outdated, as the most recent data from `comments` wasn't used to compute them.

You can resolve this issue by re-materializing the downstream asset. This will re-compute the contents with the most recent data/changes to its upstream dependency.

<Note>
  Currently, the <strong>upstream changed</strong> indicator won't display in
  the following scenarios:
  <ul>
    <li>The upstream asset is in another asset group or job</li>
    <li>The assets are partitioned</li>
  </ul>
</Note>

### Materializing assets in Dagit

There are a couple ways in Dagit to launch a run that materializes assets:

- Navigate to the Asset Details Page for the asset and click the "Materialize" button in the upper right corner.
- In the graph view of the Asset Catalog page, click the "Materialize" button in the upper right corner. You can click on assets to collect a subset to materialize.

## Source assets - representing assets that are generated somewhere else

It's common for software-defined assets to depend on assets that are generated somewhere else. For example, your data warehouse might contain a set of tables that another team is responsible for ingesting, but that your assets are derived from. To allow the assets in your <PyObject object="AssetGroup" /> to depend on them, you can include <PyObject object="SourceAsset" />s that represent them.

For example:

```python file=/concepts/assets/source_asset.py startafter=start_marker endbefore=end_marker
from dagster import AssetGroup, AssetKey, SourceAsset, asset

my_source_asset = SourceAsset(key=AssetKey("my_source_asset"))


@asset
def my_derived_asset(my_source_asset):
    return my_source_asset + [4]


asset_group = AssetGroup(assets=[my_derived_asset], source_assets=[my_source_asset])
```

In any situation where you might use a source asset, you could also have the code inside the asset's op load the data. However, using source assets has a few advantages:

- It allows Dagit to show asset lineage that includes the source assets. If a different <PyObject object="AssetGroup" /> in the same [workspace](/concepts/repositories-workspaces/workspaces) contains an asset definition with the same asset key as a <PyObject object="SourceAsset" />, Dagit can represent the asset lineage across those groups.
- If you've already factored your data-loading code into an <PyObject object="IOManager" />, Dagster can use it to load the contents of your source asset.
- It allows you to write your asset dependencies in the same way, independent of whether they're downstream from a source asset or a derived asset. This makes it easy to swap out a source asset for a derived asset and vice versa.

### Customizing how assets are materialized with IO managers

By default, materializing an asset will pickle it to a local file named "my_asset", in a temporary directory. You can specify this directory by providing a value for the `local_artifact_storage` property in your dagster.yaml file.

[IO managers](/concepts/io-management/io-managers) enable fully overriding this behavior and storing asset contents in any way you wish - e.g. writing them as tables in a database or as objects in a cloud object store. Dagster also provides built-in IO managers that pickle assets to AWS S3 (<PyObject module="dagster_aws.s3" object="s3_pickle_io_manager" />), Azure Blob Storage (<PyObject module="dagster_azure.adls2" object="adls2_pickle_io_manager" />), and GCS (<PyObject module="dagster_gcp.gcs" object="gcs_pickle_io_manager" />), or you can write your own.

To apply an IO manager to a set of assets, you can include it with them in an <PyObject object="AssetGroup" />.

```python file=/concepts/assets/asset_io_manager.py startafter=start_marker endbefore=end_marker
from dagster_aws.s3 import s3_pickle_io_manager, s3_resource

from dagster import AssetGroup, asset


@asset
def upstream_asset():
    return [1, 2, 3]


@asset
def downstream_asset(upstream_asset):
    return upstream_asset + [4]


asset_group = AssetGroup(
    [upstream_asset, downstream_asset],
    resource_defs={"io_manager": s3_pickle_io_manager, "s3": s3_resource},
)
```

This example also includes `"s3": s3_resource`, because the `s3_pickle_io_manager` depends on an s3 resource.

When `upstream_asset` is materialized, the value `[1, 2, 3]` will be will be pickled and stored in an object on S3. When `downstream_asset` is materialized, the value of `upstream_asset` will be read from S3 and depickled, and `[1, 2, 3, 4]` will be pickled and stored in a different object on S3.

Different assets can have different IO managers:

```python file=/concepts/assets/asset_different_io_managers.py startafter=start_marker endbefore=end_marker
from dagster_aws.s3 import s3_pickle_io_manager, s3_resource

from dagster import AssetGroup, asset, fs_io_manager


@asset(io_manager_key="s3_io_manager")
def upstream_asset():
    return [1, 2, 3]


@asset(io_manager_key="fs_io_manager")
def downstream_asset(upstream_asset):
    return upstream_asset + [4]


asset_group = AssetGroup(
    [upstream_asset, downstream_asset],
    resource_defs={
        "s3_io_manager": s3_pickle_io_manager,
        "s3": s3_resource,
        "fs_io_manager": fs_io_manager,
    },
)
```

When `upstream_asset` is materialized, the value `[1, 2, 3]` will be will be pickled and stored in an object on S3. When `downstream_asset` is materialized, the value of `upstream_asset` will be read from S3 and depickled, and `[1, 2, 3, 4]` will be pickled and stored in a file on the local filesystem.

The same assets can be bound to different resources and IO managers in different environments. For example, for local development, you might want to store assets on your local filesystem while, in production, you might want to store the assets in S3.

```python file=/concepts/assets/asset_io_manager_prod_local.py startafter=start_marker endbefore=end_marker
from dagster_aws.s3 import s3_pickle_io_manager, s3_resource

from dagster import AssetGroup, asset, fs_io_manager


@asset
def upstream_asset():
    return [1, 2, 3]


@asset
def downstream_asset(upstream_asset):
    return upstream_asset + [4]


prod_asset_group = AssetGroup(
    [upstream_asset, downstream_asset],
    resource_defs={"io_manager": s3_pickle_io_manager, "s3": s3_resource},
)

local_asset_group = AssetGroup(
    [upstream_asset, downstream_asset],
    resource_defs={"io_manager": fs_io_manager},
)
```

## Building jobs that materialize assets

You can define a job that materializes a fixed selection of the assets in an <PyObject object="AssetGroup" /> each time it runs.

The following snippet shows examples of three different jobs you might create out of an <PyObject object="AssetGroup" />.

```python file=/concepts/assets/build_job.py startafter=start_marker endbefore=end_marker
from dagster import AssetGroup, asset


@asset
def upstream():
    return [1, 2, 3]


@asset
def downstream_1(upstream):
    return upstream + [4]


@asset
def downstream_2(upstream):
    return len(upstream)


asset_group = AssetGroup([upstream, downstream_1, downstream_2])

all_assets = asset_group.build_job(name="my_asset_job")

downstream_assets = asset_group.build_job(
    name="my_asset_job", selection=["upstream", "downstream_1"]
)

upstream_and_downstream_1 = asset_group.build_job(
    name="my_asset_job", selection="*downstream_1"
)
```

Multiple jobs within the same repository can target overlapping sets of assets.

Like regular jobs, asset jobs can be placed on [schedules](/concepts/partitions-schedules-sensors/schedules) and [sensors](/concepts/partitions-schedules-sensors/sensors).

## Testing

When writing unit tests, you can treat the function decorated by `@asset` as a regular python function.

Consider a simple asset with no upstream dependencies:

```python file=/concepts/assets/asset_testing.py startafter=start_simple_asset endbefore=end_simple_asset
@asset
def my_simple_asset():
    return [1, 2, 3]
```

When writing a unit test, you can directly invoke the decorated function.

```python file=/concepts/assets/asset_testing.py startafter=start_test_simple_asset endbefore=end_test_simple_asset
def test_my_simple_asset():
    result = my_simple_asset()
    assert result == [1, 2, 3]
```

If you have an asset with upstream dependencies:

```python file=/concepts/assets/asset_testing.py startafter=start_more_complex_asset endbefore=end_more_complex_asset
@asset
def more_complex_asset(my_simple_asset):
    return my_simple_asset + [4, 5, 6]
```

You can manually provide values for those dependencies in your unit test. This allows you to test assets in isolation from one another.

```python file=/concepts/assets/asset_testing.py startafter=start_test_more_complex_asset endbefore=end_test_more_complex_asset
def test_more_complex_asset():
    result = more_complex_asset([0])
    assert result == [0, 4, 5, 6]
```

If you use a context object in your function, `@asset` will provide the correct context during execution. When writing a unit test, you can mock it with <PyObject object="build_op_context" />. You can use <PyObject object="build_op_context" /> to generate the `context` object because under the hood the function decorated by `@asset` is an op.

Consider this asset that uses a resource:

```python file=/concepts/assets/asset_testing.py startafter=start_with_context_asset endbefore=end_with_context_asset
@asset
def uses_context(context):
    return context.resources.foo
```

When writing a unit test, use <PyObject object="build_op_context" /> to mock the `context` and provide values for testing.

```python file=/concepts/assets/asset_testing.py startafter=start_test_with_context_asset endbefore=end_test_with_context_asset
def test_uses_context():
    context = build_op_context(resources={"foo": "bar"})
    result = uses_context(context)
    assert result == "bar"
```

## Examples

### Multi-component asset keys

Assets are often objects in systems with hierarchical namespaces, like filesystems. Because of this, it often makes sense for an asset key to be a list of strings, instead of just a single string.

```python file=/concepts/assets/multi_component_asset_key.py startafter=start_marker endbefore=end_marker
from dagster import AssetIn, asset


@asset(namespace=["one", "two", "three"])
def upstream_asset():
    return [1, 2, 3]


@asset(ins={"upstream_asset": AssetIn(namespace=["one", "two", "three"])})
def downstream_asset(upstream_asset):
    return upstream_asset + [4]
```

### Explicit dependencies

If defining dependencies by matching argument names to upstream asset names feels too magical for your tastes, you can also define dependencies in a more explicit way:

```python file=/concepts/assets/explicit_asset_dependency.py
from dagster import AssetIn, asset


@asset
def upstream_asset():
    return [1, 2, 3]


@asset(ins={"upstream": AssetIn("upstream_asset")})
def downstream_asset(upstream):
    return upstream + [4]
```

In this case, `ins={"upstream": AssetIn("upstream_asset")}` declares that the contents of the asset with the key `upstream_asset` will be provided to the function argument named `upstream`.

### Attaching metadata

Dagster supports attaching arbitrary [metadata](/\_apidocs/ops#dagster.MetadataEntry) to assets. To attach metadata, supply a `metadata` dictionary to the asset:

```python file=/concepts/assets/asset_metadata.py startafter=start_example endbefore=end_example
@asset(metadata={"cereal_name": "Sugar Sprinkles"})
def cereal_asset():
    return 5
```

Asset metadata can be viewed in Dagit on the [Asset Detail page](/concepts/dagit/dagit#asset-details).

## Further Reading

Interested in learning more about software-defined assets and working through a more complex example? Check out our [guide on software-defined assets](/guides/dagster/software-defined-assets) and our [example project](https://github.com/dagster-io/dagster/tree/master/examples/modern_data_stack_assets) that integrates software-defined assets with other Modern Data Stack tools.

## See it in action

For more examples of software-defined assets, check out the following in our [SDA Hacker News example](https://github.com/dagster-io/dagster/tree/master/examples/hacker_news_assets):

- [Defining an asset](https://github.com/dagster-io/dagster/blob/master/examples/hacker_news_assets/hacker_news_assets/activity_analytics/assets/activity_forecast.py)
- [Loading assets from dbt](https://github.com/dagster-io/dagster/blob/master/examples/hacker_news_assets/hacker_news_assets/activity_analytics/\__init\_\_.py)
- [Per-asset IO manager](https://github.com/dagster-io/dagster/blob/master/examples/hacker_news_assets/hacker_news_assets/core/assets/items.py)
- [Partitioned assets](https://github.com/dagster-io/dagster/blob/master/examples/hacker_news_assets/hacker_news_assets/core/assets/items.py)
- [AssetGroups](https://github.com/dagster-io/dagster/blob/master/examples/hacker_news_assets/hacker_news_assets/core/\__init\_\_.py)

Our [Modern Data Stack example](https://github.com/dagster-io/dagster/tree/master/examples/modern_data_stack_assets) also covers:

- [Defining assets](https://github.com/dagster-io/dagster/blob/master/examples/modern_data_stack_assets/modern_data_stack_assets/assets.py)
- [Loading assets from dbt](https://github.com/dagster-io/dagster/blob/master/examples/modern_data_stack_assets/modern_data_stack_assets/assets.py)
- [Loading assets from Airbyte](https://github.com/dagster-io/dagster/blob/master/examples/modern_data_stack_assets/modern_data_stack_assets/assets.py)

Our [Bollinger example](https://github.com/dagster-io/dagster/tree/master/examples/bollinger) also covers software-defined assets
