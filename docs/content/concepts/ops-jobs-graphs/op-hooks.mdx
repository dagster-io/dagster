---
title: Op Hooks | Dagster
description: Op hooks let you define success and failure handling policies on ops.
---

# Op Hooks

Op hooks let you define success and failure handling policies on ops.

## Relevant APIs

| Name                                                          | Description                                                                                                                |
| ------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------- |
| <PyObject module="dagster" object="failure_hook" decorator /> | The decorator to define a callback on op failure.                                                                          |
| <PyObject module="dagster" object="success_hook" decorator /> | The decorator to define a callback on op success.                                                                          |
| <PyObject module="dagster" object="HookContext"  />           | The context object available to a hook function.                                                                           |
| <PyObject module="dagster" object="build_hook_context" />     | A function for building a <PyObject object="HookContext" /> outside of execution, intended to be used when testing a hook. |

## Overview

A <PyObject module="dagster" object="success_hook" displayText="@success_hook" /> or <PyObject module="dagster" object="failure_hook" displayText="@failure_hook" /> decorated function is called an op hook. Op hooks are designed for generic purposes â€” it can be anything you would like to do at a per op level.

---

## Defining a Op Hook

```python file=/concepts/solids_pipelines/op_hooks.py startafter=start_repo_marker_0 endbefore=end_repo_marker_0
from dagster import HookContext, failure_hook, success_hook


@success_hook(required_resource_keys={"slack"})
def slack_message_on_success(context: HookContext):
    message = f"Op {context.op.name} finished successfully"
    context.resources.slack.chat.post_message(channel="#foo", text=message)


@failure_hook(required_resource_keys={"slack"})
def slack_message_on_failure(context: HookContext):
    message = f"Op {context.op.name} failed"
    context.resources.slack.chat.post_message(channel="#foo", text=message)
```

### Hook context

As you may have noticed, the hook function takes one argument, which is an instance of <PyObject module="dagster" object="HookContext" />. The available properties on this context are:

- `context.job_name`: the name of the job where the hook is triggered.
- `context.log`: loggers
- `context.hook_def`: the hook that the context object belongs to.
- `context.op`: the op associated with the hook.
- `context.op_config`: The config specific to the associated op.
- `context.op_exception`: The thrown exception in the associated failed op.
- `context.op_output_values`: The computed output values of the associated op.
- `context.step_key`: the key for the step where the hook is triggered.
- `context.resources`: the resources the hook can use.
- `context.required_resource_keys`: the resources required by this hook.

## Using Hooks

Dagster provides different ways to trigger op hooks.

### Applying a hook on every op in a job

For example, you want to send a slack message to a channel when any op fails in a job. In this case, we will be applying a hook on a job, which will apply the hook on every op instance within in that job.

The <PyObject module="dagster" object="job" displayText="@job" /> decorator accepts `hooks` as a parameter. Likewise, when creating a job from a graph, hooks are also accepted as a parameter in the <PyObject object="GraphDefinition" method="to_job" /> function. In the below example, we can pass the `slack_message_on_failure` hook above in a set as a parameter to <PyObject module="dagster" object="job" displayText="@job" />. Then, slack messages will be sent when any op in the job fails.

```python file=/concepts/solids_pipelines/op_hooks.py startafter=start_repo_marker_1 endbefore=end_repo_marker_1
@job(resource_defs={"slack": slack_resource}, hooks={slack_message_on_failure})
def notif_all():
    # the hook "slack_message_on_failure" is applied on every op instance within this graph
    a()
    b()
```

### Applying a hook on an op

Sometimes a job is a shared responsibility or you only want to be alerted on high-priority op executions. So we also provide a way to set up hooks on op instances which enables you to apply policies on a per-op basis.

```python file=/concepts/solids_pipelines/op_hooks.py startafter=start_repo_marker_2 endbefore=end_repo_marker_2
@job(resource_defs={"slack": slack_resource})
def selective_notif():
    # only op "a" triggers hooks: a slack message will be sent when it fails or succeeds
    a.with_hooks({slack_message_on_failure, slack_message_on_success})()
    # op "b" won't trigger any hooks
    b()
```

In this case, op "b" won't trigger any hooks, while when op "a" fails or succeeds it will send a slack message.

## Testing Hooks

You can test the functionality of a hook by invoking the hook definition. This will run the underlying decorated function. You can construct a context to provide to the invocation using the <PyObject object="build_hook_context" /> function.

```python file=/concepts/solids_pipelines/op_hooks.py startafter=start_testing_hooks endbefore=end_testing_hooks
from dagster import build_hook_context


@success_hook(required_resource_keys={"my_conn"})
def my_success_hook(context):
    context.resources.my_conn.send("foo")


def test_my_success_hook():
    my_conn = mock.MagicMock()
    # construct HookContext with mocked ``my_conn`` resource.
    context = build_hook_context(resources={"my_conn": my_conn})

    my_success_hook(context)

    assert my_conn.send.call_count == 1
```

## Examples

### Accessing failure information in a failure hook

In many cases, you might want to know details about an op failure. You can get the exception object thrown in the failed op via the `op_exception` property on <PyObject module="dagster" object="HookContext" />:

```python file=/concepts/solids_pipelines/op_hooks_context.py startafter=start_failure_hook_op_exception endbefore=end_failure_hook_op_exception
from dagster import HookContext, failure_hook
import traceback


@failure_hook
def my_failure_hook(context: HookContext):
    op_exception: BaseException = context.op_exception
    # print stack trace of exception
    traceback.print_tb(op_exception.__traceback__)
```

## Patterns

### <Check/> Environment-specific hooks using jobs

Hooks use resource keys to access resources. After including the resource key in its set of `required_resource_keys`, the body of the hook can access the corresponding resource via the `resources` attribute of its context object.

It also enables you to switch resource values in different jobs so that, for example, you can send slack messages only while executing a production job and mock the slack resource while testing.

Because executing a production job and a testing job share the same core of business logic, we can build these jobs from a shared [graph](/concepts/ops-jobs-graphs/jobs-graphs#from-a-graph). In the <PyObject object="GraphDefinition" method="to_job" /> method, which builds a job from a graph, you can specify environment-specific hooks and resources.

In this case, we can mock the `slack_resource` using a helper function <PyObject module="dagster" object="ResourceDefinition" displayText="ResourceDefinition.hardcoded_resource()"/>, so it won't send slack messages during development.

```python file=/concepts/solids_pipelines/op_hooks.py startafter=start_repo_marker_3 endbefore=end_repo_marker_3
@graph
def slack_notif_all():
    a()
    b()


notif_all_prod = slack_notif_all.to_job(
    name="notif_all_prod",
    resource_defs={
        "slack": ResourceDefinition.hardcoded_resource(
            slack_resource_mock, "do not send messages in dev"
        )
    },
    hooks={slack_message_on_failure},
)

notif_all_dev = slack_notif_all.to_job(
    name="notif_all_dev",
    resource_defs={"slack": slack_resource},
    hooks={slack_message_on_failure},
)
```

When we switch to production, we can provide the real slack token in the `run_config` and therefore enable sending messages to a certain slack channel when a hook is triggered.

```yaml file=/concepts/solids_pipelines/prod_op_hooks.yaml
resources:
  slack:
    config:
      token: "xoxp-1234123412341234-12341234-1234" # replace with your slack token
```

Then, we can execute a job with the config through Python API, CLI, or the Dagit UI. Here's an example of using the Python API.

```python file=/concepts/solids_pipelines/op_hooks.py startafter=start_repo_main endbefore=end_repo_main
if __name__ == "__main__":
    with open(
        file_relative_path(__file__, "prod_op_hooks.yaml"),
        "r",
        encoding="utf8",
    ) as fd:
        run_config = yaml.safe_load(fd.read())
    result = notif_all_dev.execute_in_process(
        run_config=run_config, raise_on_error=False
    )
```

### <Cross/> Job-level hooks

When you add a hook to a job, the hook will be added to every op in the job individually. The hook does not track job-scoped events and only tracks op-level success or failure events.

You may find the need to set up job-level policies. For example, you may want to run some code for every job failure.

Dagster provides a way to create a sensor that reacts to job failure events. You can find details at [Job failure sensor](/concepts/partitions-schedules-sensors/sensors#job-failure-sensor) on the [Sensors](/concepts/partitions-schedules-sensors/sensors) page.
