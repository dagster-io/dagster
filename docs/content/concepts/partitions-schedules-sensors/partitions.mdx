---
title: Partitioned assets and jobs | Dagster
description: Partitioned assets and jobs enable launching backfills, where each partition processes a subset of data.
---

# Partitioned assets and jobs

A software-defined asset can represent a collection of _partitions_ that can be tracked and materialized independently. In many ways, each partition functions like its own mini-asset, but they all share a common materialization function and dependencies. Typically, each partition will correspond to a separate file, or a slice of a table in a database.

A common use is for each partition to represent all the records in a data set that fall within a particular time window, e.g. hourly, daily or monthly. Alternatively, each partition can represent a region, a customer, an experiment - any dimension along which you want to be able to materialize and monitor independently. An asset can also be partitioned along multiple dimensions, e.g. by region and by hour.

A graph of assets with the same partitions implicitly forms a partitioned data pipeline, and you can launch a run that selects multiple assets and materializes the same partition in each asset.

Similarly, a _partitioned job_ is a job where each run corresponds to a partition. It's common to construct a partitioned job that materializes a single partition across a set of partitioned assets every time it runs.

Having defined a partitioned asset or job, you can:

- View runs by partition in Dagit.
- Define a [schedule](/concepts/partitions-schedules-sensors/schedules) that fills in a partition each time it runs. For example, a job might run each day and process the data that arrived during the previous day.
- Launch [backfills](/concepts/partitions-schedules-sensors/backfills), which are sets of runs that each process a different partition. For example, after making a code change, you might want to run your job on all partitions instead of just one of them.

---

## Partitioned assets

- [Relevant APIs](#relevant-apis)
- [Defining partitioned assets](#defining-partitioned-assets)
- [Viewing asset partition status](#viewing-asset-partition-status)
- [Materializing partitioned assets](#materializing-partitioned-assets)
- [Partition dependencies](#partition-dependencies)

### Relevant APIs

| Name                                              | Description                                                                       |
| ------------------------------------------------- | --------------------------------------------------------------------------------- |
| <PyObject object="PartitionsDefinition" />        | Superclass - defines the set of partitions that can be materialized for an asset. |
| <PyObject object="HourlyPartitionsDefinition" />  | A partitions definition with a partition for each hour.                           |
| <PyObject object="DailyPartitionsDefinition" />   | A partitions definition with a partition for each day.                            |
| <PyObject object="WeeklyPartitionsDefinition" />  | A partitions definition with a partition for each week.                           |
| <PyObject object="MonthlyPartitionsDefinition" /> | A partitions definition with a partition for each month.                          |
| <PyObject object="StaticPartitionsDefinition" />  | A partitions definition with a fixed set of partitions.                           |
| <PyObject object="MultiPartitionsDefinition" />   | A partitions definition with multiple dimensions.                                 |
| <PyObject object="MultiPartitionKey" />           | A multi-dimensional partition key.                                                |
| <PyObject object="DynamicPartitionsDefinition" /> | A partitions definition whose partitions can be dynamically added and removed.    |

A software-defined asset can be assigned a <PyObject object="PartitionsDefinition" />, which determines the set of partitions that compose it. If the asset is stored in a filesystem or an object store, then each partition will typically correspond to a file or object. If the asset is stored in a database, then each partition will typically correspond to a range of values in a table that fall within a particular window.

Once an asset has a set of partitions, you can launch materializations of individual partitions and view the materialization history by partition in Dagit.

### Defining partitioned assets

- [Partitioned assets with partitioned I/O managers](#partitioned-assets-with-partitioned-io-managers)
- [Multi-dimensionally partitioned assets](#multi-dimensionally-partitioned-assets)

For example, below is a software-defined asset with a partition for each day since the first day of 2022. Materializing partition `2022-07-23` of this asset would result in fetching data from the URL `coolweatherwebsite.com/weather_obs\&date=2022-07-23` and storing it at the path `weather_observations/2022-07-23.csv`.

```python file=/concepts/partitions_schedules_sensors/partitioned_asset.py
import urllib.request

from dagster import DailyPartitionsDefinition, asset


@asset(partitions_def=DailyPartitionsDefinition(start_date="2022-01-01"))
def my_daily_partitioned_asset(context) -> None:
    partition_date_str = context.asset_partition_key_for_output()

    url = f"coolweatherwebsite.com/weather_obs&date={partition_date_str}"
    target_location = f"weather_observations/{partition_date_str}.csv"

    urllib.request.urlretrieve(url, target_location)
```

#### Partitioned assets with partitioned I/O managers

In the above code snippet, the body of the decorated function writes out data to a file, but it's common to delegate this I/O to an [I/O manager](/concepts/io-management/io-managers). Dagster's [built-in I/O managers](/concepts/io-management/io-managers#built-in-io-managers) know how to handle partitioned assets. You can also handle them when writing your own I/O manager, following the instructions [here](/concepts/io-management/io-managers#handling-partitioned-assets).

Here's a software-defined asset that relies on an I/O manager to store its output:

```python file=/concepts/partitions_schedules_sensors/partitioned_asset_uses_io_manager.py
import pandas as pd

from dagster import DailyPartitionsDefinition, asset


@asset(partitions_def=DailyPartitionsDefinition(start_date="2022-01-01"))
def my_daily_partitioned_asset(context) -> pd.DataFrame:
    partition_date_str = context.asset_partition_key_for_output()
    return pd.read_csv(f"coolweatherwebsite.com/weather_obs&date={partition_date_str}")
```

If you're using the default I/O manager, materializing partition `2022-07-23` of this asset would store the output `DataFrame` in a pickle file at a path like `my_daily_partitioned_asset/2022-07-23`.

#### Multi-dimensionally partitioned assets

The <PyObject object="MultiPartitionsDefinition" /> class accepts a mapping of dimension name to partitions definition, creating a partition for each unique combination of dimension partitions. For example, the asset below would contain a partition for each combination of color and date: `red|2022-01-01`, `yellow|2022-01-01`, `blue|2022-01-01`, `red|2022-01-02` and so on.

```python file=/concepts/partitions_schedules_sensors/multipartitions_asset.py startafter=start_multi_partitions_marker endbefore=end_multi_partitions_marker
from dagster import (
    DailyPartitionsDefinition,
    MultiPartitionsDefinition,
    StaticPartitionsDefinition,
    asset,
)


@asset(
    partitions_def=MultiPartitionsDefinition(
        {
            "date": DailyPartitionsDefinition(start_date="2022-01-01"),
            "color": StaticPartitionsDefinition(["red", "yellow", "blue"]),
        }
    )
)
def multi_partitions_asset(context):
    context.log.info(context.partition_key.keys_by_dimension)
```

<Note>
  Currently, Dagster only allows two-dimensional multipartitions definitions.
</Note>

Notice the code snippet above fetches the partition key from the asset context. Multi-dimensional partition keys are returned as <PyObject object="MultiPartitionKey"/> objects, which contain a <PyObject object="MultiPartitionKey" method="keys_by_dimension"/> method that returns the key per dimension. This object can also be passed into partition key execution parameters:

```python file=/concepts/partitions_schedules_sensors/multipartitions_asset.py startafter=start_multi_partitions_key_marker endbefore=end_multi_partitions_key_marker
from dagster import MultiPartitionKey, materialize

result = materialize(
    [multi_partitions_asset],
    partition_key=MultiPartitionKey({"date": "2022-01-01", "color": "red"}),
)
```

#### Dynamically partitioned assets

<Experimental />

<Note>
  This feature is <strong>experimental</strong>. Until this feature is marked as
  stable, further migrations may be required and partition history is not
  guaranteed to be retained.
</Note>

Sometimes you don't know the set of partitions ahead of time when you're defining your assets. For example, maybe you want to add a new partition every time a new data file lands in a directory, or every time you want to experiment with a new set of hyperparameters. In these cases, you can use a <PyObject object="DynamicPartitionsDefinition"/>.

The <PyObject object="DynamicPartitionsDefinition" /> class accepts a name argument, representing the name of the partition set:

```python file=/concepts/partitions_schedules_sensors/dynamic_partitioned_asset.py startafter=start_dynamic_partitions_marker endbefore=end_dynamic_partitions_marker
images_partitions_def = DynamicPartitionsDefinition(name="images")


@asset(partitions_def=images_partitions_def)
def images(context):
    ...
```

For a given dynamic partition set, partition keys can be added and removed. One common pattern is detecting the presence of a new partition through a [sensor](/concepts/partitions-schedules-sensors/sensors), adding the partition, and then triggering a run for that partition:

```python file=/concepts/partitions_schedules_sensors/dynamic_partitioned_asset.py startafter=start_dynamic_partitions_2 endbefore=end_dynamic_partitions_2
images_job = define_asset_job(
    "images_job", AssetSelection.keys("images"), partitions_def=images_partitions_def
)


@sensor(job=images_job)
def image_sensor(context):
    new_images = [
        img_filename
        for img_filename in os.listdir(os.getenv("MY_DIRECTORY"))
        if not context.instance.has_dynamic_partition(
            images_partitions_def.name, img_filename
        )
    ]

    return SensorResult(
        run_requests=[
            RunRequest(partition_key=img_filename) for img_filename in new_images
        ],
        dynamic_partitions_requests=[
            images_partitions_def.build_add_request(new_images)
        ],
    )
```

The [dynamic partitions example](https://github.com/dagster-io/dagster/tree/master/examples/assets_dynamic_partitions) contains a full project that uses dynamic partitions.

### Viewing asset partition status

To view all partitions for an asset, open the **Definition** tab of the asset's details page. The bar in the **Partitions** section represents all of the partitions for the asset.

In the following image, the partitions bar is entirely gray. This is because none of the partitions have been materialized:

<Image
src="/images/concepts/partitions-schedules-sensors/partitions/partitioned-asset.png"
width={2662}
height={1618}
/>

### Materializing partitioned assets

When you materialize a partitioned asset, you choose which partitions to materialize, and Dagster will launch a run for each partition.

**Note**: If you choose more than one partition, the [Dagster daemon](/deployment/dagster-daemon) needs to be running to queue the multiple runs.

<Image
src="/images/concepts/partitions-schedules-sensors/partitions/rematerialize-partition.png"
width={2662}
height={1618}
/>

After you've materialized a partition, it will show up as green in the partitions bar:

<Image
src="/images/concepts/partitions-schedules-sensors/partitions/materialized-partitioned-asset.png"
width={2662}
height={1618}
/>

To view materializations by partition, navigate to the **Activity** tab:

<Image
src="/images/concepts/partitions-schedules-sensors/partitions/materialized-partitioned-asset-activity.png"
width={2662}
height={1618}
/>

### Partition dependencies

When a partitioned asset depends on another partitioned asset, each partition in the downstream asset depends on a partition or multiple partitions in the upstream asset.

A few rules govern default partition-to-partition dependencies:

- When the upstream asset and downstream asset have the same <PyObject object="PartitionsDefinition" />, each partition in the downstream asset depends on the same partition in the upstream asset.
- When the upstream asset and downstream asset are both time window-partitioned, each partition in the downstream asset depends on all partitions in the upstream asset that intersect its time window.

  For example, if an asset with a <PyObject object="DailyPartitionsDefinition" /> depends on an asset with an <PyObject object="HourlyPartitionsDefinition" />, then partition `2022-04-12` of the daily asset the would depend on 24 partitions of the hourly asset: `2022-04-12-00:00` through `2022-04-12-23:00`.

#### PartitionMappings

You can override the default partition dependency rules by providing a <PyObject object="PartitionMapping" /> when specifying a dependency on an asset. For example, here's how to specify that each partition of a daily-partitioned asset depends on the prior day's partition in an upstream asset:

```python file=/concepts/partitions_schedules_sensors/partition_mapping.py
from dagster import (
    AssetIn,
    DailyPartitionsDefinition,
    TimeWindowPartitionMapping,
    asset,
)

partitions_def = DailyPartitionsDefinition(start_date="2023-01-21")


@asset(partitions_def=partitions_def)
def events():
    ...


@asset(
    partitions_def=partitions_def,
    ins={
        "events": AssetIn(
            partition_mapping=TimeWindowPartitionMapping(
                start_offset=-1, end_offset=-1
            ),
        )
    },
)
def yesterday_event_stats(events):
    ...
```

Refer to the [API docs](https://docs.dagster.io/\_apidocs/partitions#partition-mapping) for a list of available `PartitionMappings`.

---

## Partitioned asset jobs

A partitioned asset job is a job that materializes a particular set of partitioned assets every time it runs.

```python file=/concepts/partitions_schedules_sensors/partitioned_asset_job.py
from dagster import (
    AssetSelection,
    Definitions,
    HourlyPartitionsDefinition,
    asset,
    define_asset_job,
)

hourly_partitions_def = HourlyPartitionsDefinition(start_date="2022-05-31-00:00")


@asset(partitions_def=hourly_partitions_def)
def asset1():
    ...


@asset(partitions_def=hourly_partitions_def)
def asset2():
    ...


partitioned_asset_job = define_asset_job(
    name="asset_1_and_2_job",
    selection=AssetSelection.assets(asset1, asset2),
    partitions_def=hourly_partitions_def,
)


defs = Definitions(
    assets=[asset1, asset2],
    jobs=[partitioned_asset_job],
)
```

---

## Partitioned non-asset jobs

- [Relevant APIs](#relevant-apis-1)
- [Defining a job with time partitions](#defining-a-job-with-time-partitions)
- [Defining a job with static partitions](#defining-a-job-with-static-partitions)
- [Creating schedules from partitioned jobs](#creating-schedules-from-partitioned-jobs)

### Relevant APIs

| Name                                                       | Description                                                                                         |
| ---------------------------------------------------------- | --------------------------------------------------------------------------------------------------- |
| <PyObject object="PartitionedConfig" />                    | Determines a set of partitions and how to generate run config for a partition.                      |
| <PyObject object="daily_partitioned_config" decorator />   | Decorator for constructing partitioned config where each partition is a date.                       |
| <PyObject object="hourly_partitioned_config" decorator />  | Decorator for constructing partitioned config where each partition is an hour of a date.            |
| <PyObject object="weekly_partitioned_config" decorator />  | Decorator for constructing partitioned config where each partition is a week.                       |
| <PyObject object="monthly_partitioned_config" decorator /> | Decorator for constructing partitioned config where each partition is a month.                      |
| <PyObject object="static_partitioned_config" decorator />  | Decorator for constructing partitioned config for a static set of partition keys.                   |
| <PyObject object="dynamic_partitioned_config" decorator /> | Decorator for constructing partitioned config for a set of partition keys that can grow over time.  |
| <PyObject object="build_schedule_from_partitioned_job" />  | A function that constructs a schedule whose interval matches the partitioning of a partitioned job. |

When defining a job that doesn't use software-defined assets, you can make it partitioned by supplying <PyObject object="PartitionedConfig" /> object as its config.

### Defining a job with time partitions

The most common kind of partitioned job is a time-partitioned job - each partition is a time window, and each run for a partition processes data within that time window.

- [Non-partitioned job with date config](#non-partitioned-job-with-date-config)
- [Date-partitioned job](#date-partitioned-job)
- [Dagit partitions tab](#dagit-partitions-tab)

#### Non-partitioned job with date config

Before we define a partitioned job, let's look at a non-partitioned job that computes some data for a given date:

```python file=/concepts/partitions_schedules_sensors/date_config_job.py
from dagster import Config, job, op


class ProcessDateConfig(Config):
    date: str


@op
def process_data_for_date(context, config: ProcessDateConfig):
    date = config.date
    context.log.info(f"processing data for {date}")


@job
def do_stuff():
    process_data_for_date()
```

It takes, as config, a string `date`. This piece of config defines which date to compute data for. For example, if you wanted to compute for May 5th, 2020, you would execute the graph with the following config:

```python file=/concepts/partitions_schedules_sensors/config.yaml
graph:
  process_data_for_date:
    config:
      date: "2020-05-05"
```

#### Date-partitioned job

With the job above, it's possible to supply any value for the `date` param, which means that, if you wanted to launch a backfill, Dagster wouldn't know what values to run it on. You can instead build a partitioned job that operates on a defined set of dates.

First, you define the <PyObject object="PartitionedConfig"/>. In this case, because each partition is a date, you can use the <PyObject object="daily_partitioned_config" decorator /> decorator. It defines the full set of partitions - every date between the start date and the current date, as well as how to determine the run config for a given partition.

```python file=/concepts/partitions_schedules_sensors/partitioned_job.py startafter=start_partitioned_config endbefore=end_partitioned_config
from dagster import daily_partitioned_config, RunConfig
from datetime import datetime


@daily_partitioned_config(start_date=datetime(2020, 1, 1))
def my_partitioned_config(start: datetime, _end: datetime):
    return RunConfig(
        ops={
            "process_data_for_date": ProcessDataForDateConfig(
                date=start.strftime("%Y-%m-%d")
            )
        }
    )
```

Then you can build a job that uses the `PartitionedConfig` by supplying it to the `config` argument when you construct the job:

```python file=/concepts/partitions_schedules_sensors/partitioned_job.py startafter=start_partitioned_job endbefore=end_partitioned_job
@job(config=my_partitioned_config)
def do_stuff_partitioned():
    process_data_for_date()
```

#### Dagit partitions tab

In Dagit, you can view runs by partition in the **Partitions tab** of a **Job** page:

<Image
alt="Partitions Tab"
src="/images/concepts/partitions-schedules-sensors/partitions/partitioned-job.png"
width={2662}
height={1618}
/>

In the "Run Matrix", each column corresponds to one of the partitions in the job. The time listed corresponds to the start time of the partition. Each row corresponds to one of the steps in the job. You can click on an individual box to navigate to logs and run information for the step.

You can view and use partitions in the Dagit Launchpad tab for a job. In the top bar, you can select from the list of all available partitions. Within the config editor, the config for the selected partition will be populated.

In the screenshot below, we select the `2020-01-02` partition, and we can see that the run config for the partition has been populated in the editor.

<Image
alt="Partitions in Dagit Launchpad"
src="/images/concepts/partitions-schedules-sensors/partitions/launchpad.png"
width={2662}
height={1618}
/>

In addition to the <PyObject object="daily_partitioned_config" decorator /> decorator, Dagster also provides <PyObject object="monthly_partitioned_config" decorator />, <PyObject object="weekly_partitioned_config" decorator />, <PyObject object="hourly_partitioned_config" decorator />. See the API docs for each of these decorators for more information on how partitions are built based on different `start_date`, `minute_offset`, `hour_offset`, and `day_offset` inputs.

### Defining a job with static partitions

Not all jobs are partitioned by time. Here's a partitioned job where the partitions are continents:

```python file=/concepts/partitions_schedules_sensors/static_partitioned_job.py
from dagster import Config, RunConfig, job, op, static_partitioned_config

CONTINENTS = [
    "Africa",
    "Antarctica",
    "Asia",
    "Europe",
    "North America",
    "Oceania",
    "South America",
]


@static_partitioned_config(partition_keys=CONTINENTS)
def continent_config(partition_key: str):
    return RunConfig(
        ops={"continent_op": ContinentOpConfig(continent_name=partition_key)}
    )


class ContinentOpConfig(Config):
    continent_name: str


@op
def continent_op(context, config: ContinentOpConfig):
    context.log.info(config.continent_name)


@job(config=continent_config)
def continent_job():
    continent_op()
```

### Creating schedules from partitioned jobs

It's common that, when you have a partitioned job, you want to run it on a schedule. For example, if your job has a partition for each date, you likely want to run that job every day, on the partition for that day.

The <PyObject object="build_schedule_from_partitioned_job"/> function allows you to construct a schedule from a date partitioned job. It creates a schedule with an interval that matches the spacing of your partition. If you wanted to create a schedule for `do_stuff_partitioned` job defined above, you could write:

```python file=/concepts/partitions_schedules_sensors/schedule_from_partitions.py startafter=start_marker endbefore=end_marker
from dagster import build_schedule_from_partitioned_job, job


@job(config=my_partitioned_config)
def do_stuff_partitioned():
    ...


do_stuff_partitioned_schedule = build_schedule_from_partitioned_job(
    do_stuff_partitioned,
)
```

Schedules can also be made from static partitioned jobs. If you wanted to make a schedule for the `continent_job` above that runs each partition, you could write:

```python file=/concepts/partitions_schedules_sensors/schedule_from_partitions.py startafter=start_static_partition endbefore=end_static_partition
from dagster import schedule, RunRequest


@schedule(cron_schedule="0 0 * * *", job=continent_job)
def continent_schedule():
    for c in CONTINENTS:
        yield RunRequest(run_key=c, partition_key=c)
```

Or a schedule that will run a subselection of the partition

```python file=/concepts/partitions_schedules_sensors/schedule_from_partitions.py startafter=start_single_partition endbefore=end_single_partition
@schedule(cron_schedule="0 0 * * *", job=continent_job)
def antarctica_schedule():
    return RunRequest(partition_key="Antarctica")
```

Refer to the [Schedules documentation](/concepts/partitions-schedules-sensors/schedules#schedules-from-partitioned-assets-and-jobs) for more info about constructing both schedule types.

---

## Testing

- [Testing partitioned config](#testing-partitioned-config)
- [Testing partitioned jobs](#testing-partitioned-jobs)

### Testing partitioned config

Invoking a <PyObject object="PartitionedConfig" /> object will directly invoke the decorated function.

If you want to check whether the generated run config is valid for the config of job, you can use the <PyObject object="validate_run_config" /> function.

```python file=/concepts/partitions_schedules_sensors/partitioned_config_test.py startafter=start_partition_config endbefore=end_partition_config
from dagster import validate_run_config, daily_partitioned_config
from datetime import datetime


@daily_partitioned_config(start_date=datetime(2020, 1, 1))
def my_partitioned_config(start: datetime, _end: datetime):
    return RunConfig(
        ops={
            "process_data_for_date": ProcessDataForDateConfig(
                date=start.strftime("%Y-%m-%d")
            )
        }
    )


def test_my_partitioned_config():
    # assert that the decorated function returns the expected output
    run_config = my_partitioned_config(datetime(2020, 1, 3), datetime(2020, 1, 4))
    assert run_config.to_config_dict() == {
        "ops": {"process_data_for_date": {"config": {"date": "2020-01-03"}}}
    }

    # assert that the output of the decorated function is valid configuration for the
    # do_stuff_partitioned job
    assert validate_run_config(do_stuff_partitioned, run_config)
```

If you want to test that your <PyObject object="PartitionedConfig" /> creates the partitions you expect, you can use the `get_partition_keys` or `get_run_config_for_partition_key` functions.

```python file=/concepts/partitions_schedules_sensors/partitioned_config_test.py startafter=start_partition_keys endbefore=end_partition_keys
from dagster import Config, RunConfig


@daily_partitioned_config(start_date=datetime(2020, 1, 1), minute_offset=15)
def my_offset_partitioned_config(start: datetime, _end: datetime):
    return RunConfig(
        ops={
            "process_data": ProcessDataConfig(
                start=start.strftime("%Y-%m-%d-%H:%M"),
                end=_end.strftime("%Y-%m-%d-%H:%M"),
            )
        }
    )


class ProcessDataConfig(Config):
    start: str
    end: str


@op
def process_data(context, config: ProcessDataConfig):
    s = config.start
    e = config.end
    context.log.info(f"processing data for {s} - {e}")


@job(config=my_offset_partitioned_config)
def do_more_stuff_partitioned():
    process_data()


def test_my_offset_partitioned_config():
    # test that the partition keys are what you expect
    keys = my_offset_partitioned_config.get_partition_keys()
    assert keys[0] == "2020-01-01"
    assert keys[1] == "2020-01-02"

    # test that the run_config for a partition is valid for do_stuff_partitioned
    run_config = my_offset_partitioned_config.get_run_config_for_partition_key(keys[0])
    assert validate_run_config(do_more_stuff_partitioned, run_config)

    # test that the contents of run_config are what you expect
    assert run_config.to_config_dict() == {
        "ops": {
            "process_data": {
                "config": {"start": "2020-01-01-00:15", "end": "2020-01-02-00:15"}
            }
        }
    }
```

### Testing partitioned jobs

To run a partitioned job in-process on a particular partition, you can supply a value for the `partition_key` argument of <PyObject object="JobDefinition" method="execute_in_process" />

```python file=/concepts/partitions_schedules_sensors/partitioned_job_test.py startafter=start endbefore=end
def test_do_stuff_partitioned():
    assert do_stuff_partitioned.execute_in_process(partition_key="2020-01-01").success
```

---

## See it in action

For more examples of partitions, check out the following in our [Hacker News example](https://github.com/dagster-io/dagster/tree/master/examples/project_fully_featured):

- [Defining partitioned assets](https://github.com/dagster-io/dagster/blob/master/examples/project_fully_featured/project_fully_featured/assets/core/items.py)
- [Defining a partitioned asset job and a schedule based on it](https://github.com/dagster-io/dagster/blob/master/examples/project_fully_featured/project_fully_featured/jobs.py)
