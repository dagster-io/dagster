---
title: "Using Dagster with Airbyte"
description: Integrate your Airbyte connections into Dagster.
---

# Using Airbyte with Dagster

<Note>
  Using Airbyte Cloud? Check out the{" "}
  <a href="/integrations/airbyte-cloud">Airbyte Cloud with Dagster guide</a>!
</Note>

Dagster can orchestrate your Airbyte connections, making it easy to chain an Airbyte sync with upstream or downstream steps in your workflow.

This guide focuses on how to work with Airbyte connections using Dagster's [software-defined asset (SDA)](/concepts/assets/software-defined-assets) framework.

<center>
  <Image
    alt="Screenshot of the Airbyte UI in a browser, showing the connection ID in the URL."
    src="/images/integrations/airbyte/airbyte_assets.png"
    width={2200}
    height={1300}
  />
</center>

---

## Airbyte connections and Dagster software-defined assets

An [Airbyte connection](https://docs.airbyte.com/understanding-airbyte/connections/) defines a series of data streams which are synced between a source and a destination. During a sync, a replica of the data from each data stream is written to the destination, typically as one or more tables. Dagster represents each of the replicas generated in the destination as a software-defined asset. This enables you to easily:

- Visualize the streams involved in an Airbyte connection and execute a sync from Dagster
- Define downstream computations which depend on replicas produced by Airbyte
- Track historical metadata and logs for each data stream
- Track data lineage through Airbyte and other tools

---

## Prerequisites

To get started, you will need to install the `dagster` and `dagster-airbyte` Python packages:

```bash
pip install dagster dagster-airbyte
```

You'll also want to have an Airbyte instance running. If you don't have one already, you can [run Airbyte locally using `docker-compose`](https://docs.airbyte.com/deploying-airbyte/local-deployment#setup--launch-airbyte).

---

## Step 1: Connecting to Airbyte

The first step in using Airbyte with Dagster is to tell Dagster how to connect to your Airbyte instance using an Airbyte [resource](/concepts/resources). This resource contains information on where the Airbyte instance is located and any credentials needed to access it.

```python startafter=start_define_instance endbefore=end_define_instance file=/integrations/airbyte/airbyte.py dedent=4
from dagster import EnvVar
from dagster_airbyte import AirbyteResource

airbyte_instance = AirbyteResource(
    host="localhost",
    port="8000",
    # If using basic auth, include username and password:
    username="airbyte",
    password=EnvVar("AIRBYTE_PASSWORD"),
)
```

If you're running Airbyte [locally using `docker-compose`](https://docs.airbyte.com/deploying-airbyte/local-deployment#setup--launch-airbyte), the `host` and `port` parameters should be set to `localhost` and `8000`, respectively. The default basic auth credentials are a username `airbyte` and password `password`.

If you're hosting Airbyte externally, you'll need to provide a hostname where the Airbyte webapp and API are accssible, typically on port `80`. For more information on the configuration options available for the Airbyte resource, see the [API reference](/\_apidocs/libraries/dagster-airbyte#dagster_airbyte.airbyte_resource).

---

## Step 2: Loading Airbyte asset definitions into Dagster

The easiest way to get started using Airbyte with Dagster is to have Dagster automatically generate asset defintions from your Airbyte project. This can be done in one of two ways:

- [Load asset definitions from an Airbyte instance via API](#loading-airbyte-asset-definitions-from-an-airbyte-instance), at initialization time
- [Load asset definitions from Airbyte YAML configuration files](#loading-airbyte-asset-definitions-from-yaml-config), generated by Airbyte's [Octavia CLI](https://github.com/airbytehq/airbyte/tree/master/octavia-cli#what-is-octavia-cli)

You can also [manually-build asset definitions](#manually-building-airbyte-asset-definitions) on a per-connection basis.

<TabGroup>

<TabItem name="Loading from Airbyte instance">

### Loading Airbyte asset definitions from an Airbyte instance

To load Airbyte assets into Dagster from a live Airbyte instance, you will need to supply the Airbyte resource that we defined above in step 1. Here, the Airbyte instance is treated as the source of truth.

```python startafter=start_load_assets_from_airbyte_instance endbefore=end_load_assets_from_airbyte_instance file=/integrations/airbyte/airbyte.py dedent=4
from dagster_airbyte import load_assets_from_airbyte_instance

# Use the airbyte_instance resource we defined in Step 1
airbyte_assets = load_assets_from_airbyte_instance(airbyte_instance)
```

The `load_assets_from_airbyte_instance` function retrieves all of the connections you have defined in the Airbyte interface, creating software-defined assets for each data stream. Each connection has an associated [op](https://docs.dagster.io/concepts/ops-jobs-graphs/ops#ops) which triggers a sync of that connection.

</TabItem>

<TabItem name="Loading from YAML config">

### Loading Airbyte asset definitions from YAML config

To load Airbyte assets into Dagster from a set of YAML configuration files, specify the Octavia project directory, which contains the `sources`, `destinations`, and `connections` subfolders. This is the directory where you first ran `octavia init`. Here, the YAML files are treated as the source of truth for building Dagster assets.

```python startafter=start_load_assets_from_airbyte_project endbefore=end_load_assets_from_airbyte_project file=/integrations/airbyte/airbyte.py dedent=4
from dagster_airbyte import load_assets_from_airbyte_project

airbyte_assets = load_assets_from_airbyte_project(
    project_dir="path/to/airbyte/project",
)
```

The `load_assets_from_airbyte_project` function parses the YAML metadata, generating a set of software-defined assets which reflect each of the data streams synced by your connections. Each connection has an associated [op](https://docs.dagster.io/concepts/ops-jobs-graphs/ops#ops) which triggers a sync of that connection.

#### Adding a resource

Assets loaded from Airbyte require an `AirbyteResource`, which defines how to connect and interact with your Airbyte instance.

We can add the Airbyte resource we configured above to our Airbyte assets by doing the following:

```python startafter=start_airbyte_project_config endbefore=end_airbyte_project_config file=/integrations/airbyte/airbyte.py dedent=4
from dagster_airbyte import load_assets_from_airbyte_project

from dagster import with_resources

# Use the airbyte_instance resource we defined in Step 1
airbyte_assets = with_resources(
    [load_assets_from_airbyte_project(project_dir="path/to/airbyte/project")],
    {"airbyte": airbyte_instance},
)
```

</TabItem>
<TabItem name="Manually building assets">

### Manually building Airbyte asset definitions

Instead of having Dagster automatically create the asset defintions for your Airbyte instance, you can opt to individually build them. First, determine the connection IDs for each of the connections you would like to build assets for. The connection ID can be seen in the URL of the connection page when viewing the Airbyte UI.

<center>
  <Image
    alt="Screenshot of the Airbyte UI in a browser, showing the connection ID in the URL."
    src="/images/integrations/airbyte/airbyte_connection_ui.png"
    width={3102}
    height={1746}
  />
</center>

Then, supply the connection ID and the list of tables which the connection creates in the destination to `build_airbyte_assets`:

```python startafter=start_manually_define_airbyte_assets endbefore=end_manually_define_airbyte_assets file=/integrations/airbyte/airbyte.py dedent=4
from dagster_airbyte import build_airbyte_assets

airbyte_assets = build_airbyte_assets(
    connection_id="87b7fe85-a22c-420e-8d74-b30e7ede77df",
    destination_tables=["releases", "tags", "teams", "stargazers"],
)
```

#### Adding a resource

Manually built Airbyte assets require an `AirbyteResource`, which defines how to connect and interact with your Airbyte instance.

We can add the Airbyte resource we configured above to our Airbyte assets by doing the following:

```python startafter=start_airbyte_manual_config endbefore=end_airbyte_manual_config file=/integrations/airbyte/airbyte.py dedent=4
from dagster_airbyte import build_airbyte_assets, AirbyteResource

from dagster import with_resources

airbyte_instance = AirbyteResource(
    host="localhost",
    port="8000",
)
airbyte_assets = with_resources(
    build_airbyte_assets(
        connection_id="87b7fe85-a22c-420e-8d74-b30e7ede77df",
        destination_tables=["releases", "tags", "teams", "stargazers"],
    ),
    # Use the airbyte_instance resource we defined in Step 1
    {"airbyte": airbyte_instance},
)
```

</TabItem>
</TabGroup>

---

## Step 3: Adding downstream assets

<Note>
  Looking to orchestrate Airbyte with dbt? Check out our{" "}
  <a href="https://github.com/dagster-io/dagster/tree/master/examples/assets_modern_data_stack">
    Modern Data Stack example
  </a>{" "}
  and our <a href="/integrations/dbt">dbt integration docs</a>.
</Note>

Once you have loaded your Airbyte assets into Dagster, you can create assets which depend on them. These can be other assets pulled in from external sources such as [dbt](/integrations/dbt) or assets defined in Python code.

<TabGroup>

<TabItem name="With an I/O manager">

In this case, we have an Airbyte connection that stores data in the `stargazers` table in our Snowflake warehouse. We specify the output [I/O manager](/concepts/io-management/io-managers) to tell downstream assets how to retrieve the data.

```python startafter=start_add_downstream_assets endbefore=end_add_downstream_assets file=/integrations/airbyte/airbyte.py dedent=8
import json
from dagster import (
    AssetSelection,
    Definitions,
    asset,
    define_asset_job,
)
from dagster_airbyte import load_assets_from_airbyte_instance, AirbyteResource
from dagster_snowflake_pandas import SnowflakePandasIOManager
import pandas as pd

airbyte_instance = AirbyteResource(
    host="localhost",
    port="8000",
)

airbyte_assets = load_assets_from_airbyte_instance(
    airbyte_instance,
    io_manager_key="snowflake_io_manager",
)

@asset
def stargazers_file(stargazers: pd.DataFrame):
    with open("stargazers.json", "w", encoding="utf8") as f:
        f.write(json.dumps(stargazers.to_json(), indent=2))

# only run the airbyte syncs necessary to materialize stargazers_file
my_upstream_job = define_asset_job(
    "my_upstream_job",
    AssetSelection.keys("stargazers_file")
    .upstream()  # all upstream assets (in this case, just the stargazers Airbyte asset)
    .required_multi_asset_neighbors(),  # all Airbyte assets linked to the same connection
)

defs = Definitions(
    jobs=[my_upstream_job],
    assets=[airbyte_assets, stargazers_file],
    resources={"snowflake_io_manager": SnowflakePandasIOManager(...)},
)
```

</TabItem>

<TabItem name="Without an I/O manager">

In this case, we have an Airbyte connection that stores data in the `stargazers` table in our Snowflake warehouse. Since we are not using an I/O manager to fetch the data in downstream assets, we will use `deps` to define dependencies. Then within the downstream asset, we can fetch the data if necessary or launch other commands that work with data in external processes.

```python startafter=start_with_deps_add_downstream_assets endbefore=end_with_deps_add_downstream_assets file=/integrations/airbyte/airbyte.py dedent=8
import json
from dagster import (
    AssetSelection,
    AssetKey,
    Definitions,
    asset,
    define_asset_job,
)
from dagster_airbyte import load_assets_from_airbyte_instance, AirbyteResource
from dagster_snowflake import SnowflakeResource

airbyte_instance = AirbyteResource(
    host="localhost",
    port="8000",
)

airbyte_assets = load_assets_from_airbyte_instance(
    airbyte_instance,
)

@asset(deps=[AssetKey("stargazers")])
def stargazers_file(snowflake: SnowflakeResource):
    with snowflake.get_connection() as conn:
        stargazers = conn.cursor.execute(
            "SELECT * FROM STARGAZERS"
        ).fetch_pandas_all()
    with open("stargazers.json", "w", encoding="utf8") as f:
        f.write(json.dumps(stargazers.to_json(), indent=2))

# only run the airbyte syncs necessary to materialize stargazers_file
my_upstream_job = define_asset_job(
    "my_upstream_job",
    AssetSelection.keys("stargazers_file")
    .upstream()  # all upstream assets (in this case, just the stargazers Airbyte asset)
    .required_multi_asset_neighbors(),  # all Airbyte assets linked to the same connection
)

defs = Definitions(
    jobs=[my_upstream_job],
    assets=[airbyte_assets, stargazers_file],
    resources={"snowflake": SnowflakeResource(...)},
)
```

</TabItem>
</TabGroup>

---

## Step 4: Scheduling Airbyte syncs

Once you have Airbyte assets, you can define a job that runs some or all of these assets on a schedule, triggering the underlying Airbyte sync.

```python startafter=start_schedule_assets endbefore=end_schedule_assets file=/integrations/airbyte/airbyte.py dedent=4
airbyte_instance = AirbyteResource(
    host="localhost",
    port="8000",
)
airbyte_assets = load_assets_from_airbyte_instance(airbyte_instance)

from dagster import (
    ScheduleDefinition,
    define_asset_job,
    AssetSelection,
    Definitions,
)

# materialize all assets
run_everything_job = define_asset_job("run_everything", selection="*")

# only run my_airbyte_connection and downstream assets
my_etl_job = define_asset_job(
    "my_etl_job", AssetSelection.groups("my_airbyte_connection").downstream()
)

defs = Definitions(
    assets=[airbyte_assets],
    schedules=[
        ScheduleDefinition(
            job=my_etl_job,
            cron_schedule="@daily",
        ),
        ScheduleDefinition(
            job=run_everything_job,
            cron_schedule="@weekly",
        ),
    ],
)
```

Refer to the [Schedule documentation](/concepts/partitions-schedules-sensors/schedules#running-the-scheduler) for more info on running jobs on a schedule.

---

## Conclusion

If you find a bug or want to add a feature to the `dagster-airbyte` library, we invite you to [contribute](/community/contributing).

If you have questions on using Airbyte with Dagster, we'd love to hear from you:

<p align="center">
  <a href="https://dagster.io/slack" target="_blank">
    <Image
      alt="join-us-on-slack"
      src="/assets/join-us-on-slack.png"
      width="160"
      height="40"
    />
  </a>
</p>

---

## Related

<ArticleList>
  <ArticleListItem
    href="/\_apidocs/libraries/dagster-airbyte"
    title="dagster-airbyte API reference"
  ></ArticleListItem>
  <ArticleListItem
    href="/concepts/assets/software-defined-assets"
    title="Software-defined assets"
  ></ArticleListItem>
  <ArticleListItem
    href="/concepts/resources"
    title="Resources"
  ></ArticleListItem>
  <ArticleListItem
    href="/concepts/partitions-schedules-sensors/schedules#running-the-scheduler"
    title="Scheduling Dagster jobs"
  ></ArticleListItem>
</ArticleList>
