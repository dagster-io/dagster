---
title: Executors | Dagster
description: Executors are responsible for executing steps within a job run.
---

# Executors

## Relevant APIs

| Name                                     | Description                                                                                  |
| ---------------------------------------- | -------------------------------------------------------------------------------------------- |
| <PyObject object="executor" decorator /> | The decorator used to define executors. Defines an <PyObject object="ExecutorDefinition" />. |
| <PyObject object="ExecutorDefinition" /> | An executor definition.                                                                      |

## Overview

Executors are responsible for executing steps within a job run. Once a run has launched and the process for the run (the [run worker](/deployment/overview#job-execution-flow)) has been allocated and started, the executor assumes responsibility for execution. Executors can range from single-process serial executors all the way to managing per-step computational resources with a sophisticated control plane.

## Specifying Executors

Every job has an executor. The default executor is the <PyObject object="multi_or_in_process_executor" />, which by default executes each step in its own process, but can be configured to execute each step within the same process.

An executor can be specified directly on the job by supplying an <PyObject object="ExecutorDefinition" /> to the `executor_def` parameter of <PyObject object="job" decorator /> or <PyObject object="GraphDefinition" method="to_job" />.

```python file=/deploying/executors/executors.py startafter=start_executor_on_job endbefore=end_executor_on_job
from dagster import multiprocess_executor, job, graph

# Providing an executor using the job decorator
@job(executor_def=multiprocess_executor)
def the_job():
    ...


@graph
def the_graph():
    ...


# Providing an executor using graph_def.to_job(...)
other_job = the_graph.to_job(executor_def=multiprocess_executor)
```

A default executor can be specified for all jobs and assets provided to a repository using the `default_executor_def` argument of <PyObject object="repository" decorator />. All jobs that don't specify an executor will use this default executor, but if a job explicitly specifies an executor, then the default provided to the repository will not be used.

Executing a job via <PyObject object="JobDefinition" method="execute_in_process" />, overrides the job's executor and uses <PyObject object="in_process_executor" /> instead.

Example executors include:

- <PyObject module="dagster" object="in_process_executor" />: Execution plan
  executes serially within the [run
  worker](/deployment/overview#job-execution-flow) itself.
- <PyObject module="dagster" object="multiprocess_executor" />: Each step
  executes within its own spawned process. Has configurable level of
  parallelism.
- <PyObject module="dagster_dask" object="dask_executor" />: Executes each step
  within a dask task.
- <PyObject module="dagster_celery" object="celery_executor" />: Executes each
  step within a celery task.
- <PyObject module="dagster_docker" object="docker_executor" />: Executes each
  step within a Docker container.
- <PyObject module="dagster_k8s" object="k8s_job_executor" />: Executes each
  step within an ephemeral kubernetes pod.
- <PyObject module="dagster_celery_k8s" object="celery_k8s_job_executor" />:
  Executes each step within a ephemeral kubernetes pod, using celery as a
  control plane for prioritization and queuing.
- <PyObject module="dagster_celery_docker" object="celery_docker_executor" />:
  Executes each step within a Docker container, using celery as a control plane
  for prioritization and queueing.

## Custom Executors

The executor system is pluggable, and it is possible to write your own executor to target a different execution substrate. This is not well-documented, and the internal APIs continue to be in flux.
