---
title: 'azure (dagster-azure)'
title_meta: 'azure (dagster-azure) API Documentation - Build Better Data Pipelines | Python Reference Documentation for Dagster'
description: 'azure (dagster-azure) Dagster API | Comprehensive Python API documentation for Dagster, the data orchestration platform. Learn how to build, test, and maintain data pipelines with our detailed guides and examples.'
last_update:
  date: '2025-12-05'
custom_edit_url: null
---

<div class="section" id="azure-dagster-azure">

# Azure (dagster-azure)

Utilities for using Azure Storage Accounts with Dagster. This is mostly aimed at Azure Data Lake
Storage Gen 2 (ADLS2) but also contains some utilities for Azure Blob Storage.

<div class="section" id="resources">

## Resources

<dl>
    <dt><Link class="anchor" id='dagster_azure.adls2.ADLS2Resource'>dagster_azure.adls2.ADLS2Resource ResourceDefinition <a href='https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-azure/dagster_azure/adls2/resources.py#L68' className='source-link' target='_blank' rel='noopener noreferrer'>[source]</a><a href="#dagster_azure.adls2.ADLS2Resource" class="hash-link"></a></Link></dt>
    <dd>

        <div className='lineblock'> </div>

    Resource containing clients to access Azure Data Lake Storage Gen2.

    Contains a client for both the Data Lake and Blob APIs, to work around the limitations
    of each.

    Example usage:

    Attach this resource to your Definitions to be used by assets and jobs.

        ```python
        from dagster import Definitions, asset, job, op
        from dagster_azure.adls2 import ADLS2Resource, ADLS2SASToken

        @asset
        def asset1(adls2: ADLS2Resource):
            adls2.adls2_client.list_file_systems()
            ...

        @op
        def my_op(adls2: ADLS2Resource):
            adls2.adls2_client.list_file_systems()
            ...

        @job
        def my_job():
            my_op()

        Definitions(
            assets=[asset1],
            jobs=[my_job],
            resources={
                "adls2": ADLS2Resource(
                    storage_account="my-storage-account",
                    credential=ADLS2SASToken(token="my-sas-token"),
                )
            },
        )
        ```
    Attach this resource to your job to make it available to your ops.

        ```python
        from dagster import job, op
        from dagster_azure.adls2 import ADLS2Resource, ADLS2SASToken

        @op
        def my_op(adls2: ADLS2Resource):
            adls2.adls2_client.list_file_systems()
            ...

        @job(
            resource_defs={
                "adls2": ADLS2Resource(
                    storage_account="my-storage-account",
                    credential=ADLS2SASToken(token="my-sas-token"),
                )
            },
        )
        def my_job():
            my_op()
        ```

    </dd>

</dl>
<dl>
    <dt><Link class="anchor" id='dagster_azure.fakes.FakeADLS2Resource'>dagster_azure.fakes.FakeADLS2Resource ResourceDefinition <a href='https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-azure/dagster_azure/fakes/fake_adls2_resource.py#L21' className='source-link' target='_blank' rel='noopener noreferrer'>[source]</a><a href="#dagster_azure.fakes.FakeADLS2Resource" class="hash-link"></a></Link></dt>
    <dd>

        <div className='lineblock'> </div>

    Stateful mock of an ADLS2Resource for testing.

    Wraps a `mock.MagicMock`. Containers are implemented using an in-memory dict.


    </dd>

</dl>
<dl>
    <dt><Link class="anchor" id='dagster_azure.blob.AzureBlobStorageResource'>dagster_azure.blob.AzureBlobStorageResource ResourceDefinition <a href='https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-azure/dagster_azure/blob/resources.py#L44' className='source-link' target='_blank' rel='noopener noreferrer'>[source]</a><a href="#dagster_azure.blob.AzureBlobStorageResource" class="hash-link"></a></Link></dt>
    <dd>

        <div className='lineblock'> </div>

    Resource for interacting with Azure Blob Storage.



    Examples:

        ```python
        import os
        from dagster import Definitions, asset, EnvVar
        from dagster_azure.blob import (
            AzureBlobStorageResource,
            AzureBlobStorageKeyCredential,
            AzureBlobStorageDefaultCredential
        )

        @asset
        def my_table(azure_blob_storage: AzureBlobStorageResource):
            with azure_blob_storage.get_client() as blob_storage_client:
                response = blob_storage_client.list_containers()

        Definitions(
            assets=[my_table],
            resources={
                "azure_blob_storage": AzureBlobStorageResource(
                    account_url=EnvVar("AZURE_BLOB_STORAGE_ACCOUNT_URL"),
                    credential=AzureBlobStorageDefaultCredential() if os.getenv("DEV") else
                        AzureBlobStorageKeyCredential(key=EnvVar("AZURE_BLOB_STORAGE_KEY"))
                ),
            },
        )
        ```

    </dd>

</dl>
<dl>
    <dt><Link class="anchor" id='dagster_azure.blob.AzureBlobComputeLogManager'>`class` dagster_azure.blob.AzureBlobComputeLogManager <a href='https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-azure/dagster_azure/blob/compute_log_manager.py#L38' className='source-link' target='_blank' rel='noopener noreferrer'>[source]</a><a href="#dagster_azure.blob.AzureBlobComputeLogManager" class="hash-link"></a></Link></dt>
    <dd>

    Logs op compute function stdout and stderr to Azure Blob Storage.

    This is also compatible with Azure Data Lake Storage.

    Users should not instantiate this class directly. Instead, use a YAML block in `dagster.yaml`. Examples provided below
    will show how to configure with various credentialing schemes.

    Parameters:
      - <strong>storage_account</strong> (<em>str</em>) – The storage account name to which to log.
      - <strong>container</strong> (<em>str</em>) – The container (or ADLS2 filesystem) to which to log.
      - <strong>secret_credential</strong> (<em>Optional</em><em>[</em><em>dict</em><em>]</em>) – Secret credential for the storage account. This should be a dictionary with keys <cite>client_id</cite>, <cite>client_secret</cite>, and <cite>tenant_id</cite>.
      - <strong>access_key_or_sas_token</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) – Access key or SAS token for the storage account.
      - <strong>default_azure_credential</strong> (<em>Optional</em><em>[</em><em>dict</em><em>]</em>) – Use and configure DefaultAzureCredential. Cannot be used with sas token or secret key config.
      - <strong>local_dir</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) – Path to the local directory in which to stage logs. Default: `dagster_shared.seven.get_system_temp_directory()`.
      - <strong>prefix</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) – Prefix for the log file keys.
      - <strong>upload_interval</strong> (<em>Optional</em><em>[</em><em>int</em><em>]</em>) – Interval in seconds to upload partial log files blob storage. By default, will only upload when the capture is complete.
      - <strong>show_url_only</strong> (<em>bool</em>) – Only show the URL of the log file in the UI, instead of fetching and displaying the full content. Default False.
      - <strong>inst_data</strong> (<em>Optional</em><em>[</em>[*ConfigurableClassData*](../dagster/internals.mdx#dagster._serdes.ConfigurableClassData)<em>]</em>) – Serializable representation of the compute log manager when newed up from config.


    Examples:
    Using an Azure Blob Storage account with an [AzureSecretCredential](https://learn.microsoft.com/en-us/python/api/azure-identity/azure.identity.clientsecretcredential?view=azure-python):

        ```YAML
        compute_logs:
          module: dagster_azure.blob.compute_log_manager
          class: AzureBlobComputeLogManager
          config:
            storage_account: my-storage-account
            container: my-container
            secret_credential:
              client_id: my-client-id
              client_secret: my-client-secret
              tenant_id: my-tenant-id
            prefix: "dagster-test-"
            local_dir: "/tmp/cool"
            upload_interval: 30
            show_url_only: false
        ```
    Using an Azure Blob Storage account with a [DefaultAzureCredential](https://learn.microsoft.com/en-us/python/api/azure-identity/azure.identity.defaultazurecredential?view=azure-python):

        ```YAML
        compute_logs:
          module: dagster_azure.blob.compute_log_manager
          class: AzureBlobComputeLogManager
          config:
            storage_account: my-storage-account
            container: my-container
            default_azure_credential:
              exclude_environment_credential: false
            prefix: "dagster-test-"
            local_dir: "/tmp/cool"
            upload_interval: 30
            show_url_only: false
        ```
    Using an Azure Blob Storage account with an access key:

        ```YAML
        compute_logs:
          module: dagster_azure.blob.compute_log_manager
          class: AzureBlobComputeLogManager
            config:
            storage_account: my-storage-account
            container: my-container
            access_key_or_sas_token: my-access-key
            prefix: "dagster-test-"
            local_dir: "/tmp/cool"
            upload_interval: 30
            show_url_only: false
        ```

    </dd>

</dl>
</div>

<div class="section" id="i-o-manager">

## I/O Manager

<dl>
    <dt><Link class="anchor" id='dagster_azure.adls2.ADLS2PickleIOManager'>dagster_azure.adls2.ADLS2PickleIOManager IOManagerDefinition <a href='https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-azure/dagster_azure/adls2/io_manager.py#L118' className='source-link' target='_blank' rel='noopener noreferrer'>[source]</a><a href="#dagster_azure.adls2.ADLS2PickleIOManager" class="hash-link"></a></Link></dt>
    <dd>

        <div className='lineblock'> </div>

    Persistent IO manager using Azure Data Lake Storage Gen2 for storage.

    Serializes objects via pickling. Suitable for objects storage for distributed executors, so long
    as each execution node has network connectivity and credentials for ADLS and the backing
    container.

    Assigns each op output to a unique filepath containing run ID, step key, and output name.
    Assigns each asset to a single filesystem path, at “\<base_dir>/\<asset_key>”. If the asset key
    has multiple components, the final component is used as the name of the file, and the preceding
    components as parent directories under the base_dir.

    Subsequent materializations of an asset will overwrite previous materializations of that asset.
    With a base directory of “/my/base/path”, an asset with key
    <cite>AssetKey([“one”, “two”, “three”])</cite> would be stored in a file called “three” in a directory
    with path “/my/base/path/one/two/”.

    Example usage:

    1. Attach this IO manager to a set of assets.
        ```python
        from dagster import Definitions, asset
        from dagster_azure.adls2 import ADLS2PickleIOManager, ADLS2Resource, ADLS2SASToken

        @asset
        def asset1():
            # create df ...
            return df

        @asset
        def asset2(asset1):
            return df[:5]

        Definitions(
            assets=[asset1, asset2],
            resources={
                "io_manager": ADLS2PickleIOManager(
                    adls2_file_system="my-cool-fs",
                    adls2_prefix="my-cool-prefix",
                    adls2=ADLS2Resource(
                        storage_account="my-storage-account",
                        credential=ADLS2SASToken(token="my-sas-token"),
                    ),
                ),
            },
        )
        ```
    2. Attach this IO manager to your job to make it available to your ops.
        ```python
        from dagster import job
        from dagster_azure.adls2 import ADLS2PickleIOManager, ADLS2Resource, ADLS2SASToken

        @job(
            resource_defs={
                "io_manager": ADLS2PickleIOManager(
                    adls2_file_system="my-cool-fs",
                    adls2_prefix="my-cool-prefix",
                    adls2=ADLS2Resource(
                        storage_account="my-storage-account",
                        credential=ADLS2SASToken(token="my-sas-token"),
                    ),
                ),
            },
        )
        def my_job():
            ...
        ```

    </dd>

</dl>
</div>

<div class="section" id="file-manager">

## File Manager

<dl>
    <dt><Link class="anchor" id='dagster_azure.adls2.adls2_file_manager'>dagster_azure.adls2.adls2_file_manager ResourceDefinition <a href='https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-azure/dagster_azure/adls2/resources.py#L219' className='source-link' target='_blank' rel='noopener noreferrer'>[source]</a><a href="#dagster_azure.adls2.adls2_file_manager" class="hash-link"></a></Link></dt>
    <dd>

        <div className='lineblock'> </div>

    FileManager that provides abstract access to ADLS2.

    Implements the [`FileManager`](../dagster/internals.mdx#dagster._core.storage.file_manager.FileManager) API.


    </dd>

</dl>
<dl>
    <dt><Link class="anchor" id='dagster_azure.adls2.ADLS2FileHandle'>`class` dagster_azure.adls2.ADLS2FileHandle <a href='https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-azure/dagster_azure/adls2/file_manager.py#L17' className='source-link' target='_blank' rel='noopener noreferrer'>[source]</a><a href="#dagster_azure.adls2.ADLS2FileHandle" class="hash-link"></a></Link></dt>
    <dd>
    A reference to a file on ADLS2.
    </dd>

</dl>
</div>

<div class="section" id="pipes">

## Pipes

<dl>
    <dt><Link class="anchor" id='dagster_azure.pipes.PipesAzureBlobStorageContextInjector'>`class` dagster_azure.pipes.PipesAzureBlobStorageContextInjector <a href='https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-azure/dagster_azure/pipes/context_injectors.py#L16' className='source-link' target='_blank' rel='noopener noreferrer'>[source]</a><a href="#dagster_azure.pipes.PipesAzureBlobStorageContextInjector" class="hash-link"></a></Link></dt>
    <dd>

    A context injector that injects context by writing to a temporary AzureBlobStorage location.

    Parameters:
      - <strong>container</strong> (<em>str</em>) – The AzureBlobStorage container to write to.
      - <strong>client</strong> (<em>azure.storage.blob.BlobServiceClient</em>) – An Azure Blob Storage client.
      - <strong>key_prefix</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) – An optional prefix to use for the Azure Blob Storage key. Defaults to a random string.



    </dd>

</dl>
<dl>
    <dt><Link class="anchor" id='dagster_azure.pipes.PipesAzureBlobStorageMessageReader'>`class` dagster_azure.pipes.PipesAzureBlobStorageMessageReader <a href='https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-azure/dagster_azure/pipes/message_readers.py#L14' className='source-link' target='_blank' rel='noopener noreferrer'>[source]</a><a href="#dagster_azure.pipes.PipesAzureBlobStorageMessageReader" class="hash-link"></a></Link></dt>
    <dd>

    Message reader that reads messages by periodically reading message chunks from a specified AzureBlobStorage
    container.

    If <cite>log_readers</cite> is passed, this reader will also start the passed readers
    when the first message is received from the external process.

    Parameters:
      - <strong>interval</strong> (<em>float</em>) – interval in seconds between attempts to download a chunk
      - <strong>container</strong> (<em>str</em>) – The AzureBlobStorage container to read from.
      - <strong>client</strong> (<em>azure.storage.blob.BlobServiceClient</em>) – An azure BlobServiceClient.
      - <strong>log_readers</strong> (<em>Optional</em><em>[</em><em>Sequence</em><em>[</em><em>PipesLogReader</em><em>]</em><em>]</em>) – A set of log readers for logs on AzureBlobStorage.
      - <strong>include_stdio_in_messages</strong> (<em>bool</em>) – Whether to send stdout/stderr to Dagster via Pipes messages. Defaults to False.



    </dd>

</dl>
<dl>
    <dt><Link class="anchor" id='dagster_azure.pipes.clients.PipesAzureMLClient'>`class` dagster_azure.pipes.clients.PipesAzureMLClient <a href='https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-azure/dagster_azure/pipes/clients/azureml.py#L21' className='source-link' target='_blank' rel='noopener noreferrer'>[source]</a><a href="#dagster_azure.pipes.clients.PipesAzureMLClient" class="hash-link"></a></Link></dt>
    <dd>

    Pipes client for Azure ML.

    Parameters:
      - <strong>client</strong> (<em>MLClient</em>) – An Azure ML <cite>MLClient</cite> object.
      - <strong>context_injector</strong> ([*PipesContextInjector*](../dagster/pipes.mdx#dagster.PipesContextInjector)) – A context injector to use to inject context into the Azure ML job process.
      - <strong>message_reader</strong> ([*PipesMessageReader*](../dagster/pipes.mdx#dagster.PipesMessageReader)) – A message reader to use to read messages from the Azure ML job.
      - <strong>poll_interval_seconds</strong> (<em>float</em>) – How long to sleep between checking the status of the job run. Defaults to 5.
      - <strong>forward_termination</strong> (<em>bool</em>) – Whether to cancel the Azure ML job if the orchestration process is interrupted or canceled. Defaults to True.



    </dd>

</dl>
</div>

<div class="section" id="legacy">

## Legacy

<dl>
    <dt><Link class="anchor" id='dagster_azure.adls2.ConfigurablePickledObjectADLS2IOManager'>dagster_azure.adls2.ConfigurablePickledObjectADLS2IOManager IOManagerDefinition <a href='https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-azure/dagster_azure/adls2/io_manager.py#L224' className='source-link' target='_blank' rel='noopener noreferrer'>[source]</a><a href="#dagster_azure.adls2.ConfigurablePickledObjectADLS2IOManager" class="hash-link"></a></Link></dt>
    <dd>

        <div className='lineblock'> </div>

        :::warning[deprecated]
        This API will be removed in version 2.0.
         Please use ADLS2PickleIOManager instead..

        :::

    Renamed to ADLS2PickleIOManager. See ADLS2PickleIOManager for documentation.


    </dd>

</dl>
<dl>
    <dt><Link class="anchor" id='dagster_azure.adls2.adls2_resource'>dagster_azure.adls2.adls2_resource ResourceDefinition <a href='https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-azure/dagster_azure/adls2/resources.py#L165' className='source-link' target='_blank' rel='noopener noreferrer'>[source]</a><a href="#dagster_azure.adls2.adls2_resource" class="hash-link"></a></Link></dt>
    <dd>

        <div className='lineblock'> </div>

    Resource that gives ops access to Azure Data Lake Storage Gen2.

    The underlying client is a `DataLakeServiceClient`.

    Attach this resource definition to a [`JobDefinition`](../dagster/jobs.mdx#dagster.JobDefinition) in order to make it
    available to your ops.



    Example:

        ```python
        from dagster import job, op
        from dagster_azure.adls2 import adls2_resource

        @op(required_resource_keys={'adls2'})
        def example_adls2_op(context):
            return list(context.resources.adls2.adls2_client.list_file_systems())

        @job(resource_defs={"adls2": adls2_resource})
        def my_job():
            example_adls2_op()
        ```
    Note that your ops must also declare that they require this resource with
    <cite>required_resource_keys</cite>, or it will not be initialized for the execution of their compute
    functions.

    You may pass credentials to this resource using either a SAS token, a key or by passing the
    <cite>DefaultAzureCredential</cite> object.

        ```YAML
        resources:
          adls2:
            config:
              storage_account: my_storage_account
              # str: The storage account name.
              credential:
                sas: my_sas_token
                # str: the SAS token for the account.
                key:
                  env: AZURE_DATA_LAKE_STORAGE_KEY
                # str: The shared access key for the account.
                DefaultAzureCredential: {}
                # dict: The keyword arguments used for DefaultAzureCredential
                # or leave the object empty for no arguments
                DefaultAzureCredential:
                    exclude_environment_credential: true
        ```

    </dd>

</dl>
<dl>
    <dt><Link class="anchor" id='dagster_azure.adls2.adls2_pickle_io_manager'>dagster_azure.adls2.adls2_pickle_io_manager IOManagerDefinition <a href='https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-azure/dagster_azure/adls2/io_manager.py#L234' className='source-link' target='_blank' rel='noopener noreferrer'>[source]</a><a href="#dagster_azure.adls2.adls2_pickle_io_manager" class="hash-link"></a></Link></dt>
    <dd>

        <div className='lineblock'> </div>

    Persistent IO manager using Azure Data Lake Storage Gen2 for storage.

    Serializes objects via pickling. Suitable for objects storage for distributed executors, so long
    as each execution node has network connectivity and credentials for ADLS and the backing
    container.

    Assigns each op output to a unique filepath containing run ID, step key, and output name.
    Assigns each asset to a single filesystem path, at “\<base_dir>/\<asset_key>”. If the asset key
    has multiple components, the final component is used as the name of the file, and the preceding
    components as parent directories under the base_dir.

    Subsequent materializations of an asset will overwrite previous materializations of that asset.
    With a base directory of “/my/base/path”, an asset with key
    <cite>AssetKey([“one”, “two”, “three”])</cite> would be stored in a file called “three” in a directory
    with path “/my/base/path/one/two/”.

    Example usage:

    Attach this IO manager to a set of assets.

        ```python
        from dagster import Definitions, asset
        from dagster_azure.adls2 import adls2_pickle_io_manager, adls2_resource

        @asset
        def asset1():
            # create df ...
            return df

        @asset
        def asset2(asset1):
            return df[:5]

        Definitions(
            assets=[asset1, asset2],
            resources={
                "io_manager": adls2_pickle_io_manager.configured(
                    {"adls2_file_system": "my-cool-fs", "adls2_prefix": "my-cool-prefix"}
                ),
                "adls2": adls2_resource,
            },
        )
        ```
    Attach this IO manager to your job to make it available to your ops.

        ```python
        from dagster import job
        from dagster_azure.adls2 import adls2_pickle_io_manager, adls2_resource

        @job(
            resource_defs={
                "io_manager": adls2_pickle_io_manager.configured(
                    {"adls2_file_system": "my-cool-fs", "adls2_prefix": "my-cool-prefix"}
                ),
                "adls2": adls2_resource,
            },
        )
        def my_job():
            ...
        ```

    </dd>

</dl>
</div></div>
