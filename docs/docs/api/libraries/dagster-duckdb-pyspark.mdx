---
title: 'duckdb + pyspark (dagster-duckdb-pyspark)'
title_meta: 'duckdb + pyspark (dagster-duckdb-pyspark) API Documentation - Build Better Data Pipelines | Python Reference Documentation for Dagster'
description: 'duckdb + pyspark (dagster-duckdb-pyspark) Dagster API | Comprehensive Python API documentation for Dagster, the data orchestration platform. Learn how to build, test, and maintain data pipelines with our detailed guides and examples.'
last_update:
  date: '2025-12-05'
custom_edit_url: null
---

<div class="section" id="duckdb-pyspark-dagster-duckdb-pyspark">

# DuckDB + PySpark (dagster-duckdb-pyspark)

This library provides an integration with the [DuckDB](https://duckdb.org) database and PySpark data processing library.

Related guides:

- [Using Dagster with DuckDB guide](https://docs.dagster.io/integrations/libraries/duckdb)
- [DuckDB I/O manager reference](https://docs.dagster.io/integrations/libraries/duckdb/reference)

<dl>
    <dt><Link class="anchor" id='dagster_duckdb_pyspark.DuckDBPySparkIOManager'>dagster_duckdb_pyspark.DuckDBPySparkIOManager IOManagerDefinition <a href='https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-duckdb-pyspark/dagster_duckdb_pyspark/duckdb_pyspark_type_handler.py#L180' className='source-link' target='_blank' rel='noopener noreferrer'>[source]</a><a href="#dagster_duckdb_pyspark.DuckDBPySparkIOManager" class="hash-link"></a></Link></dt>
    <dd>

        <div className='lineblock'> </div>

    An I/O manager definition that reads inputs from and writes PySpark DataFrames to DuckDB. When
    using the DuckDBPySparkIOManager, any inputs and outputs without type annotations will be loaded
    as PySpark DataFrames.

    Returns: IOManagerDefinition


    Examples:

        ```python
        from dagster_duckdb_pyspark import DuckDBPySparkIOManager

        @asset(
            key_prefix=["my_schema"]  # will be used as the schema in DuckDB
        )
        def my_table() -> pyspark.sql.DataFrame:  # the name of the asset will be the table name
            ...

        Definitions(
            assets=[my_table],
            resources={"io_manager": DuckDBPySparkIOManager(database="my_db.duckdb")}
        )
        ```
    You can set a default schema to store the assets using the `schema` configuration value of the DuckDB I/O
    Manager. This schema will be used if no other schema is specified directly on an asset or op.

        ```python
        Definitions(
            assets=[my_table],
            resources={"io_manager": DuckDBPySparkIOManager(database="my_db.duckdb", schema="my_schema")}
        )
        ```
    On individual assets, you an also specify the schema where they should be stored using metadata or
    by adding a `key_prefix` to the asset key. If both `key_prefix` and metadata are defined, the metadata will
    take precedence.

        ```python
        @asset(
            key_prefix=["my_schema"]  # will be used as the schema in duckdb
        )
        def my_table() -> pyspark.sql.DataFrame:
            ...

        @asset(
            metadata={"schema": "my_schema"}  # will be used as the schema in duckdb
        )
        def my_other_table() -> pyspark.sql.DataFrame:
            ...
        ```
    For ops, the schema can be specified by including a “schema” entry in output metadata.

        ```python
        @op(
            out={"my_table": Out(metadata={"schema": "my_schema"})}
        )
        def make_my_table() -> pyspark.sql.DataFrame:
            ...
        ```
    If none of these is provided, the schema will default to “public”.

    To only use specific columns of a table as input to a downstream op or asset, add the metadata “columns” to the
    In or AssetIn.

        ```python
        @asset(
            ins={"my_table": AssetIn("my_table", metadata={"columns": ["a"]})}
        )
        def my_table_a(my_table: pyspark.sql.DataFrame) -> pyspark.sql.DataFrame:
            # my_table will just contain the data from column "a"
            ...
        ```

    </dd>

</dl>
<dl>
    <dt><Link class="anchor" id='dagster_duckdb_pyspark.DuckDBPySparkTypeHandler'>`class` dagster_duckdb_pyspark.DuckDBPySparkTypeHandler <a href='https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-duckdb-pyspark/dagster_duckdb_pyspark/duckdb_pyspark_type_handler.py#L21' className='source-link' target='_blank' rel='noopener noreferrer'>[source]</a><a href="#dagster_duckdb_pyspark.DuckDBPySparkTypeHandler" class="hash-link"></a></Link></dt>
    <dd>

    Stores PySpark DataFrames in DuckDB.

    To use this type handler, return it from the `type_handlers` method of an I/O manager that inherits from ``DuckDBIOManager`.



    Example:

        ```python
        from dagster_duckdb import DuckDBIOManager
        from dagster_duckdb_pyspark import DuckDBPySparkTypeHandler

        class MyDuckDBIOManager(DuckDBIOManager):
            @staticmethod
            def type_handlers() -> Sequence[DbTypeHandler]:
                return [DuckDBPySparkTypeHandler()]

        @asset(
            key_prefix=["my_schema"]  # will be used as the schema in duckdb
        )
        def my_table() -> pyspark.sql.DataFrame:  # the name of the asset will be the table name
            ...

        Definitions(
            assets=[my_table],
            resources={"io_manager": MyDuckDBIOManager(database="my_db.duckdb")}
        )
        ```

    </dd>

</dl>
<div class="section" id="legacy">

## Legacy

<dl>
    <dt><Link class="anchor" id='dagster_duckdb_pyspark.duckdb_pyspark_io_manager'>dagster_duckdb_pyspark.duckdb_pyspark_io_manager IOManagerDefinition <a href='https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-duckdb/dagster_duckdb/io_manager.py#L115' className='source-link' target='_blank' rel='noopener noreferrer'>[source]</a><a href="#dagster_duckdb_pyspark.duckdb_pyspark_io_manager" class="hash-link"></a></Link></dt>
    <dd>

        <div className='lineblock'> </div>

    An I/O manager definition that reads inputs from and writes PySpark DataFrames to DuckDB. When
    using the duckdb_pyspark_io_manager, any inputs and outputs without type annotations will be loaded
    as PySpark DataFrames.

    Returns: IOManagerDefinition


    Examples:

        ```python
        from dagster_duckdb_pyspark import duckdb_pyspark_io_manager

        @asset(
            key_prefix=["my_schema"]  # will be used as the schema in DuckDB
        )
        def my_table() -> pyspark.sql.DataFrame:  # the name of the asset will be the table name
            ...

        Definitions(
            assets=[my_table],
            resources={"io_manager": duckdb_pyspark_io_manager.configured({"database": "my_db.duckdb"})}
        )
        ```
    You can set a default schema to store the assets using the `schema` configuration value of the DuckDB I/O
    Manager. This schema will be used if no other schema is specified directly on an asset or op.

        ```python
        Definitions(
            assets=[my_table],
            resources={"io_manager": duckdb_pyspark_io_manager.configured({"database": "my_db.duckdb", "schema": "my_schema"})}
        )
        ```
    On individual assets, you an also specify the schema where they should be stored using metadata or
    by adding a `key_prefix` to the asset key. If both `key_prefix` and metadata are defined, the metadata will
    take precedence.

        ```python
        @asset(
            key_prefix=["my_schema"]  # will be used as the schema in duckdb
        )
        def my_table() -> pyspark.sql.DataFrame:
            ...

        @asset(
            metadata={"schema": "my_schema"}  # will be used as the schema in duckdb
        )
        def my_other_table() -> pyspark.sql.DataFrame:
            ...
        ```
    For ops, the schema can be specified by including a “schema” entry in output metadata.

        ```python
        @op(
            out={"my_table": Out(metadata={"schema": "my_schema"})}
        )
        def make_my_table() -> pyspark.sql.DataFrame:
            ...
        ```
    If none of these is provided, the schema will default to “public”.

    To only use specific columns of a table as input to a downstream op or asset, add the metadata “columns” to the
    In or AssetIn.

        ```python
        @asset(
            ins={"my_table": AssetIn("my_table", metadata={"columns": ["a"]})}
        )
        def my_table_a(my_table: pyspark.sql.DataFrame) -> pyspark.sql.DataFrame:
            # my_table will just contain the data from column "a"
            ...
        ```

    </dd>

</dl>
</div></div>
