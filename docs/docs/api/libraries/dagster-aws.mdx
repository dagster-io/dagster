---
title: 'aws (dagster-aws)'
title_meta: 'aws (dagster-aws) API Documentation - Build Better Data Pipelines | Python Reference Documentation for Dagster'
description: 'aws (dagster-aws) Dagster API | Comprehensive Python API documentation for Dagster, the data orchestration platform. Learn how to build, test, and maintain data pipelines with our detailed guides and examples.'
last_update:
  date: '2025-12-05'
custom_edit_url: null
---

<div class="section" id="aws-dagster-aws">

# AWS (dagster-aws)

Utilities for interfacing with AWS with Dagster.

<div class="section" id="s3">

## S3

<dl>
    <dt><Link class="anchor" id='dagster_aws.s3.S3Resource'>dagster_aws.s3.S3Resource ResourceDefinition <a href='https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-aws/dagster_aws/s3/resources.py#L53' className='source-link' target='_blank' rel='noopener noreferrer'>[source]</a><a href="#dagster_aws.s3.S3Resource" class="hash-link"></a></Link></dt>
    <dd>

        <div className='lineblock'> </div>

    Resource that gives access to S3.

    The underlying S3 session is created by calling
    `boto3.session.Session(profile_name)`.
    The returned resource object is an S3 client, an instance of <cite>botocore.client.S3</cite>.



    Example:

        ```python
        from dagster import job, op, Definitions
        from dagster_aws.s3 import S3Resource

        @op
        def example_s3_op(s3: S3Resource):
            return s3.get_client().list_objects_v2(
                Bucket='my-bucket',
                Prefix='some-key'
            )

        @job
        def example_job():
            example_s3_op()

        Definitions(
            jobs=[example_job],
            resources={'s3': S3Resource(region_name='us-west-1')}
        )
        ```

    </dd>

</dl>
<dl>
    <dt><Link class="anchor" id='dagster_aws.s3.S3PickleIOManager'>dagster_aws.s3.S3PickleIOManager IOManagerDefinition <a href='https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-aws/dagster_aws/s3/io_manager.py#L82' className='source-link' target='_blank' rel='noopener noreferrer'>[source]</a><a href="#dagster_aws.s3.S3PickleIOManager" class="hash-link"></a></Link></dt>
    <dd>

        <div className='lineblock'> </div>

    Persistent IO manager using S3 for storage.

    Serializes objects via pickling. Suitable for objects storage for distributed executors, so long
    as each execution node has network connectivity and credentials for S3 and the backing bucket.

    Assigns each op output to a unique filepath containing run ID, step key, and output name.
    Assigns each asset to a single filesystem path, at “\<base_dir>/\<asset_key>”. If the asset key
    has multiple components, the final component is used as the name of the file, and the preceding
    components as parent directories under the base_dir.

    Subsequent materializations of an asset will overwrite previous materializations of that asset.
    With a base directory of “/my/base/path”, an asset with key
    <cite>AssetKey([“one”, “two”, “three”])</cite> would be stored in a file called “three” in a directory
    with path “/my/base/path/one/two/”.

    Example usage:

        ```python
        from dagster import asset, Definitions
        from dagster_aws.s3 import S3PickleIOManager, S3Resource


        @asset
        def asset1():
            # create df ...
            return df

        @asset
        def asset2(asset1):
            return asset1[:5]

        Definitions(
            assets=[asset1, asset2],
            resources={
                "io_manager": S3PickleIOManager(
                    s3_resource=S3Resource(),
                    s3_bucket="my-cool-bucket",
                    s3_prefix="my-cool-prefix",
                )
            }
        )
        ```

    </dd>

</dl>
<dl>
    <dt><Link class="anchor" id='dagster_aws.s3.S3ComputeLogManager'>`class` dagster_aws.s3.S3ComputeLogManager <a href='https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-aws/dagster_aws/s3/compute_log_manager.py#L32' className='source-link' target='_blank' rel='noopener noreferrer'>[source]</a><a href="#dagster_aws.s3.S3ComputeLogManager" class="hash-link"></a></Link></dt>
    <dd>

    Logs compute function stdout and stderr to S3.

    Users should not instantiate this class directly. Instead, use a YAML block in `dagster.yaml`
    such as the following:

        ```YAML
        compute_logs:
          module: dagster_aws.s3.compute_log_manager
          class: S3ComputeLogManager
          config:
            bucket: "mycorp-dagster-compute-logs"
            local_dir: "/tmp/cool"
            prefix: "dagster-test-"
            use_ssl: true
            verify: true
            verify_cert_path: "/path/to/cert/bundle.pem"
            endpoint_url: "http://alternate-s3-host.io"
            skip_empty_files: true
            upload_interval: 30
            upload_extra_args:
              ServerSideEncryption: "AES256"
            show_url_only: false
            region: "us-west-1"
        ```
    Parameters:
      - <strong>bucket</strong> (<em>str</em>) – The name of the s3 bucket to which to log.
      - <strong>local_dir</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) – Path to the local directory in which to stage logs. Default: `dagster_shared.seven.get_system_temp_directory()`.
      - <strong>prefix</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) – Prefix for the log file keys.
      - <strong>use_ssl</strong> (<em>Optional</em><em>[</em><em>bool</em><em>]</em>) – Whether or not to use SSL. Default True.
      - <strong>verify</strong> (<em>Optional</em><em>[</em><em>bool</em><em>]</em>) – Whether or not to verify SSL certificates. Default True.
      - <strong>verify_cert_path</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) – A filename of the CA cert bundle to use. Only used if <cite>verify</cite> set to False.
      - <strong>endpoint_url</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) – Override for the S3 endpoint url.
      - <strong>skip_empty_files</strong> – (Optional[bool]): Skip upload of empty log files.
      - <strong>upload_interval</strong> – (Optional[int]): Interval in seconds to upload partial log files to S3. By default, will only upload when the capture is complete.
      - <strong>upload_extra_args</strong> – (Optional[dict]): Extra args for S3 file upload
      - <strong>show_url_only</strong> – (Optional[bool]): Only show the URL of the log file in the UI, instead of fetching and displaying the full content. Default False.
      - <strong>region</strong> – (Optional[str]): The region of the S3 bucket. If not specified, will use the default region of the AWS session.
      - <strong>inst_data</strong> (<em>Optional</em><em>[</em>[*ConfigurableClassData*](../dagster/internals.mdx#dagster._serdes.ConfigurableClassData)<em>]</em>) – Serializable representation of the compute log manager when newed up from config.



    </dd>

</dl>
<dl>
    <dt><Link class="anchor" id='dagster_aws.s3.S3Coordinate'>dagster_aws.s3.S3Coordinate DagsterType<a href="#dagster_aws.s3.S3Coordinate" class="hash-link"></a></Link></dt>
    <dd>

    A [`dagster.DagsterType`](../dagster/types.mdx#dagster.DagsterType) intended to make it easier to pass information about files on S3
    from op to op. Objects of this type should be dicts with `'bucket'` and `'key'` keys,
    and may be hydrated from config in the intuitive way, e.g., for an input with the name
    `s3_file`:

        ```YAML
        inputs:
          s3_file:
            value:
              bucket: my-bucket
              key: my-key
        ```

    </dd>

</dl>
<div class="section" id="file-manager">

### File Manager

<dl>
    <dt><Link class="anchor" id='dagster_aws.s3.S3FileHandle'>`class` dagster_aws.s3.S3FileHandle <a href='https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-aws/dagster_aws/s3/file_manager.py#L14' className='source-link' target='_blank' rel='noopener noreferrer'>[source]</a><a href="#dagster_aws.s3.S3FileHandle" class="hash-link"></a></Link></dt>
    <dd>
    A reference to a file on S3.
    </dd>

</dl>
<dl>
    <dt><Link class="anchor" id='dagster_aws.s3.S3FileManagerResource'>dagster_aws.s3.S3FileManagerResource ResourceDefinition <a href='https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-aws/dagster_aws/s3/resources.py#L180' className='source-link' target='_blank' rel='noopener noreferrer'>[source]</a><a href="#dagster_aws.s3.S3FileManagerResource" class="hash-link"></a></Link></dt>
    <dd>

        <div className='lineblock'> </div>

    Base class for Dagster resources that utilize structured config.

    This class is a subclass of both `ResourceDefinition` and `Config`.

    Example definition:

        ```python
        class WriterResource(ConfigurableResource):
            prefix: str

            def output(self, text: str) -> None:
                print(f"{self.prefix}{text}")
        ```
    Example usage:

        ```python
        @asset
        def asset_that_uses_writer(writer: WriterResource):
            writer.output("text")

        defs = Definitions(
            assets=[asset_that_uses_writer],
            resources={"writer": WriterResource(prefix="a_prefix")},
        )
        ```
    You can optionally use this class to model configuration only and vend an object
    of a different type for use at runtime. This is useful for those who wish to
    have a separate object that manages configuration and a separate object at runtime. Or
    where you want to directly use a third-party class that you do not control.

    To do this you override the <cite>create_resource</cite> methods to return a different object.

        ```python
        class WriterResource(ConfigurableResource):
            prefix: str

            def create_resource(self, context: InitResourceContext) -> Writer:
                # Writer is pre-existing class defined else
                return Writer(self.prefix)
        ```
    Example usage:

        ```python
        @asset
        def use_preexisting_writer_as_resource(writer: ResourceParam[Writer]):
            writer.output("text")

        defs = Definitions(
            assets=[use_preexisting_writer_as_resource],
            resources={"writer": WriterResource(prefix="a_prefix")},
        )
        ```

    </dd>

</dl>
</div></div>

<div class="section" id="ecs">

## ECS

<dl>
    <dt><Link class="anchor" id='dagster_aws.ecs.EcsRunLauncher'>dagster_aws.ecs.EcsRunLauncher RunLauncher <a href='https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-aws/dagster_aws/ecs/launcher.py#L89' className='source-link' target='_blank' rel='noopener noreferrer'>[source]</a><a href="#dagster_aws.ecs.EcsRunLauncher" class="hash-link"></a></Link></dt>
    <dd>

        <div className='lineblock'> </div>

    RunLauncher that starts a task in ECS for each Dagster job run.

    Parameters:
      - <strong>inst_data</strong> (<em>Optional</em><em>[</em>[*ConfigurableClassData*](../dagster/internals.mdx#dagster._serdes.ConfigurableClassData)<em>]</em>) – If not provided, defaults to None.
      - <strong>task_definition</strong> – If not provided, defaults to None.
      - <strong>container_name</strong> (<em>str</em>) – If not provided, defaults to “run”.
      - <strong>secrets</strong> (<em>Optional</em><em>[</em><em>list</em><em>[</em><em>str</em><em>]</em><em>]</em>) – If not provided, defaults to None.
      - <strong>secrets_tag</strong> (<em>str</em>) – If not provided, defaults to “dagster”.
      - <strong>env_vars</strong> (<em>Optional</em><em>[</em><em>Sequence</em><em>[</em><em>str</em><em>]</em><em>]</em>) – If not provided, defaults to None.
      - <strong>include_sidecars</strong> (<em>bool</em>) – If not provided, defaults to False.
      - <strong>use_current_ecs_task_config</strong> (<em>bool</em>) – If not provided, defaults to True.
      - <strong>run_task_kwargs</strong> (<em>Optional</em><em>[</em><em>Mapping</em><em>[</em><em>str</em><em>, </em><em>Any</em><em>]</em><em>]</em>) – If not provided, defaults to None.
      - <strong>run_resources</strong> (<em>Optional</em><em>[</em><em>dict</em><em>[</em><em>str</em><em>, </em><em>Any</em><em>]</em><em>]</em>) – If not provided, defaults to None.
      - <strong>run_ecs_tags</strong> (<em>Optional</em><em>[</em><em>list</em><em>[</em><em>dict</em><em>[</em><em>str</em><em>, </em><em>Optional</em><em>[</em><em>str</em><em>]</em><em>]</em><em>]</em><em>]</em>) – If not provided, defaults to None.
      - <strong>propagate_tags</strong> (<em>Optional</em><em>[</em><em>dict</em><em>[</em><em>str</em><em>, </em><em>Any</em><em>]</em><em>]</em>) – If not provided, defaults to None.
      - <strong>task_definition_prefix</strong> (<em>str</em>) – If not provided, defaults to “run”.



    </dd>

</dl>
<dl>
    <dt><Link class="anchor" id='dagster_aws.ecs.ecs_executor'>dagster_aws.ecs.ecs_executor ExecutorDefinition <a href='https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-aws/dagster_aws/ecs/executor.py#L79' className='source-link' target='_blank' rel='noopener noreferrer'>[source]</a><a href="#dagster_aws.ecs.ecs_executor" class="hash-link"></a></Link></dt>
    <dd>

        <div className='lineblock'> </div>

    Executor which launches steps as ECS tasks.

    To use the <cite>ecs_executor</cite>, set it as the <cite>executor_def</cite> when defining a job:

        ```python
        from dagster_aws.ecs import ecs_executor

        from dagster import job, op


        @op(
            tags={"ecs/cpu": "256", "ecs/memory": "512"},
        )
        def ecs_op():
            pass


        @job(executor_def=ecs_executor)
        def ecs_job():
            ecs_op()


        ```
    Then you can configure the executor with run config as follows:

        ```YAML
        execution:
          config:
            cpu: 1024
            memory: 2048
            ephemeral_storage: 10
            task_overrides:
            containerOverrides:
              - name: run
                environment:
                  - name: MY_ENV_VAR
                    value: "my_value"
        ```
    <cite>max_concurrent</cite> limits the number of ECS tasks that will execute concurrently for one run. By default
    there is no limit- it will maximally parallel as allowed by the DAG. Note that this is not a
    global limit.

    Configuration set on the ECS tasks created by the <cite>ECSRunLauncher</cite> will also be
    set on the tasks created by the <cite>ecs_executor</cite>.

    Configuration set using <cite>tags</cite> on a <cite>@job</cite> will only apply to the <cite>run</cite> level. For configuration
    to apply at each <cite>step</cite> it must be set using <cite>tags</cite> for each <cite>@op</cite>.


    </dd>

</dl>
</div>

<div class="section" id="rds">

## RDS

<dl>
    <dt><Link class="anchor" id='dagster_aws.rds.RDSResource'>dagster_aws.rds.RDSResource ResourceDefinition <a href='https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-aws/dagster_aws/rds/resources.py#L14' className='source-link' target='_blank' rel='noopener noreferrer'>[source]</a><a href="#dagster_aws.rds.RDSResource" class="hash-link"></a></Link></dt>
    <dd>

        <div className='lineblock'> </div>

    A resource for interacting with the AWS RDS service.

    It wraps both the AWS RDS client ([https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/rds.html](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/rds.html)),
    and the AWS RDS Data client ([https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/rds-data.html](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/rds-data.html)).

    The AWS-RDS client (`RDSResource.get_rds_client()`) allows access to the management layer of RDS (creating, starting, configuring databases).
    The AWS RDS Data (`RDSResource.get_data_client`) allows executing queries on the SQL databases themselves.
    Note that AWS RDS Data service is only available for Aurora database. For accessing data from other types of RDS databases,
    you should directly use the corresponding SQL client instead (e.g. Postgres/MySQL).



    Example:

        ```python
        from dagster import Definitions, asset
        from dagster_aws.rds import RDSResource

        @asset
        def my_table(rds_resource: RDSResource):
            with rds_resource.get_rds_client() as rds_client:
                rds_client.describe_db_instances()['DBInstances']
            with rds_resource.get_data_client() as data_client:
                data_client.execute_statement(
                    resourceArn="RESOURCE_ARN",
                    secretArn="SECRET_ARN",
                    sql="SELECT * from mytable",
                )

        Definitions(
            assets=[my_table],
            resources={
                "rds_resource": RDSResource(
                    region_name="us-west-1"
                )
            }
        )
        ```

    </dd>

</dl>
</div>

<div class="section" id="redshift">

## Redshift

<dl>
    <dt><Link class="anchor" id='dagster_aws.redshift.RedshiftClientResource'>dagster_aws.redshift.RedshiftClientResource ResourceDefinition <a href='https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-aws/dagster_aws/redshift/resources.py#L282' className='source-link' target='_blank' rel='noopener noreferrer'>[source]</a><a href="#dagster_aws.redshift.RedshiftClientResource" class="hash-link"></a></Link></dt>
    <dd>

        <div className='lineblock'> </div>

    This resource enables connecting to a Redshift cluster and issuing queries against that
    cluster.



    Example:

        ```python
        from dagster import Definitions, asset, EnvVar
        from dagster_aws.redshift import RedshiftClientResource

        @asset
        def example_redshift_asset(context, redshift: RedshiftClientResource):
            redshift.get_client().execute_query('SELECT 1', fetch_results=True)

        redshift_configured = RedshiftClientResource(
            host='my-redshift-cluster.us-east-1.redshift.amazonaws.com',
            port=5439,
            user='dagster',
            password=EnvVar("DAGSTER_REDSHIFT_PASSWORD"),
            database='dev',
        )

        Definitions(
            assets=[example_redshift_asset],
            resources={'redshift': redshift_configured},
        )
        ```

    </dd>

</dl>
<div class="section" id="testing">

### Testing

<dl>
    <dt><Link class="anchor" id='dagster_aws.redshift.FakeRedshiftClientResource'>dagster_aws.redshift.FakeRedshiftClientResource ResourceDefinition <a href='https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-aws/dagster_aws/redshift/resources.py#L356' className='source-link' target='_blank' rel='noopener noreferrer'>[source]</a><a href="#dagster_aws.redshift.FakeRedshiftClientResource" class="hash-link"></a></Link></dt>
    <dd>

        <div className='lineblock'> </div>

    This resource enables connecting to a Redshift cluster and issuing queries against that
    cluster.



    Example:

        ```python
        from dagster import Definitions, asset, EnvVar
        from dagster_aws.redshift import RedshiftClientResource

        @asset
        def example_redshift_asset(context, redshift: RedshiftClientResource):
            redshift.get_client().execute_query('SELECT 1', fetch_results=True)

        redshift_configured = RedshiftClientResource(
            host='my-redshift-cluster.us-east-1.redshift.amazonaws.com',
            port=5439,
            user='dagster',
            password=EnvVar("DAGSTER_REDSHIFT_PASSWORD"),
            database='dev',
        )

        Definitions(
            assets=[example_redshift_asset],
            resources={'redshift': redshift_configured},
        )
        ```

    </dd>

</dl>
</div></div>

<div class="section" id="emr">

## EMR

<dl>
    <dt><Link class="anchor" id='dagster_aws.emr.emr_pyspark_step_launcher'>dagster_aws.emr.emr_pyspark_step_launcher ResourceDefinition <a href='https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-aws/dagster_aws/emr/pyspark_step_launcher.py#L35' className='source-link' target='_blank' rel='noopener noreferrer'>[source]</a><a href="#dagster_aws.emr.emr_pyspark_step_launcher" class="hash-link"></a></Link></dt>
    <dd>

        <div className='lineblock'> </div>

        :::warning[superseded]
        This API has been superseded.
         While there is no plan to remove this functionality, for new projects, we recommend using Dagster Pipes. For more information, see https://docs.dagster.io/guides/build/external-pipelines.

        :::

      - <strong>spark_config</strong>:
      - <strong>cluster_id</strong>: Name of the job flow (cluster) on which to execute.
      - <strong>region_name</strong>: The AWS region that the cluster is in.
      - <strong>action_on_failure</strong>: The EMR action to take when the cluster step fails: [https://docs.aws.amazon.com/emr/latest/APIReference/API_StepConfig.html](https://docs.aws.amazon.com/emr/latest/APIReference/API_StepConfig.html)
      - <strong>staging_bucket</strong>: S3 bucket to use for passing files between the plan process and EMR process.
      - <strong>staging_prefix</strong>: S3 key prefix inside the staging_bucket to use for files passed the plan process and EMR process
      - <strong>wait_for_logs</strong>: If set, the system will wait for EMR logs to appear on S3. Note that logs are copied every 5 minutes, so enabling this will add several minutes to the job runtime.
      - <strong>local_job_package_path</strong>: Absolute path to the package that contains the job definition(s) whose steps will execute remotely on EMR. This is a path on the local fileystem of the process executing the job. The expectation is that this package will also be available on the python path of the launched process running the Spark step on EMR, either deployed on step launch via the deploy_local_job_package option, referenced on s3 via the s3_job_package_path option, or installed on the cluster via bootstrap actions.
      - <strong>local_pipeline_package_path</strong>: (legacy) Absolute path to the package that contains the pipeline definition(s) whose steps will execute remotely on EMR. This is a path on the local fileystem of the process executing the pipeline. The expectation is that this package will also be available on the python path of the launched process running the Spark step on EMR, either deployed on step launch via the deploy_local_pipeline_package option, referenced on s3 via the s3_pipeline_package_path option, or installed on the cluster via bootstrap actions.
      - <strong>deploy_local_job_package</strong>: If set, before every step run, the launcher will zip up all the code in local_job_package_path, upload it to s3, and pass it to spark-submit’s –py-files option. This gives the remote process access to up-to-date user code. If not set, the assumption is that some other mechanism is used for distributing code to the EMR cluster. If this option is set to True, s3_job_package_path should not also be set.
      - <strong>deploy_local_pipeline_package</strong>: (legacy) If set, before every step run, the launcher will zip up all the code in local_job_package_path, upload it to s3, and pass it to spark-submit’s –py-files option. This gives the remote process access to up-to-date user code. If not set, the assumption is that some other mechanism is used for distributing code to the EMR cluster. If this option is set to True, s3_job_package_path should not also be set.
      - <strong>s3_job_package_path</strong>: If set, this path will be passed to the –py-files option of spark-submit. This should usually be a path to a zip file.  If this option is set, deploy_local_job_package should not be set to True.
      - <strong>s3_pipeline_package_path</strong>: If set, this path will be passed to the –py-files option of spark-submit. This should usually be a path to a zip file.  If this option is set, deploy_local_pipeline_package should not be set to True.



    </dd>

</dl>
<dl>
    <dt><Link class="anchor" id='dagster_aws.emr.EmrJobRunner'>`class` dagster_aws.emr.EmrJobRunner <a href='https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-aws/dagster_aws/emr/emr.py#L49' className='source-link' target='_blank' rel='noopener noreferrer'>[source]</a><a href="#dagster_aws.emr.EmrJobRunner" class="hash-link"></a></Link></dt>
    <dd>

    </dd>

</dl>
<dl>
    <dt><Link class="anchor" id='dagster_aws.emr.EmrError'>`class` dagster_aws.emr.EmrError <a href='https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-aws/dagster_aws/emr/emr.py#L45' className='source-link' target='_blank' rel='noopener noreferrer'>[source]</a><a href="#dagster_aws.emr.EmrError" class="hash-link"></a></Link></dt>
    <dd>

    </dd>

</dl>
<dl>
    <dt><Link class="anchor" id='dagster_aws.emr.EmrClusterState'>dagster_aws.emr.EmrClusterState `=` \<enum 'EmrClusterState'> <a href='https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-aws/dagster_aws/emr/types.py#L10' className='source-link' target='_blank' rel='noopener noreferrer'>[source]</a><a href="#dagster_aws.emr.EmrClusterState" class="hash-link"></a></Link></dt>
    <dd>
    Cluster state for EMR.
    </dd>

</dl>
<dl>
    <dt><Link class="anchor" id='dagster_aws.emr.EmrStepState'>dagster_aws.emr.EmrStepState `=` \<enum 'EmrStepState'> <a href='https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-aws/dagster_aws/emr/types.py#L31' className='source-link' target='_blank' rel='noopener noreferrer'>[source]</a><a href="#dagster_aws.emr.EmrStepState" class="hash-link"></a></Link></dt>
    <dd>
    Step state for EMR.
    </dd>

</dl>
</div>

<div class="section" id="cloudwatch">

## CloudWatch

<dl>
    <dt><Link class="anchor" id='dagster_aws.cloudwatch.cloudwatch_logger'>dagster_aws.cloudwatch.cloudwatch_logger LoggerDefinition <a href='https://github.com/dagster-io/dagster/blob/master/python_modules/dagster/dagster/_core/definitions/logger_definition.py#L50' className='source-link' target='_blank' rel='noopener noreferrer'>[source]</a><a href="#dagster_aws.cloudwatch.cloudwatch_logger" class="hash-link"></a></Link></dt>
    <dd>

        <div className='lineblock'> </div>

    Core class for defining loggers.

    Loggers are job-scoped logging handlers, which will be automatically invoked whenever
    dagster messages are logged from within a job.

    Parameters:
      - <strong>logger_fn</strong> (<em>Callable</em><em>[</em><em>[</em>[*InitLoggerContext*](../dagster/loggers.mdx#dagster.InitLoggerContext)<em>]</em><em>, </em><em>logging.Logger</em><em>]</em>) – User-provided function to instantiate the logger. This logger will be automatically invoked whenever the methods on `context.log` are called from within job compute logic.
      - <strong>config_schema</strong> (<em>Optional</em><em>[</em>[*ConfigSchema*](../dagster/config.mdx#dagster.ConfigSchema)<em>]</em>) – The schema for the config. Configuration data available in <cite>init_context.logger_config</cite>. If not set, Dagster will accept any config provided.
      - <strong>description</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) – A human-readable description of this logger.



    </dd>

</dl>
</div>

<div class="section" id="secretsmanager">

## SecretsManager

Resources which surface SecretsManager secrets for use in Dagster resources and jobs.

<dl>
    <dt><Link class="anchor" id='dagster_aws.secretsmanager.SecretsManagerResource'>dagster_aws.secretsmanager.SecretsManagerResource ResourceDefinition <a href='https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-aws/dagster_aws/secretsmanager/resources.py#L27' className='source-link' target='_blank' rel='noopener noreferrer'>[source]</a><a href="#dagster_aws.secretsmanager.SecretsManagerResource" class="hash-link"></a></Link></dt>
    <dd>

        <div className='lineblock'> </div>

        :::info[beta]
        This API is currently in beta, and may have breaking changes in minor version releases, with behavior changes in patch releases.


        :::

    Resource that gives access to AWS SecretsManager.

    The underlying SecretsManager session is created by calling
    `boto3.session.Session(profile_name)`.
    The returned resource object is a SecretsManager client, an instance of <cite>botocore.client.SecretsManager</cite>.



    Example:

        ```python
        from dagster import build_op_context, job, op
        from dagster_aws.secretsmanager import SecretsManagerResource

        @op
        def example_secretsmanager_op(secretsmanager: SecretsManagerResource):
            return secretsmanager.get_client().get_secret_value(
                SecretId='arn:aws:secretsmanager:region:aws_account_id:secret:appauthexample-AbCdEf'
            )

        @job
        def example_job():
            example_secretsmanager_op()

        Definitions(
            jobs=[example_job],
            resources={
                'secretsmanager': SecretsManagerResource(
                    region_name='us-west-1'
                )
            }
        )
        ```

    </dd>

</dl>
<dl>
    <dt><Link class="anchor" id='dagster_aws.secretsmanager.SecretsManagerSecretsResource'>dagster_aws.secretsmanager.SecretsManagerSecretsResource ResourceDefinition <a href='https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-aws/dagster_aws/secretsmanager/resources.py#L135' className='source-link' target='_blank' rel='noopener noreferrer'>[source]</a><a href="#dagster_aws.secretsmanager.SecretsManagerSecretsResource" class="hash-link"></a></Link></dt>
    <dd>

        <div className='lineblock'> </div>

        :::info[beta]
        This API is currently in beta, and may have breaking changes in minor version releases, with behavior changes in patch releases.


        :::

    Resource that provides a dict which maps selected SecretsManager secrets to
    their string values. Also optionally sets chosen secrets as environment variables.



    Example:

        ```python
        import os
        from dagster import build_op_context, job, op, ResourceParam
        from dagster_aws.secretsmanager import SecretsManagerSecretsResource

        @op
        def example_secretsmanager_secrets_op(secrets: SecretsManagerSecretsResource):
            return secrets.fetch_secrets().get("my-secret-name")

        @op
        def example_secretsmanager_secrets_op_2(secrets: SecretsManagerSecretsResource):
            with secrets.secrets_in_environment():
                return os.getenv("my-other-secret-name")

        @job
        def example_job():
            example_secretsmanager_secrets_op()
            example_secretsmanager_secrets_op_2()

        Definitions(
            jobs=[example_job],
            resources={
                'secrets': SecretsManagerSecretsResource(
                    region_name='us-west-1',
                    secrets_tag="dagster",
                    add_to_environment=True,
                )
            }
        )
        ```
    Note that your ops must also declare that they require this resource with or it will not be initialized
    for the execution of their compute functions.


    </dd>

</dl>
</div>

<div class="section" id="ssm">

## SSM

<dl>
    <dt><Link class="anchor" id='dagster_aws.ssm.SSMResource'>dagster_aws.ssm.SSMResource ResourceDefinition <a href='https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-aws/dagster_aws/ssm/resources.py#L29' className='source-link' target='_blank' rel='noopener noreferrer'>[source]</a><a href="#dagster_aws.ssm.SSMResource" class="hash-link"></a></Link></dt>
    <dd>

        <div className='lineblock'> </div>

        :::info[beta]
        This API is currently in beta, and may have breaking changes in minor version releases, with behavior changes in patch releases.


        :::

    Resource that gives access to AWS Systems Manager Parameter Store.

    The underlying Parameter Store session is created by calling
    `boto3.session.Session(profile_name)`.
    The returned resource object is a Systems Manager client, an instance of <cite>botocore.client.ssm</cite>.



    Example:

        ```python
        from typing import Any
        from dagster import build_op_context, job, op
        from dagster_aws.ssm import SSMResource

        @op
        def example_ssm_op(ssm: SSMResource):
            return ssm.get_client().get_parameter(
                Name="a_parameter"
            )

        @job
        def example_job():
            example_ssm_op()

        Definitions(
            jobs=[example_job],
            resources={
                'ssm': SSMResource(
                    region_name='us-west-1'
                )
            }
        )
        ```

    </dd>

</dl>
<dl>
    <dt><Link class="anchor" id='dagster_aws.ssm.ParameterStoreResource'>dagster_aws.ssm.ParameterStoreResource ResourceDefinition <a href='https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-aws/dagster_aws/ssm/resources.py#L144' className='source-link' target='_blank' rel='noopener noreferrer'>[source]</a><a href="#dagster_aws.ssm.ParameterStoreResource" class="hash-link"></a></Link></dt>
    <dd>

        <div className='lineblock'> </div>

        :::info[beta]
        This API is currently in beta, and may have breaking changes in minor version releases, with behavior changes in patch releases.


        :::

    Resource that provides a dict which maps selected SSM Parameter Store parameters to
    their string values. Optionally sets selected parameters as environment variables.



    Example:

        ```python
        import os
        from typing import Dict
        from dagster import build_op_context, job, op
        from dagster_aws.ssm import ParameterStoreResource, ParameterStoreTag

        @op
        def example_parameter_store_op(parameter_store: ParameterStoreResource):
            return parameter_store.fetch_parameters().get("my-parameter-name")

        @op
        def example_parameter_store_op_2(parameter_store: ParameterStoreResource):
            with parameter_store.parameters_in_environment():
                return os.getenv("my-other-parameter-name")

        @job
        def example_job():
            example_parameter_store_op()
            example_parameter_store_op_2()

        defs = Definitions(
            jobs=[example_job],
            resource_defs={
                'parameter_store': ParameterStoreResource(
                    region_name='us-west-1',
                    parameter_tags=[ParameterStoreTag(key='my-tag-key', values=['my-tag-value'])],
                    add_to_environment=True,
                    with_decryption=True,
                )
            },
        )
        ```

    </dd>

</dl>
<dl>
    <dt><Link class="anchor" id='dagster_aws.ssm.ParameterStoreTag'>`class` dagster_aws.ssm.ParameterStoreTag <a href='https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-aws/dagster_aws/ssm/resources.py#L138' className='source-link' target='_blank' rel='noopener noreferrer'>[source]</a><a href="#dagster_aws.ssm.ParameterStoreTag" class="hash-link"></a></Link></dt>
    <dd>

        :::info[beta]
        This API is currently in beta, and may have breaking changes in minor version releases, with behavior changes in patch releases.


        :::


    </dd>

</dl>
</div>

<div class="section" id="pipes">

## Pipes

<div class="section" id="context-injectors">

### Context Injectors

<dl>
    <dt><Link class="anchor" id='dagster_aws.pipes.PipesS3ContextInjector'>`class` dagster_aws.pipes.PipesS3ContextInjector <a href='https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-aws/dagster_aws/pipes/context_injectors.py#L20' className='source-link' target='_blank' rel='noopener noreferrer'>[source]</a><a href="#dagster_aws.pipes.PipesS3ContextInjector" class="hash-link"></a></Link></dt>
    <dd>

    A context injector that injects context by writing to a temporary S3 location.

    Parameters:
      - <strong>bucket</strong> (<em>str</em>) – The S3 bucket to write to.
      - <strong>client</strong> (<em>S3Client</em>) – A boto3 client to use to write to S3.
      - <strong>key_prefix</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) – An optional prefix to use for the S3 key. Defaults to a random string.



    </dd>

</dl>
<dl>
    <dt><Link class="anchor" id='dagster_aws.pipes.PipesLambdaEventContextInjector'>`class` dagster_aws.pipes.PipesLambdaEventContextInjector <a href='https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-aws/dagster_aws/pipes/context_injectors.py#L55' className='source-link' target='_blank' rel='noopener noreferrer'>[source]</a><a href="#dagster_aws.pipes.PipesLambdaEventContextInjector" class="hash-link"></a></Link></dt>
    <dd>
    Injects context via AWS Lambda event input.
    Should be paired with :py:class`~dagster_pipes.PipesMappingParamsLoader` on the Lambda side.
    </dd>

</dl>
</div>

<div class="section" id="message-readers">

### Message Readers

<dl>
    <dt><Link class="anchor" id='dagster_aws.pipes.PipesS3MessageReader'>`class` dagster_aws.pipes.PipesS3MessageReader <a href='https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-aws/dagster_aws/pipes/message_readers.py#L100' className='source-link' target='_blank' rel='noopener noreferrer'>[source]</a><a href="#dagster_aws.pipes.PipesS3MessageReader" class="hash-link"></a></Link></dt>
    <dd>

    Message reader that reads messages by periodically reading message chunks from a specified S3
    bucket.

    If <cite>log_readers</cite> is passed, this reader will also start the passed readers
    when the first message is received from the external process.

    Parameters:
      - <strong>interval</strong> (<em>float</em>) – interval in seconds between attempts to download a chunk
      - <strong>bucket</strong> (<em>str</em>) – The S3 bucket to read from.
      - <strong>client</strong> (<em>boto3.client</em>) – A boto3 S3 client.
      - <strong>log_readers</strong> (<em>Optional</em><em>[</em><em>Sequence</em><em>[</em><em>PipesLogReader</em><em>]</em><em>]</em>) – A set of log readers for logs on S3.
      - <strong>include_stdio_in_messages</strong> (<em>bool</em>) – Whether to send stdout/stderr to Dagster via Pipes messages. Defaults to False.



    </dd>

</dl>
<dl>
    <dt><Link class="anchor" id='dagster_aws.pipes.PipesCloudWatchMessageReader'>`class` dagster_aws.pipes.PipesCloudWatchMessageReader <a href='https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-aws/dagster_aws/pipes/message_readers.py#L344' className='source-link' target='_blank' rel='noopener noreferrer'>[source]</a><a href="#dagster_aws.pipes.PipesCloudWatchMessageReader" class="hash-link"></a></Link></dt>
    <dd>
    Message reader that consumes AWS CloudWatch logs to read pipes messages.
    </dd>

</dl>
</div>

<div class="section" id="clients">

### Clients

<dl>
    <dt><Link class="anchor" id='dagster_aws.pipes.PipesLambdaClient'>`class` dagster_aws.pipes.PipesLambdaClient <a href='https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-aws/dagster_aws/pipes/clients/lambda_.py#L22' className='source-link' target='_blank' rel='noopener noreferrer'>[source]</a><a href="#dagster_aws.pipes.PipesLambdaClient" class="hash-link"></a></Link></dt>
    <dd>

    A pipes client for invoking AWS lambda.

    By default context is injected via the lambda input event and messages are parsed out of the
    4k tail of logs.

    Parameters:
      - <strong>client</strong> (<em>boto3.client</em>) – The boto lambda client used to call invoke.
      - <strong>context_injector</strong> (<em>Optional</em><em>[</em>[*PipesContextInjector*](../dagster/pipes.mdx#dagster.PipesContextInjector)<em>]</em>) – A context injector to use to inject context into the lambda function. Defaults to [`PipesLambdaEventContextInjector`](#dagster_aws.pipes.PipesLambdaEventContextInjector).
      - <strong>message_reader</strong> (<em>Optional</em><em>[</em>[*PipesMessageReader*](../dagster/pipes.mdx#dagster.PipesMessageReader)<em>]</em>) – A message reader to use to read messages from the lambda function. Defaults to `PipesLambdaLogsMessageReader`.


    <dl>
        <dt><Link class="anchor" id='dagster_aws.pipes.PipesLambdaClient.run'>run <a href='https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-aws/dagster_aws/pipes/clients/lambda_.py#L50' className='source-link' target='_blank' rel='noopener noreferrer'>[source]</a><a href="#dagster_aws.pipes.PipesLambdaClient.run" class="hash-link"></a></Link></dt>
        <dd>

        Synchronously invoke a lambda function, enriched with the pipes protocol.

        Parameters:
          - <strong>function_name</strong> (<em>str</em>) – The name of the function to use.
          - <strong>event</strong> (<em>Mapping</em><em>[</em><em>str</em><em>, </em><em>Any</em><em>]</em>) – A JSON serializable object to pass as input to the lambda.
          - <strong>context</strong> (<em>Union</em><em>[</em>[*OpExecutionContext*](../dagster/execution.mdx#dagster.OpExecutionContext)<em>, </em>[*AssetExecutionContext*](../dagster/execution.mdx#dagster.AssetExecutionContext)<em>]</em>) – The context of the currently executing Dagster op or asset.


        Returns: Wrapper containing results reported by the external
        process.Return type: PipesClientCompletedInvocation

        </dd>

    </dl>

    </dd>

</dl>
<dl>
    <dt><Link class="anchor" id='dagster_aws.pipes.PipesGlueClient'>`class` dagster_aws.pipes.PipesGlueClient <a href='https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-aws/dagster_aws/pipes/clients/glue.py#L28' className='source-link' target='_blank' rel='noopener noreferrer'>[source]</a><a href="#dagster_aws.pipes.PipesGlueClient" class="hash-link"></a></Link></dt>
    <dd>

    A pipes client for invoking AWS Glue jobs.

    Parameters:
      - <strong>context_injector</strong> (<em>Optional</em><em>[</em>[*PipesContextInjector*](../dagster/pipes.mdx#dagster.PipesContextInjector)<em>]</em>) – A context injector to use to inject context into the Glue job, for example, [`PipesS3ContextInjector`](#dagster_aws.pipes.PipesS3ContextInjector).
      - <strong>message_reader</strong> (<em>Optional</em><em>[</em>[*PipesMessageReader*](../dagster/pipes.mdx#dagster.PipesMessageReader)<em>]</em>) – A message reader to use to read messages from the glue job run. Defaults to `PipesCloudWatchsMessageReader`. When provided with [`PipesCloudWatchMessageReader`](#dagster_aws.pipes.PipesCloudWatchMessageReader), it will be used to recieve logs and events from the `.../output/\<job-run-id>` CloudWatch log stream created by AWS Glue. Note that AWS Glue routes both `stderr` and `stdout` from the main job process into this LogStream.
      - <strong>client</strong> (<em>Optional</em><em>[</em><em>boto3.client</em><em>]</em>) – The boto Glue client used to launch the Glue job
      - <strong>forward_termination</strong> (<em>bool</em>) – Whether to cancel the Glue job run when the Dagster process receives a termination signal.


    <dl>
        <dt><Link class="anchor" id='dagster_aws.pipes.PipesGlueClient.run'>run <a href='https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-aws/dagster_aws/pipes/clients/glue.py#L60' className='source-link' target='_blank' rel='noopener noreferrer'>[source]</a><a href="#dagster_aws.pipes.PipesGlueClient.run" class="hash-link"></a></Link></dt>
        <dd>

        Start a Glue job, enriched with the pipes protocol.

        See also: [AWS API Documentation](https://docs.aws.amazon.com/goto/WebAPI/glue-2017-03-31/StartJobRun)

        Parameters:
          - <strong>context</strong> (<em>Union</em><em>[</em>[*OpExecutionContext*](../dagster/execution.mdx#dagster.OpExecutionContext)<em>, </em>[*AssetExecutionContext*](../dagster/execution.mdx#dagster.AssetExecutionContext)<em>]</em>) – The context of the currently executing Dagster op or asset.
          - <strong>start_job_run_params</strong> (<em>Dict</em>) – Parameters for the `start_job_run` boto3 Glue client call.
          - <strong>extras</strong> (<em>Optional</em><em>[</em><em>Dict</em><em>[</em><em>str</em><em>, </em><em>Any</em><em>]</em><em>]</em>) – Additional Dagster metadata to pass to the Glue job.


        Returns: Wrapper containing results reported by the external
        process.Return type: PipesClientCompletedInvocation

        </dd>

    </dl>

    </dd>

</dl>
<dl>
    <dt><Link class="anchor" id='dagster_aws.pipes.PipesECSClient'>`class` dagster_aws.pipes.PipesECSClient <a href='https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-aws/dagster_aws/pipes/clients/ecs.py#L33' className='source-link' target='_blank' rel='noopener noreferrer'>[source]</a><a href="#dagster_aws.pipes.PipesECSClient" class="hash-link"></a></Link></dt>
    <dd>

    A pipes client for running AWS ECS tasks.

    Parameters:
      - <strong>client</strong> (<em>Any</em>) – The boto ECS client used to launch the ECS task
      - <strong>context_injector</strong> (<em>Optional</em><em>[</em>[*PipesContextInjector*](../dagster/pipes.mdx#dagster.PipesContextInjector)<em>]</em>) – A context injector to use to inject context into the ECS task. Defaults to `PipesEnvContextInjector`.
      - <strong>message_reader</strong> (<em>Optional</em><em>[</em>[*PipesMessageReader*](../dagster/pipes.mdx#dagster.PipesMessageReader)<em>]</em>) – A message reader to use to read messages from the ECS task. Defaults to [`PipesCloudWatchMessageReader`](#dagster_aws.pipes.PipesCloudWatchMessageReader).
      - <strong>forward_termination</strong> (<em>bool</em>) – Whether to cancel the ECS task when the Dagster process receives a termination signal.


    <dl>
        <dt><Link class="anchor" id='dagster_aws.pipes.PipesECSClient.run'>run <a href='https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-aws/dagster_aws/pipes/clients/ecs.py#L61' className='source-link' target='_blank' rel='noopener noreferrer'>[source]</a><a href="#dagster_aws.pipes.PipesECSClient.run" class="hash-link"></a></Link></dt>
        <dd>

        Run ECS tasks, enriched with the pipes protocol.

        Parameters:
          - <strong>context</strong> (<em>Union</em><em>[</em>[*OpExecutionContext*](../dagster/execution.mdx#dagster.OpExecutionContext)<em>, </em>[*AssetExecutionContext*](../dagster/execution.mdx#dagster.AssetExecutionContext)<em>]</em>) – The context of the currently executing Dagster op or asset.
          - <strong>run_task_params</strong> (<em>dict</em>) – Parameters for the `run_task` boto3 ECS client call. Must contain `taskDefinition` key. See [Boto3 API Documentation](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/ecs/client/run_task.html#run-task)
          - <strong>extras</strong> (<em>Optional</em><em>[</em><em>Dict</em><em>[</em><em>str</em><em>, </em><em>Any</em><em>]</em><em>]</em>) – Additional information to pass to the Pipes session in the external process.
          - <strong>pipes_container_name</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) – If running more than one container in the task, and using [`PipesCloudWatchMessageReader`](#dagster_aws.pipes.PipesCloudWatchMessageReader), specify the container name which will be running Pipes.
          - <strong>waiter_config</strong> (<em>Optional</em><em>[</em><em>WaiterConfig</em><em>]</em>) – Optional waiter configuration to use. Defaults to 70 days (Delay: 6, MaxAttempts: 1000000).


        Returns: Wrapper containing results reported by the external
        process.Return type: PipesClientCompletedInvocation

        </dd>

    </dl>

    </dd>

</dl>
<dl>
    <dt><Link class="anchor" id='dagster_aws.pipes.PipesEMRClient'>`class` dagster_aws.pipes.PipesEMRClient <a href='https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-aws/dagster_aws/pipes/clients/emr.py#L41' className='source-link' target='_blank' rel='noopener noreferrer'>[source]</a><a href="#dagster_aws.pipes.PipesEMRClient" class="hash-link"></a></Link></dt>
    <dd>

    A pipes client for running jobs on AWS EMR.

    Parameters:
      - <strong>message_reader</strong> (<em>Optional</em><em>[</em>[*PipesMessageReader*](../dagster/pipes.mdx#dagster.PipesMessageReader)<em>]</em>) – A message reader to use to read messages from the EMR jobs. Recommended to use [`PipesS3MessageReader`](#dagster_aws.pipes.PipesS3MessageReader) with <cite>expect_s3_message_writer</cite> set to <cite>True</cite>.
      - <strong>client</strong> (<em>Optional</em><em>[</em><em>boto3.client</em><em>]</em>) – The boto3 EMR client used to interact with AWS EMR.
      - <strong>context_injector</strong> (<em>Optional</em><em>[</em>[*PipesContextInjector*](../dagster/pipes.mdx#dagster.PipesContextInjector)<em>]</em>) – A context injector to use to inject context into AWS EMR job. Defaults to `PipesEnvContextInjector`.
      - <strong>forward_termination</strong> (<em>bool</em>) – Whether to cancel the EMR job if the Dagster process receives a termination signal.
      - <strong>wait_for_s3_logs_seconds</strong> (<em>int</em>) – The number of seconds to wait for S3 logs to be written after execution completes.
      - <strong>s3_application_logs_prefix</strong> (<em>str</em>) – The prefix to use when looking for application logs in S3. Defaults to <cite>containers</cite>. Another common value is <cite>steps</cite> (for non-yarn clusters).


    <dl>
        <dt><Link class="anchor" id='dagster_aws.pipes.PipesEMRClient.run'>run <a href='https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-aws/dagster_aws/pipes/clients/emr.py#L89' className='source-link' target='_blank' rel='noopener noreferrer'>[source]</a><a href="#dagster_aws.pipes.PipesEMRClient.run" class="hash-link"></a></Link></dt>
        <dd>

        Run a job on AWS EMR, enriched with the pipes protocol.

        Starts a new EMR cluster for each invocation.

        Parameters:
          - <strong>context</strong> (<em>Union</em><em>[</em>[*OpExecutionContext*](../dagster/execution.mdx#dagster.OpExecutionContext)<em>, </em>[*AssetExecutionContext*](../dagster/execution.mdx#dagster.AssetExecutionContext)<em>]</em>) – The context of the currently executing Dagster op or asset.
          - <strong>run_job_flow_params</strong> (<em>Optional</em><em>[</em><em>dict</em><em>]</em>) – Parameters for the `run_job_flow` boto3 EMR client call. See [Boto3 EMR API Documentation](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/emr.html)
          - <strong>extras</strong> (<em>Optional</em><em>[</em><em>Dict</em><em>[</em><em>str</em><em>, </em><em>Any</em><em>]</em><em>]</em>) – Additional information to pass to the Pipes session in the external process.


        Returns: Wrapper containing results reported by the external process.Return type: PipesClientCompletedInvocation

        </dd>

    </dl>

    </dd>

</dl>
<dl>
    <dt><Link class="anchor" id='dagster_aws.pipes.PipesEMRContainersClient'>`class` dagster_aws.pipes.PipesEMRContainersClient <a href='https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-aws/dagster_aws/pipes/clients/emr_containers.py#L35' className='source-link' target='_blank' rel='noopener noreferrer'>[source]</a><a href="#dagster_aws.pipes.PipesEMRContainersClient" class="hash-link"></a></Link></dt>
    <dd>

        :::info[preview]
        This API is currently in preview, and may have breaking changes in patch version releases. This API is not considered ready for production use.


        :::

    A pipes client for running workloads on AWS EMR Containers.

    Parameters:
      - <strong>client</strong> (<em>Optional</em><em>[</em><em>boto3.client</em><em>]</em>) – The boto3 AWS EMR containers client used to interact with AWS EMR Containers.
      - <strong>context_injector</strong> (<em>Optional</em><em>[</em>[*PipesContextInjector*](../dagster/pipes.mdx#dagster.PipesContextInjector)<em>]</em>) – A context injector to use to inject context into AWS EMR Containers workload. Defaults to `PipesEnvContextInjector`.
      - <strong>message_reader</strong> (<em>Optional</em><em>[</em>[*PipesMessageReader*](../dagster/pipes.mdx#dagster.PipesMessageReader)<em>]</em>) – A message reader to use to read messages from the AWS EMR Containers workload. It’s recommended to use [`PipesS3MessageReader`](#dagster_aws.pipes.PipesS3MessageReader).
      - <strong>forward_termination</strong> (<em>bool</em>) – Whether to cancel the AWS EMR Containers workload if the Dagster process receives a termination signal.
      - <strong>pipes_params_bootstrap_method</strong> (<em>Literal</em><em>[</em><em>"args"</em><em>, </em><em>"env"</em><em>]</em>) – The method to use to inject parameters into the AWS EMR Containers workload. Defaults to “args”.
      - <strong>waiter_config</strong> (<em>Optional</em><em>[</em><em>WaiterConfig</em><em>]</em>) – Optional waiter configuration to use. Defaults to 70 days (Delay: 6, MaxAttempts: 1000000).


    <dl>
        <dt><Link class="anchor" id='dagster_aws.pipes.PipesEMRContainersClient.run'>run <a href='https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-aws/dagster_aws/pipes/clients/emr_containers.py#L85' className='source-link' target='_blank' rel='noopener noreferrer'>[source]</a><a href="#dagster_aws.pipes.PipesEMRContainersClient.run" class="hash-link"></a></Link></dt>
        <dd>

        Run a workload on AWS EMR Containers, enriched with the pipes protocol.

        Parameters:
          - <strong>context</strong> (<em>Union</em><em>[</em>[*OpExecutionContext*](../dagster/execution.mdx#dagster.OpExecutionContext)<em>, </em>[*AssetExecutionContext*](../dagster/execution.mdx#dagster.AssetExecutionContext)<em>]</em>) – The context of the currently executing Dagster op or asset.
          - <strong>params</strong> (<em>dict</em>) – Parameters for the `start_job_run` boto3 AWS EMR Containers client call. See [Boto3 EMR Containers API Documentation](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/emr-containers/client/start_job_run.html)
          - <strong>extras</strong> (<em>Optional</em><em>[</em><em>Dict</em><em>[</em><em>str</em><em>, </em><em>Any</em><em>]</em><em>]</em>) – Additional information to pass to the Pipes session in the external process.


        Returns: Wrapper containing results reported by the external
        process.Return type: PipesClientCompletedInvocation

        </dd>

    </dl>

    </dd>

</dl>
<dl>
    <dt><Link class="anchor" id='dagster_aws.pipes.PipesEMRServerlessClient'>`class` dagster_aws.pipes.PipesEMRServerlessClient <a href='https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-aws/dagster_aws/pipes/clients/emr_serverless.py#L38' className='source-link' target='_blank' rel='noopener noreferrer'>[source]</a><a href="#dagster_aws.pipes.PipesEMRServerlessClient" class="hash-link"></a></Link></dt>
    <dd>

    A pipes client for running workloads on AWS EMR Serverless.

    Parameters:
      - <strong>client</strong> (<em>Optional</em><em>[</em><em>boto3.client</em><em>]</em>) – The boto3 AWS EMR Serverless client used to interact with AWS EMR Serverless.
      - <strong>context_injector</strong> (<em>Optional</em><em>[</em>[*PipesContextInjector*](../dagster/pipes.mdx#dagster.PipesContextInjector)<em>]</em>) – A context injector to use to inject context into AWS EMR Serverless workload. Defaults to `PipesEnvContextInjector`.
      - <strong>message_reader</strong> (<em>Optional</em><em>[</em>[*PipesMessageReader*](../dagster/pipes.mdx#dagster.PipesMessageReader)<em>]</em>) – A message reader to use to read messages from the AWS EMR Serverless workload. Defaults to [`PipesCloudWatchMessageReader`](#dagster_aws.pipes.PipesCloudWatchMessageReader).
      - <strong>forward_termination</strong> (<em>bool</em>) – Whether to cancel the AWS EMR Serverless workload if the Dagster process receives a termination signal.
      - <strong>poll_interval</strong> (<em>float</em>) – The interval in seconds to poll the AWS EMR Serverless workload for status updates. Defaults to 5 seconds.


    <dl>
        <dt><Link class="anchor" id='dagster_aws.pipes.PipesEMRServerlessClient.run'>run <a href='https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-aws/dagster_aws/pipes/clients/emr_serverless.py#L86' className='source-link' target='_blank' rel='noopener noreferrer'>[source]</a><a href="#dagster_aws.pipes.PipesEMRServerlessClient.run" class="hash-link"></a></Link></dt>
        <dd>

        Run a workload on AWS EMR Serverless, enriched with the pipes protocol.

        Parameters:
          - <strong>context</strong> (<em>Union</em><em>[</em>[*OpExecutionContext*](../dagster/execution.mdx#dagster.OpExecutionContext)<em>, </em>[*AssetExecutionContext*](../dagster/execution.mdx#dagster.AssetExecutionContext)<em>]</em>) – The context of the currently executing Dagster op or asset.
          - <strong>params</strong> (<em>dict</em>) – Parameters for the `start_job_run` boto3 AWS EMR Serverless client call. See [Boto3 EMR Serverless API Documentation](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/emr-serverless/client/start_job_run.html)
          - <strong>extras</strong> (<em>Optional</em><em>[</em><em>Dict</em><em>[</em><em>str</em><em>, </em><em>Any</em><em>]</em><em>]</em>) – Additional information to pass to the Pipes session in the external process.


        Returns: Wrapper containing results reported by the external
        process.Return type: PipesClientCompletedInvocation

        </dd>

    </dl>

    </dd>

</dl>
</div></div>

<div class="section" id="legacy">

## Legacy

<dl>
    <dt><Link class="anchor" id='dagster_aws.s3.ConfigurablePickledObjectS3IOManager'>dagster_aws.s3.ConfigurablePickledObjectS3IOManager IOManagerDefinition <a href='https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-aws/dagster_aws/s3/io_manager.py#L153' className='source-link' target='_blank' rel='noopener noreferrer'>[source]</a><a href="#dagster_aws.s3.ConfigurablePickledObjectS3IOManager" class="hash-link"></a></Link></dt>
    <dd>

        <div className='lineblock'> </div>

        :::warning[deprecated]
        This API will be removed in version 2.0.
         Please use S3PickleIOManager instead..

        :::

    Renamed to S3PickleIOManager. See S3PickleIOManager for documentation.


    </dd>

</dl>
<dl>
    <dt><Link class="anchor" id='dagster_aws.s3.s3_resource'>dagster_aws.s3.s3_resource ResourceDefinition <a href='https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-aws/dagster_aws/s3/resources.py#L106' className='source-link' target='_blank' rel='noopener noreferrer'>[source]</a><a href="#dagster_aws.s3.s3_resource" class="hash-link"></a></Link></dt>
    <dd>

        <div className='lineblock'> </div>

    Resource that gives access to S3.

    The underlying S3 session is created by calling
    `boto3.session.Session(profile_name)`.
    The returned resource object is an S3 client, an instance of <cite>botocore.client.S3</cite>.



    Example:

        ```python
        from dagster import build_op_context, job, op
        from dagster_aws.s3 import s3_resource

        @op(required_resource_keys={'s3'})
        def example_s3_op(context):
            return context.resources.s3.list_objects_v2(
                Bucket='my-bucket',
                Prefix='some-key'
            )

        @job(resource_defs={'s3': s3_resource})
        def example_job():
            example_s3_op()

        example_job.execute_in_process(
            run_config={
                'resources': {
                    's3': {
                        'config': {
                            'region_name': 'us-west-1',
                        }
                    }
                }
            }
        )
        ```
    Note that your ops must also declare that they require this resource with
    <cite>required_resource_keys</cite>, or it will not be initialized for the execution of their compute
    functions.

    You may configure this resource as follows:

        ```YAML
        resources:
          s3:
            config:
              region_name: "us-west-1"
              # Optional[str]: Specifies a custom region for the S3 session. Default is chosen
              # through the ordinary boto credential chain.
              use_unsigned_session: false
              # Optional[bool]: Specifies whether to use an unsigned S3 session. Default: True
              endpoint_url: "http://localhost"
              # Optional[str]: Specifies a custom endpoint for the S3 session. Default is None.
              profile_name: "dev"
              # Optional[str]: Specifies a custom profile for S3 session. Default is default
              # profile as specified in ~/.aws/credentials file
              use_ssl: true
              # Optional[bool]: Whether or not to use SSL. By default, SSL is used.
              verify: None
              # Optional[str]: Whether or not to verify SSL certificates. By default SSL certificates are verified.
              # You can also specify this argument if you want to use a different CA cert bundle than the one used by botocore."
              aws_access_key_id: None
              # Optional[str]: The access key to use when creating the client.
              aws_secret_access_key: None
              # Optional[str]: The secret key to use when creating the client.
              aws_session_token: None
              # Optional[str]:  The session token to use when creating the client.
        ```

    </dd>

</dl>
<dl>
    <dt><Link class="anchor" id='dagster_aws.s3.s3_pickle_io_manager'>dagster_aws.s3.s3_pickle_io_manager IOManagerDefinition <a href='https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-aws/dagster_aws/s3/io_manager.py#L163' className='source-link' target='_blank' rel='noopener noreferrer'>[source]</a><a href="#dagster_aws.s3.s3_pickle_io_manager" class="hash-link"></a></Link></dt>
    <dd>

        <div className='lineblock'> </div>

    Persistent IO manager using S3 for storage.

    Serializes objects via pickling. Suitable for objects storage for distributed executors, so long
    as each execution node has network connectivity and credentials for S3 and the backing bucket.

    Assigns each op output to a unique filepath containing run ID, step key, and output name.
    Assigns each asset to a single filesystem path, at “\<base_dir>/\<asset_key>”. If the asset key
    has multiple components, the final component is used as the name of the file, and the preceding
    components as parent directories under the base_dir.

    Subsequent materializations of an asset will overwrite previous materializations of that asset.
    With a base directory of “/my/base/path”, an asset with key
    <cite>AssetKey([“one”, “two”, “three”])</cite> would be stored in a file called “three” in a directory
    with path “/my/base/path/one/two/”.

    Example usage:

    1. Attach this IO manager to a set of assets.
        ```python
        from dagster import Definitions, asset
        from dagster_aws.s3 import s3_pickle_io_manager, s3_resource


        @asset
        def asset1():
            # create df ...
            return df

        @asset
        def asset2(asset1):
            return asset1[:5]

        Definitions(
            assets=[asset1, asset2],
            resources={
                "io_manager": s3_pickle_io_manager.configured(
                    {"s3_bucket": "my-cool-bucket", "s3_prefix": "my-cool-prefix"}
                ),
                "s3": s3_resource,
            },
        )
        ```
    2. Attach this IO manager to your job to make it available to your ops.
        ```python
        from dagster import job
        from dagster_aws.s3 import s3_pickle_io_manager, s3_resource

        @job(
            resource_defs={
                "io_manager": s3_pickle_io_manager.configured(
                    {"s3_bucket": "my-cool-bucket", "s3_prefix": "my-cool-prefix"}
                ),
                "s3": s3_resource,
            },
        )
        def my_job():
            ...
        ```

    </dd>

</dl>
<dl>
    <dt><Link class="anchor" id='dagster_aws.s3.s3_file_manager'>dagster_aws.s3.s3_file_manager ResourceDefinition <a href='https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-aws/dagster_aws/s3/resources.py#L208' className='source-link' target='_blank' rel='noopener noreferrer'>[source]</a><a href="#dagster_aws.s3.s3_file_manager" class="hash-link"></a></Link></dt>
    <dd>

        <div className='lineblock'> </div>

    FileManager that provides abstract access to S3.

    Implements the [`FileManager`](../dagster/internals.mdx#dagster._core.storage.file_manager.FileManager) API.


    </dd>

</dl>
<dl>
    <dt><Link class="anchor" id='dagster_aws.redshift.redshift_resource'>dagster_aws.redshift.redshift_resource ResourceDefinition <a href='https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-aws/dagster_aws/redshift/resources.py#L361' className='source-link' target='_blank' rel='noopener noreferrer'>[source]</a><a href="#dagster_aws.redshift.redshift_resource" class="hash-link"></a></Link></dt>
    <dd>

        <div className='lineblock'> </div>

    This resource enables connecting to a Redshift cluster and issuing queries against that
    cluster.



    Example:

        ```python
        from dagster import build_op_context, op
        from dagster_aws.redshift import redshift_resource

        @op(required_resource_keys={'redshift'})
        def example_redshift_op(context):
            return context.resources.redshift.execute_query('SELECT 1', fetch_results=True)

        redshift_configured = redshift_resource.configured({
            'host': 'my-redshift-cluster.us-east-1.redshift.amazonaws.com',
            'port': 5439,
            'user': 'dagster',
            'password': 'dagster',
            'database': 'dev',
        })
        context = build_op_context(resources={'redshift': redshift_configured})
        assert example_redshift_op(context) == [(1,)]
        ```

    </dd>

</dl>
<dl>
    <dt><Link class="anchor" id='dagster_aws.redshift.fake_redshift_resource'>dagster_aws.redshift.fake_redshift_resource ResourceDefinition <a href='https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-aws/dagster_aws/redshift/resources.py#L394' className='source-link' target='_blank' rel='noopener noreferrer'>[source]</a><a href="#dagster_aws.redshift.fake_redshift_resource" class="hash-link"></a></Link></dt>
    <dd>

        <div className='lineblock'> </div>


    </dd>

</dl>
<dl>
    <dt><Link class="anchor" id='dagster_aws.secretsmanager.secretsmanager_resource'>dagster_aws.secretsmanager.secretsmanager_resource ResourceDefinition <a href='https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-aws/dagster_aws/secretsmanager/resources.py#L79' className='source-link' target='_blank' rel='noopener noreferrer'>[source]</a><a href="#dagster_aws.secretsmanager.secretsmanager_resource" class="hash-link"></a></Link></dt>
    <dd>

        <div className='lineblock'> </div>

        :::info[beta]
        This API is currently in beta, and may have breaking changes in minor version releases, with behavior changes in patch releases.


        :::

    Resource that gives access to AWS SecretsManager.

    The underlying SecretsManager session is created by calling
    `boto3.session.Session(profile_name)`.
    The returned resource object is a SecretsManager client, an instance of <cite>botocore.client.SecretsManager</cite>.



    Example:

        ```python
        from dagster import build_op_context, job, op
        from dagster_aws.secretsmanager import secretsmanager_resource

        @op(required_resource_keys={'secretsmanager'})
        def example_secretsmanager_op(context):
            return context.resources.secretsmanager.get_secret_value(
                SecretId='arn:aws:secretsmanager:region:aws_account_id:secret:appauthexample-AbCdEf'
            )

        @job(resource_defs={'secretsmanager': secretsmanager_resource})
        def example_job():
            example_secretsmanager_op()

        example_job.execute_in_process(
            run_config={
                'resources': {
                    'secretsmanager': {
                        'config': {
                            'region_name': 'us-west-1',
                        }
                    }
                }
            }
        )
        ```
    You may configure this resource as follows:

        ```YAML
        resources:
          secretsmanager:
            config:
              region_name: "us-west-1"
              # Optional[str]: Specifies a custom region for the SecretsManager session. Default is chosen
              # through the ordinary boto credential chain.
              profile_name: "dev"
              # Optional[str]: Specifies a custom profile for SecretsManager session. Default is default
              # profile as specified in ~/.aws/credentials file
        ```

    </dd>

</dl>
<dl>
    <dt><Link class="anchor" id='dagster_aws.secretsmanager.secretsmanager_secrets_resource'>dagster_aws.secretsmanager.secretsmanager_secrets_resource ResourceDefinition <a href='https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-aws/dagster_aws/secretsmanager/resources.py#L257' className='source-link' target='_blank' rel='noopener noreferrer'>[source]</a><a href="#dagster_aws.secretsmanager.secretsmanager_secrets_resource" class="hash-link"></a></Link></dt>
    <dd>

        <div className='lineblock'> </div>

        :::info[beta]
        This API is currently in beta, and may have breaking changes in minor version releases, with behavior changes in patch releases.


        :::

    Resource that provides a dict which maps selected SecretsManager secrets to
    their string values. Also optionally sets chosen secrets as environment variables.



    Example:

        ```python
        import os
        from dagster import build_op_context, job, op
        from dagster_aws.secretsmanager import secretsmanager_secrets_resource

        @op(required_resource_keys={'secrets'})
        def example_secretsmanager_secrets_op(context):
            return context.resources.secrets.get("my-secret-name")

        @op(required_resource_keys={'secrets'})
        def example_secretsmanager_secrets_op_2(context):
            return os.getenv("my-other-secret-name")

        @job(resource_defs={'secrets': secretsmanager_secrets_resource})
        def example_job():
            example_secretsmanager_secrets_op()
            example_secretsmanager_secrets_op_2()

        example_job.execute_in_process(
            run_config={
                'resources': {
                    'secrets': {
                        'config': {
                            'region_name': 'us-west-1',
                            'secrets_tag': 'dagster',
                            'add_to_environment': True,
                        }
                    }
                }
            }
        )
        ```
    Note that your ops must also declare that they require this resource with
    <cite>required_resource_keys</cite>, or it will not be initialized for the execution of their compute
    functions.

    You may configure this resource as follows:

        ```YAML
        resources:
          secretsmanager:
            config:
              region_name: "us-west-1"
              # Optional[str]: Specifies a custom region for the SecretsManager session. Default is chosen
              # through the ordinary boto credential chain.
              profile_name: "dev"
              # Optional[str]: Specifies a custom profile for SecretsManager session. Default is default
              # profile as specified in ~/.aws/credentials file
              secrets: ["arn:aws:secretsmanager:region:aws_account_id:secret:appauthexample-AbCdEf"]
              # Optional[List[str]]: Specifies a list of secret ARNs to pull from SecretsManager.
              secrets_tag: "dagster"
              # Optional[str]: Specifies a tag, all secrets which have the tag set will be pulled
              # from SecretsManager.
              add_to_environment: true
              # Optional[bool]: Whether to set the selected secrets as environment variables. Defaults
              # to false.
        ```

    </dd>

</dl>
</div></div>
