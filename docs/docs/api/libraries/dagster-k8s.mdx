---
title: 'kubernetes (dagster-k8s)'
title_meta: 'kubernetes (dagster-k8s) API Documentation - Build Better Data Pipelines | Python Reference Documentation for Dagster'
description: 'kubernetes (dagster-k8s) Dagster API | Comprehensive Python API documentation for Dagster, the data orchestration platform. Learn how to build, test, and maintain data pipelines with our detailed guides and examples.'
last_update:
  date: '2025-12-05'
custom_edit_url: null
---

<div class="section" id="kubernetes-dagster-k8s">

# Kubernetes (dagster-k8s)

See also the [Kubernetes deployment guide](https://docs.dagster.io/deployment/oss/deployment-options/kubernetes).

This library contains utilities for running Dagster with Kubernetes. This includes a Python API
allowing the webserver to launch runs as Kubernetes Jobs, as well as a Helm chart you can use as the basis
for a Dagster deployment on a Kubernetes cluster.

<div class="section" id="apis">

## APIs

<dl>
    <dt><Link class="anchor" id='dagster_k8s.K8sRunLauncher'>dagster_k8s.K8sRunLauncher RunLauncher <a href='https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-k8s/dagster_k8s/launcher.py#L24' className='source-link' target='_blank' rel='noopener noreferrer'>[source]</a><a href="#dagster_k8s.K8sRunLauncher" class="hash-link"></a></Link></dt>
    <dd>

        <div className='lineblock'> </div>

    RunLauncher that starts a Kubernetes Job for each Dagster job run.

    Encapsulates each run in a separate, isolated invocation of `dagster-graphql`.

    You can configure a Dagster instance to use this RunLauncher by adding a section to your
    `dagster.yaml` like the following:

        ```yaml
        run_launcher:
          module: dagster_k8s.launcher
          class: K8sRunLauncher
          config:
            service_account_name: your_service_account
            job_image: my_project/dagster_image:latest
            instance_config_map: dagster-instance
            postgres_password_secret: dagster-postgresql-secret
        ```

    </dd>

</dl>
<dl>
    <dt><Link class="anchor" id='dagster_k8s.k8s_job_executor'>dagster_k8s.k8s_job_executor ExecutorDefinition <a href='https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-k8s/dagster_k8s/executor.py#L105' className='source-link' target='_blank' rel='noopener noreferrer'>[source]</a><a href="#dagster_k8s.k8s_job_executor" class="hash-link"></a></Link></dt>
    <dd>

        <div className='lineblock'> </div>

    Executor which launches steps as Kubernetes Jobs.

    To use the <cite>k8s_job_executor</cite>, set it as the <cite>executor_def</cite> when defining a job:

        ```python
        from dagster_k8s import k8s_job_executor

        from dagster import job

        @job(executor_def=k8s_job_executor)
        def k8s_job():
            pass
        ```
    Then you can configure the executor with run config as follows:

        ```YAML
        execution:
          config:
            job_namespace: 'some-namespace'
            image_pull_policy: ...
            image_pull_secrets: ...
            service_account_name: ...
            env_config_maps: ...
            env_secrets: ...
            env_vars: ...
            job_image: ... # leave out if using userDeployments
            max_concurrent: ...
        ```
    <cite>max_concurrent</cite> limits the number of pods that will execute concurrently for one run. By default
    there is no limit- it will maximally parallel as allowed by the DAG. Note that this is not a
    global limit.

    Configuration set on the Kubernetes Jobs and Pods created by the <cite>K8sRunLauncher</cite> will also be
    set on Kubernetes Jobs and Pods created by the <cite>k8s_job_executor</cite>.

    Configuration set using <cite>tags</cite> on a <cite>@job</cite> will only apply to the <cite>run</cite> level. For configuration
    to apply at each <cite>step</cite> it must be set using <cite>tags</cite> for each <cite>@op</cite>.


    </dd>

</dl>
</div>

<div class="section" id="ops">

## Ops

<dl>
    <dt><Link class="anchor" id='dagster_k8s.k8s_job_op'>dagster_k8s.k8s_job_op `=` \<dagster._core.definitions.op_definition.OpDefinition object> <a href='https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-k8s/dagster_k8s/ops/k8s_job_op.py#L458' className='source-link' target='_blank' rel='noopener noreferrer'>[source]</a><a href="#dagster_k8s.k8s_job_op" class="hash-link"></a></Link></dt>
    <dd>

        <div className='lineblock'> </div>

        :::info[beta]
        This API is currently in beta, and may have breaking changes in minor version releases, with behavior changes in patch releases.


        :::

    An op that runs a Kubernetes job using the k8s API.

    Contrast with the <cite>k8s_job_executor</cite>, which runs each Dagster op in a Dagster job in its
    own k8s job.

    This op may be useful when:
          - You need to orchestrate a command that isn’t a Dagster op (or isn’t written in Python)
          - You want to run the rest of a Dagster job using a specific executor, and only a single op in k8s.


    For example:

        ```python
        from dagster_k8s import k8s_job_op

        from dagster import job

        first_op = k8s_job_op.configured(
            {
                "image": "busybox",
                "command": ["/bin/sh", "-c"],
                "args": ["echo HELLO"],
            },
            name="first_op",
        )
        second_op = k8s_job_op.configured(
            {
                "image": "busybox",
                "command": ["/bin/sh", "-c"],
                "args": ["echo GOODBYE"],
            },
            name="second_op",
        )

        @job
        def full_job():
            second_op(first_op())
        ```
    You can create your own op with the same implementation by calling the <cite>execute_k8s_job</cite> function
    inside your own op.

    The service account that is used to run this job should have the following RBAC permissions:

        ```YAML
        rules:
          - apiGroups: ["batch"]
              resources: ["jobs", "jobs/status"]
              verbs: ["*"]
          # The empty arg "" corresponds to the core API group
          - apiGroups: [""]
              resources: ["pods", "pods/log", "pods/status"]
              verbs: ["*"]'
        ```

    </dd>

</dl>
<dl>
    <dt><Link class="anchor" id='dagster_k8s.execute_k8s_job'>dagster_k8s.execute_k8s_job <a href='https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-k8s/dagster_k8s/ops/k8s_job_op.py#L138' className='source-link' target='_blank' rel='noopener noreferrer'>[source]</a><a href="#dagster_k8s.execute_k8s_job" class="hash-link"></a></Link></dt>
    <dd>

        :::info[beta]
        This API is currently in beta, and may have breaking changes in minor version releases, with behavior changes in patch releases.


        :::

    This function is a utility for executing a Kubernetes job from within a Dagster op.

    Parameters:
      - <strong>image</strong> (<em>str</em>) – The image in which to launch the k8s job.
      - <strong>command</strong> (<em>Optional</em><em>[</em><em>List</em><em>[</em><em>str</em><em>]</em><em>]</em>) – The command to run in the container within the launched k8s job. Default: None.
      - <strong>args</strong> (<em>Optional</em><em>[</em><em>List</em><em>[</em><em>str</em><em>]</em><em>]</em>) – The args for the command for the container. Default: None.
      - <strong>namespace</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) – Override the kubernetes namespace in which to run the k8s job. Default: None.
      - <strong>image_pull_policy</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) – Allows the image pull policy to be overridden, e.g. to facilitate local testing with [kind](https://kind.sigs.k8s.io/). Default: `"Always"`. See: [https://kubernetes.io/docs/concepts/containers/images/#updating-images](https://kubernetes.io/docs/concepts/containers/images/#updating-images).
      - <strong>image_pull_secrets</strong> (<em>Optional</em><em>[</em><em>List</em><em>[</em><em>Dict</em><em>[</em><em>str</em><em>, </em><em>str</em><em>]</em><em>]</em><em>]</em>) – Optionally, a list of dicts, each of which corresponds to a Kubernetes `LocalObjectReference` (e.g., `\{'name': 'myRegistryName'}`). This allows you to specify the ``imagePullSecrets` on a pod basis. Typically, these will be provided through the service account, when needed, and you will not need to pass this argument. See: [https://kubernetes.io/docs/concepts/containers/images/#specifying-imagepullsecrets-on-a-pod](https://kubernetes.io/docs/concepts/containers/images/#specifying-imagepullsecrets-on-a-pod) and [https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.17/#podspec-v1-core](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.17/#podspec-v1-core)
      - <strong>service_account_name</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) – The name of the Kubernetes service account under which to run the Job. Defaults to “default”        env_config_maps (Optional[List[str]]): A list of custom ConfigMapEnvSource names from which to draw environment variables (using `envFrom`) for the Job. Default: `[]`. See: [https://kubernetes.io/docs/tasks/inject-data-application/define-environment-variable-container/#define-an-environment-variable-for-a-container](https://kubernetes.io/docs/tasks/inject-data-application/define-environment-variable-container/#define-an-environment-variable-for-a-container)
      - <strong>env_secrets</strong> (<em>Optional</em><em>[</em><em>List</em><em>[</em><em>str</em><em>]</em><em>]</em>) – A list of custom Secret names from which to draw environment variables (using `envFrom`) for the Job. Default: `[]`. See: [https://kubernetes.io/docs/tasks/inject-data-application/distribute-credentials-secure/#configure-all-key-value-pairs-in-a-secret-as-container-environment-variables](https://kubernetes.io/docs/tasks/inject-data-application/distribute-credentials-secure/#configure-all-key-value-pairs-in-a-secret-as-container-environment-variables)
      - <strong>env_vars</strong> (<em>Optional</em><em>[</em><em>List</em><em>[</em><em>str</em><em>]</em><em>]</em>) – A list of environment variables to inject into the Job. Default: `[]`. See: [https://kubernetes.io/docs/tasks/inject-data-application/distribute-credentials-secure/#configure-all-key-value-pairs-in-a-secret-as-container-environment-variables](https://kubernetes.io/docs/tasks/inject-data-application/distribute-credentials-secure/#configure-all-key-value-pairs-in-a-secret-as-container-environment-variables)
      - <strong>volume_mounts</strong> (<em>Optional</em><em>[</em><em>List</em><em>[</em>[*Permissive*](../dagster/config.mdx#dagster.Permissive)<em>]</em><em>]</em>) – A list of volume mounts to include in the job’s container. Default: `[]`. See: [https://v1-18.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.18/#volumemount-v1-core](https://v1-18.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.18/#volumemount-v1-core)
      - <strong>volumes</strong> (<em>Optional</em><em>[</em><em>List</em><em>[</em>[*Permissive*](../dagster/config.mdx#dagster.Permissive)<em>]</em><em>]</em>) – A list of volumes to include in the Job’s Pod. Default: `[]`. See: [https://v1-18.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.18/#volume-v1-core](https://v1-18.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.18/#volume-v1-core)
      - <strong>labels</strong> (<em>Optional</em><em>[</em><em>Dict</em><em>[</em><em>str</em><em>, </em><em>str</em><em>]</em><em>]</em>) – Additional labels that should be included in the Job’s Pod. See: [https://kubernetes.io/docs/concepts/overview/working-with-objects/labels](https://kubernetes.io/docs/concepts/overview/working-with-objects/labels)
      - <strong>resources</strong> (<em>Optional</em><em>[</em><em>Dict</em><em>[</em><em>str</em><em>, </em><em>Any</em><em>]</em><em>]</em>) – [https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/](https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/)
      - <strong>scheduler_name</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) – Use a custom Kubernetes scheduler for launched Pods. See: [https://kubernetes.io/docs/tasks/extend-kubernetes/configure-multiple-schedulers/](https://kubernetes.io/docs/tasks/extend-kubernetes/configure-multiple-schedulers/)
      - <strong>load_incluster_config</strong> (<em>bool</em>) – Whether the op is running within a k8s cluster. If `True`, we assume the launcher is running within the target cluster and load config using `kubernetes.config.load_incluster_config`. Otherwise, we will use the k8s config specified in `kubeconfig_file` (using `kubernetes.config.load_kube_config`) or fall back to the default kubeconfig. Default: True,
      - <strong>kubeconfig_file</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) – The kubeconfig file from which to load config. Defaults to using the default kubeconfig. Default: None.
      - <strong>timeout</strong> (<em>Optional</em><em>[</em><em>int</em><em>]</em>) – Raise an exception if the op takes longer than this timeout in seconds to execute. Default: None.
      - <strong>container_config</strong> (<em>Optional</em><em>[</em><em>Dict</em><em>[</em><em>str</em><em>, </em><em>Any</em><em>]</em><em>]</em>) – Raw k8s config for the k8s pod’s main container ([https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.30/#container-v1-core](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.30/#container-v1-core)). Keys can either snake_case or camelCase.Default: None.
      - <strong>pod_template_spec_metadata</strong> (<em>Optional</em><em>[</em><em>Dict</em><em>[</em><em>str</em><em>, </em><em>Any</em><em>]</em><em>]</em>) – Raw k8s config for the k8s pod’s metadata ([https://kubernetes.io/docs/reference/kubernetes-api/common-definitions/object-meta/#ObjectMeta](https://kubernetes.io/docs/reference/kubernetes-api/common-definitions/object-meta/#ObjectMeta)). Keys can either snake_case or camelCase. Default: None.
      - <strong>pod_spec_config</strong> (<em>Optional</em><em>[</em><em>Dict</em><em>[</em><em>str</em><em>, </em><em>Any</em><em>]</em><em>]</em>) – Raw k8s config for the k8s pod’s pod spec ([https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/pod-v1/#PodSpec](https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/pod-v1/#PodSpec)). Keys can either snake_case or camelCase. Default: None.
      - <strong>job_metadata</strong> (<em>Optional</em><em>[</em><em>Dict</em><em>[</em><em>str</em><em>, </em><em>Any</em><em>]</em><em>]</em>) – Raw k8s config for the k8s job’s metadata ([https://kubernetes.io/docs/reference/kubernetes-api/common-definitions/object-meta/#ObjectMeta](https://kubernetes.io/docs/reference/kubernetes-api/common-definitions/object-meta/#ObjectMeta)). Keys can either snake_case or camelCase. Default: None.
      - <strong>job_spec_config</strong> (<em>Optional</em><em>[</em><em>Dict</em><em>[</em><em>str</em><em>, </em><em>Any</em><em>]</em><em>]</em>) – Raw k8s config for the k8s job’s job spec ([https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.30/#jobspec-v1-batch](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.30/#jobspec-v1-batch)). Keys can either snake_case or camelCase.Default: None.
      - <strong>k8s_job_name</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) – Overrides the name of the k8s job. If not set, will be set to a unique name based on the current run ID and the name of the calling op. If set, make sure that the passed in name is a valid Kubernetes job name that does not already exist in the cluster.
      - <strong>merge_behavior</strong> (<em>Optional</em><em>[</em><em>K8sConfigMergeBehavior</em><em>]</em>) – How raw k8s config set on this op should be merged with any raw k8s config set on the code location that launched the op. By default, the value is K8sConfigMergeBehavior.DEEP, meaning that the two dictionaries are recursively merged, appending list fields together and merging dictionary fields. Setting it to SHALLOW will make the dictionaries shallowly merged - any shared values in the dictionaries will be replaced by the values set on this op.
      - <strong>delete_failed_k8s_jobs</strong> (<em>bool</em>) – Whether to immediately delete failed Kubernetes jobs. If False, failed jobs will remain accessible through the Kubernetes API until deleted by a user or cleaned up by the .spec.ttlSecondsAfterFinished parameter of the job. ([https://kubernetes.io/docs/concepts/workloads/controllers/ttlafterfinished/](https://kubernetes.io/docs/concepts/workloads/controllers/ttlafterfinished/)). Defaults to True.



    </dd>

</dl>
</div>

<div class="section" id="python-api">

## Python API

The `K8sRunLauncher` allows webserver instances to be configured to launch new runs by starting
per-run Kubernetes Jobs. To configure the `K8sRunLauncher`, your `dagster.yaml` should
include a section like:

    ```yaml
    run_launcher:
      module: dagster_k8s.launcher
      class: K8sRunLauncher
      config:
        image_pull_secrets:
        service_account_name: dagster
        job_image: "my-company.com/image:latest"
        dagster_home: "/opt/dagster/dagster_home"
        postgres_password_secret: "dagster-postgresql-secret"
        image_pull_policy: "IfNotPresent"
        job_namespace: "dagster"
        instance_config_map: "dagster-instance"
        env_config_maps:
          - "dagster-k8s-job-runner-env"
        env_secrets:
          - "dagster-k8s-some-secret"
        env_vars:
          - "ENV_VAR=1"
        labels:
        resources:
        run_k8s_config:
          pod_template_spec_metadata:
          pod_spec_config:
          job_metadata:
          job_spec_config:
          container_config:
        volume_mounts:
        volumes:
        security_context:
        scheduler_name:
        kubeconfig_file:
    ```

</div>

<div class="section" id="helm-chart">

## Helm chart

For local dev (e.g., on kind or minikube):

    ```shell
    helm install \
        --set dagsterWebserver.image.repository="dagster.io/buildkite-test-image" \
        --set dagsterWebserver.image.tag="py310-latest" \
        --set job_runner.image.repository="dagster.io/buildkite-test-image" \
        --set job_runner.image.tag="py310-latest" \
        --set imagePullPolicy="IfNotPresent" \
        dagster \
        helm/dagster/
    ```

Upon installation, the Helm chart will provide instructions for port forwarding
the Dagster webserver and Flower (if configured).

</div>

<div class="section" id="running-tests">

## Running tests

To run the unit tests:

    ```default
    pytest -m "not integration"
    ```

To run the integration tests, you must have [Docker](https://docs.docker.com/install),
[kind](https://kind.sigs.k8s.io/docs/user/quick-start#installation),
and [helm](https://helm.sh/docs/intro/install) installed.

On macOS:

    ```default
    brew install kind
    brew install helm
    ```

Docker must be running.

You may experience slow first test runs thanks to image pulls (run `pytest -svv --fulltrace` for
visibility). Building images and loading them to the kind cluster is slow, and there is
no visibility into the progress of the load.

<strong>NOTE:</strong> This process is quite slow, as it requires bootstrapping a local `kind` cluster with Docker
images and the `dagster-k8s` Helm chart. For faster development, you can either:

1. Keep a warm kind cluster
2. Use a remote K8s cluster, e.g. via AWS EKS or GCP GKE
   Instructions are below.

<div class="section" id="faster-local-development-with-kind">

### Faster local development (with kind)

You may find that the kind cluster creation, image loading, and kind cluster creation loop
is too slow for effective local dev.

You may bypass cluster creation and image loading in the following way. First add the `--no-cleanup`
flag to your pytest invocation:

    ```shell
    pytest --no-cleanup -s -vvv -m "not integration"
    ```

The tests will run as before, but the kind cluster will be left running after the tests are completed.

For subsequent test runs, you can run:

    ```shell
    pytest --kind-cluster="cluster-d9971c84d44d47f382a2928c8c161faa" --existing-helm-namespace="dagster-test-95590a" -s -vvv -m "not integration"
    ```

This will bypass cluster creation, image loading, and Helm chart installation, for much faster tests.

The kind cluster name and Helm namespace for this command can be found in the logs, or retrieved
via the respective CLIs, using `kind get clusters` and `kubectl get namespaces`. Note that
for `kubectl` and `helm` to work correctly with a kind cluster, you should override your
kubeconfig file location with:

    ```shell
    kind get kubeconfig --name kind-test > /tmp/kubeconfig
    export KUBECONFIG=/tmp/kubeconfig
    ```

<div class="section" id="manual-kind-cluster-setup">

#### Manual kind cluster setup

The test fixtures provided by `dagster-k8s` automate the process described below, but sometimes
it’s useful to manually configure a kind cluster and load images onto it.

First, ensure you have a Docker image appropriate for your Python version. Run, from the root of
the repo:

    ```shell
    ./python_modules/dagster-test/dagster_test/test_project/build.sh 3.7.6 \
        dagster.io.priv/buildkite-test-image:py310-latest
    ```

In the above invocation, the Python majmin version should be appropriate for your desired tests.

Then run the following commands to create the cluster and load the image. Note that there is no
feedback from the loading process.

    ```shell
    kind create cluster --name kind-test
    kind load docker-image --name kind-test dagster.io/dagster-docker-buildkite:py310-latest
    ```

If you are deploying the Helm chart with an in-cluster Postgres (rather than an external database),
and/or with dagster-celery workers (and a RabbitMQ), you’ll also want to have images present for
rabbitmq and postgresql:

    ```shell
    docker pull docker.io/bitnami/rabbitmq
    docker pull docker.io/bitnami/postgresql

    kind load docker-image --name kind-test docker.io/bitnami/rabbitmq:latest
    kind load docker-image --name kind-test docker.io/bitnami/postgresql:latest
    ```

Then you can run pytest as follows:

    ```shell
    pytest --kind-cluster=kind-test
    ```

</div></div>

<div class="section" id="faster-local-development-with-an-existing-k8s-cluster">

### Faster local development (with an existing K8s cluster)

If you already have a development K8s cluster available, you can run tests on that cluster vs.
running locally in `kind`.

For this to work, first build and deploy the test image to a registry available to your cluster.
For example, with a private ECR repository:

    ```default
    ./python_modules/dagster-test/dagster_test/test_project/build.sh 3.7.6
    docker tag dagster-docker-buildkite:latest $AWS_ACCOUNT_ID.dkr.ecr.us-west-2.amazonaws.com/dagster-k8s-tests:2020-04-21T21-04-06

    aws ecr get-login --no-include-email --region us-west-1 | sh
    docker push $AWS_ACCOUNT_ID.dkr.ecr.us-west-1.amazonaws.com/dagster-k8s-tests:2020-04-21T21-04-06
    ```

Then, you can run tests on EKS with:

    ```default
    export DAGSTER_DOCKER_IMAGE_TAG="2020-04-21T21-04-06"
    export DAGSTER_DOCKER_REPOSITORY="$AWS_ACCOUNT_ID.dkr.ecr.us-west-2.amazonaws.com"
    export DAGSTER_DOCKER_IMAGE="dagster-k8s-tests"

    # First run with --no-cleanup to leave Helm chart in place
    pytest --cluster-provider="kubeconfig" --no-cleanup -s -vvv

    # Subsequent runs against existing Helm chart
    pytest --cluster-provider="kubeconfig" --existing-helm-namespace="dagster-test-<some id>" -s -vvv
    ```

</div>

<div class="section" id="validating-helm-charts">

### Validating Helm charts

To test / validate Helm charts, you can run:

    ```shell
    helm install dagster --dry-run --debug helm/dagster
    helm lint
    ```

</div>

<div class="section" id="enabling-gcr-access-from-minikube">

### Enabling GCR access from Minikube

To enable GCR access from Minikube:

    ```shell
    kubectl create secret docker-registry element-dev-key \
        --docker-server=https://gcr.io \
        --docker-username=oauth2accesstoken \
        --docker-password="$(gcloud auth print-access-token)" \
        --docker-email=my@email.com
    ```

</div>

<div class="section" id="a-note-about-pvcs">

### A note about PVCs

Both the Postgres and the RabbitMQ Helm charts will store credentials using Persistent Volume
Claims, which will outlive test invocations and calls to `helm uninstall`. These must be deleted if
you want to change credentials. To view your pvcs, run:

    ```default
    kubectl get pvc
    ```

</div>

<div class="section" id="testing-redis">

### Testing Redis

The Redis Helm chart installs w/ a randomly-generated password by default; turn this off:

    ```default
    helm install dagredis stable/redis --set usePassword=false
    ```

Then, to connect to your database from outside the cluster execute the following commands:

    ```default
    kubectl port-forward --namespace default svc/dagredis-master 6379:6379
    redis-cli -h 127.0.0.1 -p 6379
    ```

</div></div>

<div class="section" id="pipes">

## Pipes

<dl>
    <dt><Link class="anchor" id='dagster_k8s.PipesK8sClient'>`class` dagster_k8s.PipesK8sClient <a href='https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-k8s/dagster_k8s/pipes.py#L345' className='source-link' target='_blank' rel='noopener noreferrer'>[source]</a><a href="#dagster_k8s.PipesK8sClient" class="hash-link"></a></Link></dt>
    <dd>

    A pipes client for launching kubernetes pods.

    By default context is injected via environment variables and messages are parsed out of
    the pod logs, with other logs forwarded to stdout of the orchestration process.

    The first container within the containers list of the pod spec is expected (or set) to be
    the container prepared for pipes protocol communication.

    Parameters:
      - <strong>env</strong> (<em>Optional</em><em>[</em><em>Mapping</em><em>[</em><em>str</em><em>, </em><em>str</em><em>]</em><em>]</em>) – An optional dict of environment variables to pass to the subprocess.
      - <strong>context_injector</strong> (<em>Optional</em><em>[</em>[*PipesContextInjector*](../dagster/pipes.mdx#dagster.PipesContextInjector)<em>]</em>) – A context injector to use to inject context into the k8s container process. Defaults to `PipesEnvContextInjector`.
      - <strong>message_reader</strong> (<em>Optional</em><em>[</em>[*PipesMessageReader*](../dagster/pipes.mdx#dagster.PipesMessageReader)<em>]</em>) – A message reader to use to read messages from the k8s container process. Defaults to [`PipesK8sPodLogsMessageReader`](#dagster_k8s.PipesK8sPodLogsMessageReader).
      - <strong>load_incluster_config</strong> (<em>Optional</em><em>[</em><em>bool</em><em>]</em>) – Whether this client is expected to be running from inside a kubernetes cluster and should load config using `kubernetes.config.load_incluster_config`. Otherwise `kubernetes.config.load_kube_config` is used with the kubeconfig_file argument. Default: None
      - <strong>kubeconfig_file</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) – The value to pass as the config_file argument to `kubernetes.config.load_kube_config`. Default: None.
      - <strong>kube_context</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) – The value to pass as the context argument to `kubernetes.config.load_kube_config`. Default: None.
      - <strong>poll_interval</strong> (<em>Optional</em><em>[</em><em>float</em><em>]</em>) – How many seconds to wait between requests when polling the kubernetes API Default: 10.


    <dl>
        <dt><Link class="anchor" id='dagster_k8s.PipesK8sClient.run'>run <a href='https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-k8s/dagster_k8s/pipes.py#L440' className='source-link' target='_blank' rel='noopener noreferrer'>[source]</a><a href="#dagster_k8s.PipesK8sClient.run" class="hash-link"></a></Link></dt>
        <dd>

        Publish a kubernetes pod and wait for it to complete, enriched with the pipes protocol.

        Parameters:
          - <strong>context</strong> (<em>Union</em><em>[</em>[*OpExecutionContext*](../dagster/execution.mdx#dagster.OpExecutionContext)<em>, </em>[*AssetExecutionContext*](../dagster/execution.mdx#dagster.AssetExecutionContext)<em>]</em>) – The execution context.
          - <strong>image</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) – The image to set the first container in the pod spec to use.
          - <strong>command</strong> (<em>Optional</em><em>[</em><em>Union</em><em>[</em><em>str</em><em>, </em><em>Sequence</em><em>[</em><em>str</em><em>]</em><em>]</em><em>]</em>) – The command to set the first container in the pod spec to use.
          - <strong>namespace</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) – Which kubernetes namespace to use, defaults to the current namespace if running inside a kubernetes cluster or falling back to “default”.
          - <strong>env</strong> (<em>Optional</em><em>[</em><em>Mapping</em><em>[</em><em>str</em><em>,</em><em>str</em><em>]</em><em>]</em>) – A mapping of environment variable names to values to set on the first container in the pod spec, on top of those configured on resource.
          - <strong>base_pod_meta</strong> (<em>Optional</em><em>[</em><em>Mapping</em><em>[</em><em>str</em><em>, </em><em>Any</em><em>]</em><em>]</em>) – Raw k8s config for the k8s pod’s metadata ([https://kubernetes.io/docs/reference/kubernetes-api/common-definitions/object-meta/#ObjectMeta](https://kubernetes.io/docs/reference/kubernetes-api/common-definitions/object-meta/#ObjectMeta)) Keys can either snake_case or camelCase. The name value will be overridden.
          - <strong>base_pod_spec</strong> (<em>Optional</em><em>[</em><em>Mapping</em><em>[</em><em>str</em><em>, </em><em>Any</em><em>]</em><em>]</em>) – Raw k8s config for the k8s pod’s pod spec ([https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/pod-v1/#PodSpec](https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/pod-v1/#PodSpec)). Keys can either snake_case or camelCase. The dagster context will be readable from any container within the pod, but only the first container in the <cite>pod.spec.containers</cite> will be able to communicate back to Dagster.
          - <strong>extras</strong> (<em>Optional</em><em>[</em><em>PipesExtras</em><em>]</em>) – Extra values to pass along as part of the ext protocol.
          - <strong>context_injector</strong> (<em>Optional</em><em>[</em>[*PipesContextInjector*](../dagster/pipes.mdx#dagster.PipesContextInjector)<em>]</em>) – Override the default ext protocol context injection.
          - <strong>message_reader</strong> (<em>Optional</em><em>[</em>[*PipesMessageReader*](../dagster/pipes.mdx#dagster.PipesMessageReader)<em>]</em>) – Override the default ext protocol message reader.
          - <strong>ignore_containers</strong> (<em>Optional</em><em>[</em><em>Set</em><em>]</em>) – Ignore certain containers from waiting for termination. Defaults to None.
          - <strong>enable_multi_container_logs</strong> (<em>bool</em>) – Whether or not to enable multi-container log consumption.
          - <strong>pod_wait_timeout</strong> (<em>float</em>) – How long to wait for the pod to terminate before raising an exception. Defaults to 24h. Set to 0 to disable.


        Returns:
        Wrapper containing results reported by the external
            process.

        Return type: PipesClientCompletedInvocation

        </dd>

    </dl>

    </dd>

</dl>
<dl>
    <dt><Link class="anchor" id='dagster_k8s.PipesK8sPodLogsMessageReader'>`class` dagster_k8s.PipesK8sPodLogsMessageReader <a href='https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-k8s/dagster_k8s/pipes.py#L75' className='source-link' target='_blank' rel='noopener noreferrer'>[source]</a><a href="#dagster_k8s.PipesK8sPodLogsMessageReader" class="hash-link"></a></Link></dt>
    <dd>
    Message reader that reads messages from kubernetes pod logs.
    </dd>

</dl>
</div></div>
