---
title: 'gcp (dagster-gcp)'
title_meta: 'gcp (dagster-gcp) API Documentation - Build Better Data Pipelines | Python Reference Documentation for Dagster'
description: 'gcp (dagster-gcp) Dagster API | Comprehensive Python API documentation for Dagster, the data orchestration platform. Learn how to build, test, and maintain data pipelines with our detailed guides and examples.'
last_update:
  date: '2025-12-05'
custom_edit_url: null
---

<div class="section" id="gcp-dagster-gcp">

# GCP (dagster-gcp)

<div class="section" id="bigquery">

## BigQuery

Related Guides:

- [Using Dagster with BigQuery](https://docs.dagster.io/integrations/libraries/gcp/bigquery)
- [BigQuery I/O manager reference](https://docs.dagster.io/integrations/libraries/gcp/bigquery/reference)

<div class="section" id="bigquery-resource">

### BigQuery Resource

<dl>
    <dt><Link class="anchor" id='dagster_gcp.BigQueryResource'>dagster_gcp.BigQueryResource ResourceDefinition <a href='https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-gcp/dagster_gcp/bigquery/resources.py#L19' className='source-link' target='_blank' rel='noopener noreferrer'>[source]</a><a href="#dagster_gcp.BigQueryResource" class="hash-link"></a></Link></dt>
    <dd>

        <div className='lineblock'> </div>

    Resource for interacting with Google BigQuery.



    Examples:

        ```python
        from dagster import Definitions, asset
        from dagster_gcp import BigQueryResource

        @asset
        def my_table(bigquery: BigQueryResource):
            with bigquery.get_client() as client:
                client.query("SELECT * FROM my_dataset.my_table")

        defs = Definitions(
            assets=[my_table],
            resources={
                "bigquery": BigQueryResource(project="my-project")
            }
        )
        ```

    </dd>

</dl>
</div>

<div class="section" id="bigquery-i-o-manager">

### BigQuery I/O Manager

<dl>
    <dt><Link class="anchor" id='dagster_gcp.BigQueryIOManager'>dagster_gcp.BigQueryIOManager IOManagerDefinition <a href='https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-gcp/dagster_gcp/bigquery/io_manager.py#L171' className='source-link' target='_blank' rel='noopener noreferrer'>[source]</a><a href="#dagster_gcp.BigQueryIOManager" class="hash-link"></a></Link></dt>
    <dd>

        <div className='lineblock'> </div>

    Base class for an I/O manager definition that reads inputs from and writes outputs to BigQuery.



    Examples:

        ```python
        from dagster_gcp import BigQueryIOManager
        from dagster_bigquery_pandas import BigQueryPandasTypeHandler
        from dagster import Definitions, EnvVar

        class MyBigQueryIOManager(BigQueryIOManager):
            @staticmethod
            def type_handlers() -> Sequence[DbTypeHandler]:
                return [BigQueryPandasTypeHandler()]

        @asset(
            key_prefix=["my_dataset"]  # my_dataset will be used as the dataset in BigQuery
        )
        def my_table() -> pd.DataFrame:  # the name of the asset will be the table name
            ...

        defs = Definitions(
            assets=[my_table],
            resources={
                "io_manager": MyBigQueryIOManager(project=EnvVar("GCP_PROJECT"))
            }
        )
        ```
    You can set a default dataset to store the assets using the `dataset` configuration value of the BigQuery I/O
    Manager. This dataset will be used if no other dataset is specified directly on an asset or op.

        ```python
        defs = Definitions(
            assets=[my_table],
            resources={
                    "io_manager": MyBigQueryIOManager(project=EnvVar("GCP_PROJECT"), dataset="my_dataset")
                }
        )
        ```
    On individual assets, you an also specify the dataset where they should be stored using metadata or
    by adding a `key_prefix` to the asset key. If both `key_prefix` and metadata are defined, the metadata will
    take precedence.

        ```python
        @asset(
            key_prefix=["my_dataset"]  # will be used as the dataset in BigQuery
        )
        def my_table() -> pd.DataFrame:
            ...

        @asset(
            # note that the key needs to be "schema"
            metadata={"schema": "my_dataset"}  # will be used as the dataset in BigQuery
        )
        def my_other_table() -> pd.DataFrame:
            ...
        ```
    For ops, the dataset can be specified by including a “schema” entry in output metadata.

        ```python
        @op(
            out={"my_table": Out(metadata={"schema": "my_schema"})}
        )
        def make_my_table() -> pd.DataFrame:
            ...
        ```
    If none of these is provided, the dataset will default to “public”.

    To only use specific columns of a table as input to a downstream op or asset, add the metadata `columns` to the
    [`In`](../dagster/ops.mdx#dagster.In) or [`AssetIn`](../dagster/assets.mdx#dagster.AssetIn).

        ```python
        @asset(
            ins={"my_table": AssetIn("my_table", metadata={"columns": ["a"]})}
        )
        def my_table_a(my_table: pd.DataFrame) -> pd.DataFrame:
            # my_table will just contain the data from column "a"
            ...
        ```
    If you cannot upload a file to your Dagster deployment, or otherwise cannot
    [authenticate with GCP](https://cloud.google.com/docs/authentication/provide-credentials-adc)
    via a standard method, you can provide a service account key as the `gcp_credentials` configuration.
    Dagster will store this key in a temporary file and set `GOOGLE_APPLICATION_CREDENTIALS` to point to the file.
    After the run completes, the file will be deleted, and `GOOGLE_APPLICATION_CREDENTIALS` will be
    unset. The key must be base64 encoded to avoid issues with newlines in the keys. You can retrieve
    the base64 encoded with this shell command: `cat $GOOGLE_APPLICATION_CREDENTIALS | base64`


    </dd>

</dl>
</div>

<div class="section" id="bigquery-ops">

### BigQuery Ops

<dl>
    <dt><Link class="anchor" id='dagster_gcp.bq_create_dataset'>dagster_gcp.bq_create_dataset <a href='https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-gcp/dagster_gcp/bigquery/ops.py#L147' className='source-link' target='_blank' rel='noopener noreferrer'>[source]</a><a href="#dagster_gcp.bq_create_dataset" class="hash-link"></a></Link></dt>
    <dd>

    BigQuery Create Dataset.

    This op encapsulates creating a BigQuery dataset.

    Expects a BQ client to be provisioned in resources as context.resources.bigquery.


    </dd>

</dl>
<dl>
    <dt><Link class="anchor" id='dagster_gcp.bq_delete_dataset'>dagster_gcp.bq_delete_dataset <a href='https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-gcp/dagster_gcp/bigquery/ops.py#L164' className='source-link' target='_blank' rel='noopener noreferrer'>[source]</a><a href="#dagster_gcp.bq_delete_dataset" class="hash-link"></a></Link></dt>
    <dd>

    BigQuery Delete Dataset.

    This op encapsulates deleting a BigQuery dataset.

    Expects a BQ client to be provisioned in resources as context.resources.bigquery.


    </dd>

</dl>
<dl>
    <dt><Link class="anchor" id='dagster_gcp.bq_op_for_queries'>dagster_gcp.bq_op_for_queries <a href='https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-gcp/dagster_gcp/bigquery/ops.py#L43' className='source-link' target='_blank' rel='noopener noreferrer'>[source]</a><a href="#dagster_gcp.bq_op_for_queries" class="hash-link"></a></Link></dt>
    <dd>

    Executes BigQuery SQL queries.

    Expects a BQ client to be provisioned in resources as context.resources.bigquery.


    </dd>

</dl>
<dl>
    <dt><Link class="anchor" id='dagster_gcp.import_df_to_bq'>dagster_gcp.import_df_to_bq <a href='https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-gcp/dagster_gcp/bigquery/ops.py#L99' className='source-link' target='_blank' rel='noopener noreferrer'>[source]</a><a href="#dagster_gcp.import_df_to_bq" class="hash-link"></a></Link></dt>
    <dd>

    </dd>

</dl>
<dl>
    <dt><Link class="anchor" id='dagster_gcp.import_file_to_bq'>dagster_gcp.import_file_to_bq <a href='https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-gcp/dagster_gcp/bigquery/ops.py#L109' className='source-link' target='_blank' rel='noopener noreferrer'>[source]</a><a href="#dagster_gcp.import_file_to_bq" class="hash-link"></a></Link></dt>
    <dd>

    </dd>

</dl>
<dl>
    <dt><Link class="anchor" id='dagster_gcp.import_gcs_paths_to_bq'>dagster_gcp.import_gcs_paths_to_bq <a href='https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-gcp/dagster_gcp/bigquery/ops.py#L89' className='source-link' target='_blank' rel='noopener noreferrer'>[source]</a><a href="#dagster_gcp.import_gcs_paths_to_bq" class="hash-link"></a></Link></dt>
    <dd>

    </dd>

</dl>
</div>

<div class="section" id="data-freshness">

### Data Freshness

<dl>
    <dt><Link class="anchor" id='dagster_gcp.fetch_last_updated_timestamps'>dagster_gcp.fetch_last_updated_timestamps <a href='https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-gcp/dagster_gcp/bigquery/resources.py#L108' className='source-link' target='_blank' rel='noopener noreferrer'>[source]</a><a href="#dagster_gcp.fetch_last_updated_timestamps" class="hash-link"></a></Link></dt>
    <dd>

    Get the last updated timestamps of a list BigQuery table.

    Note that this only works on BigQuery tables, and not views.

    Parameters:
      - <strong>client</strong> (<em>bigquery.Client</em>) – The BigQuery client.
      - <strong>dataset_id</strong> (<em>str</em>) – The BigQuery dataset ID.
      - <strong>table_ids</strong> (<em>Sequence</em><em>[</em><em>str</em><em>]</em>) – The table IDs to get the last updated timestamp for.


    Returns: A mapping of table IDs to their last updated timestamps (UTC).Return type: Mapping[str, datetime]

    </dd>

</dl>
</div>

<div class="section" id="other">

### Other

<dl>
    <dt><Link class="anchor" id='dagster_gcp.BigQueryError'>`class` dagster_gcp.BigQueryError <a href='https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-gcp/dagster_gcp/bigquery/types.py#L154' className='source-link' target='_blank' rel='noopener noreferrer'>[source]</a><a href="#dagster_gcp.BigQueryError" class="hash-link"></a></Link></dt>
    <dd>

    </dd>

</dl>
</div></div>

<div class="section" id="gcs">

## GCS

<div class="section" id="gcs-resource">

### GCS Resource

<dl>
    <dt><Link class="anchor" id='dagster_gcp.GCSResource'>dagster_gcp.GCSResource ResourceDefinition <a href='https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-gcp/dagster_gcp/gcs/resources.py#L11' className='source-link' target='_blank' rel='noopener noreferrer'>[source]</a><a href="#dagster_gcp.GCSResource" class="hash-link"></a></Link></dt>
    <dd>

        <div className='lineblock'> </div>

    Resource for interacting with Google Cloud Storage.



    Example:

        ```python
        @asset
        def my_asset(gcs: GCSResource):
            client = gcs.get_client()
            # client is a google.cloud.storage.Client
            ...
        ```

    </dd>

</dl>
</div>

<div class="section" id="gcs-i-o-manager">

### GCS I/O Manager

<dl>
    <dt><Link class="anchor" id='dagster_gcp.GCSPickleIOManager'>dagster_gcp.GCSPickleIOManager IOManagerDefinition <a href='https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-gcp/dagster_gcp/gcs/io_manager.py#L80' className='source-link' target='_blank' rel='noopener noreferrer'>[source]</a><a href="#dagster_gcp.GCSPickleIOManager" class="hash-link"></a></Link></dt>
    <dd>

        <div className='lineblock'> </div>

    Persistent IO manager using GCS for storage.

    Serializes objects via pickling. Suitable for objects storage for distributed executors, so long
    as each execution node has network connectivity and credentials for GCS and the backing bucket.

    Assigns each op output to a unique filepath containing run ID, step key, and output name.
    Assigns each asset to a single filesystem path, at `\<base_dir>/\<asset_key>`. If the asset key
    has multiple components, the final component is used as the name of the file, and the preceding
    components as parent directories under the base_dir.

    Subsequent materializations of an asset will overwrite previous materializations of that asset.
    With a base directory of `/my/base/path`, an asset with key
    `AssetKey(["one", "two", "three"])` would be stored in a file called `three` in a directory
    with path `/my/base/path/one/two/`.

    Example usage:

    1. Attach this IO manager to a set of assets.
        ```python
        from dagster import asset, Definitions
        from dagster_gcp.gcs import GCSPickleIOManager, GCSResource

        @asset
        def asset1():
            # create df ...
            return df

        @asset
        def asset2(asset1):
            return asset1[:5]

        Definitions(
            assets=[asset1, asset2],
            resources={
                "io_manager": GCSPickleIOManager(
                    gcs_bucket="my-cool-bucket",
                    gcs_prefix="my-cool-prefix",
                    gcs=GCSResource(project="my-cool-project")
                ),

            }
        )
        ```
    2. Attach this IO manager to your job to make it available to your ops.
        ```python
        from dagster import job
        from dagster_gcp.gcs import GCSPickleIOManager, GCSResource

        @job(
            resource_defs={
                "io_manager": GCSPickleIOManager(
                    gcs=GCSResource(project="my-cool-project")
                    gcs_bucket="my-cool-bucket",
                    gcs_prefix="my-cool-prefix"
                ),
            }
        )
        def my_job():
            ...
        ```

    </dd>

</dl>
</div>

<div class="section" id="gcs-sensor">

### GCS Sensor

<dl>
    <dt><Link class="anchor" id='dagster_gcp.gcs.sensor.get_gcs_keys'>dagster_gcp.gcs.sensor.get_gcs_keys <a href='https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-gcp/dagster_gcp/gcs/sensor.py#L7' className='source-link' target='_blank' rel='noopener noreferrer'>[source]</a><a href="#dagster_gcp.gcs.sensor.get_gcs_keys" class="hash-link"></a></Link></dt>
    <dd>

    Return a list of updated keys in a GCS bucket.

    Parameters:
      - <strong>bucket</strong> (<em>str</em>) – The name of the GCS bucket.
      - <strong>prefix</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) – The prefix to filter the keys by.
      - <strong>since_key</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) – The key to start from. If provided, only keys updated after this key will be returned.
      - <strong>gcs_session</strong> (<em>Optional</em><em>[</em><em>google.cloud.storage.client.Client</em><em>]</em>) – A GCS client session. If not provided, a new session will be created.


    Returns: A list of keys in the bucket, sorted by update time, that are newer than the <cite>since_key</cite>.Return type: List[str]


    Example:

        ```python
        @resource
        def google_cloud_storage_client(context):
            return storage.Client().from_service_account_json("my-service-account.json")

        @sensor(job=my_job, required_resource_keys={"google_cloud_storage_client"})
        def my_gcs_sensor(context):
            since_key = context.cursor or None
            new_gcs_keys = get_gcs_keys(
                "my-bucket",
                prefix="data",
                since_key=since_key,
                gcs_session=context.resources.google_cloud_storage_client
            )

            if not new_gcs_keys:
                return SkipReason("No new gcs files found for bucket 'my-bucket'.")

            for gcs_key in new_gcs_keys:
                yield RunRequest(run_key=gcs_key, run_config={
                    "ops": {
                        "gcs_files": {
                            "config": {
                                "gcs_key": gcs_key
                            }
                        }
                    }
                })

            last_key = new_gcs_keys[-1]
            context.update_cursor(last_key)
        ```

    </dd>

</dl>
</div>

<div class="section" id="file-manager">

### File Manager

<dl>
    <dt><Link class="anchor" id='dagster_gcp.GCSFileHandle'>`class` dagster_gcp.GCSFileHandle <a href='https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-gcp/dagster_gcp/gcs/file_manager.py#L16' className='source-link' target='_blank' rel='noopener noreferrer'>[source]</a><a href="#dagster_gcp.GCSFileHandle" class="hash-link"></a></Link></dt>
    <dd>
    A reference to a file on GCS.
    </dd>

</dl>
<dl>
    <dt><Link class="anchor" id='dagster_gcp.GCSFileManagerResource'>dagster_gcp.GCSFileManagerResource ResourceDefinition <a href='https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-gcp/dagster_gcp/gcs/resources.py#L46' className='source-link' target='_blank' rel='noopener noreferrer'>[source]</a><a href="#dagster_gcp.GCSFileManagerResource" class="hash-link"></a></Link></dt>
    <dd>

        <div className='lineblock'> </div>

    FileManager that provides abstract access to GCS.


    </dd>

</dl>
</div>

<div class="section" id="gcs-compute-log-manager">

### GCS Compute Log Manager

<dl>
    <dt><Link class="anchor" id='dagster_gcp.gcs.GCSComputeLogManager'>`class` dagster_gcp.gcs.GCSComputeLogManager <a href='https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-gcp/dagster_gcp/gcs/compute_log_manager.py#L30' className='source-link' target='_blank' rel='noopener noreferrer'>[source]</a><a href="#dagster_gcp.gcs.GCSComputeLogManager" class="hash-link"></a></Link></dt>
    <dd>

    Logs op compute function stdout and stderr to GCS.

    Users should not instantiate this class directly. Instead, use a YAML block in `dagster.yaml`
    such as the following:

        ```YAML
        compute_logs:
          module: dagster_gcp.gcs.compute_log_manager
          class: GCSComputeLogManager
          config:
            bucket: "mycorp-dagster-compute-logs"
            local_dir: "/tmp/cool"
            prefix: "dagster-test-"
            upload_interval: 30
        ```
    There are more configuration examples in the instance documentation guide: [https://docs.dagster.io/deployment/oss/oss-instance-configuration#compute-log-storage](https://docs.dagster.io/deployment/oss/oss-instance-configuration#compute-log-storage)

    Parameters:
      - <strong>bucket</strong> (<em>str</em>) – The name of the GCS bucket to which to log.
      - <strong>local_dir</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) – Path to the local directory in which to stage logs. Default: `dagster_shared.seven.get_system_temp_directory()`.
      - <strong>prefix</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) – Prefix for the log file keys.
      - <strong>json_credentials_envvar</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) – Environment variable that contains the JSON with a private key and other credentials information. If this is set, `GOOGLE_APPLICATION_CREDENTIALS` will be ignored. Can be used when the private key cannot be used as a file.
      - <strong>upload_interval</strong> – (Optional[int]): Interval in seconds to upload partial log files to GCS. By default, will only upload when the capture is complete.
      - <strong>show_url_only</strong> – (Optional[bool]): Only show the URL of the log file in the UI, instead of fetching and displaying the full content. Default False.
      - <strong>inst_data</strong> (<em>Optional</em><em>[</em>[*ConfigurableClassData*](../dagster/internals.mdx#dagster._serdes.ConfigurableClassData)<em>]</em>) – Serializable representation of the compute log manager when instantiated from config.



    </dd>

</dl>
</div></div>

<div class="section" id="dataproc">

## Dataproc

<div class="section" id="dataproc-resource">

### Dataproc Resource

<dl>
    <dt><Link class="anchor" id='dagster_gcp.DataprocResource'>dagster_gcp.DataprocResource ResourceDefinition <a href='https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-gcp/dagster_gcp/dataproc/resources.py#L158' className='source-link' target='_blank' rel='noopener noreferrer'>[source]</a><a href="#dagster_gcp.DataprocResource" class="hash-link"></a></Link></dt>
    <dd>

        <div className='lineblock'> </div>

        :::info[beta]
        This API is currently in beta, and may have breaking changes in minor version releases, with behavior changes in patch releases.


        :::

    Resource for connecting to a Dataproc cluster.



    Example:

        ```default
        @asset
        def my_asset(dataproc: DataprocResource):
            with dataproc.get_client() as client:
                # client is a dagster_gcp.DataprocClient
                ...
        ```

    </dd>

</dl>
</div>

<div class="section" id="dataproc-ops">

### Dataproc Ops

<dl>
    <dt><Link class="anchor" id='dagster_gcp.dataproc_op'>dagster_gcp.dataproc_op `=` \<dagster._core.definitions.op_definition.OpDefinition object> <a href='https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-gcp/dagster_gcp/dataproc/ops.py#L96' className='source-link' target='_blank' rel='noopener noreferrer'>[source]</a><a href="#dagster_gcp.dataproc_op" class="hash-link"></a></Link></dt>
    <dd>

        <div className='lineblock'> </div>

        :::info[beta]
        This API is currently in beta, and may have breaking changes in minor version releases, with behavior changes in patch releases.


        :::


    </dd>

</dl>
</div></div>

<div class="section" id="pipes">

## Pipes

<div class="section" id="clients">

### Clients

<dl>
    <dt><Link class="anchor" id='dagster_gcp.pipes.PipesDataprocJobClient'>`class` dagster_gcp.pipes.PipesDataprocJobClient <a href='https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-gcp/dagster_gcp/pipes/clients/dataproc_job.py#L69' className='source-link' target='_blank' rel='noopener noreferrer'>[source]</a><a href="#dagster_gcp.pipes.PipesDataprocJobClient" class="hash-link"></a></Link></dt>
    <dd>

        :::info[preview]
        This API is currently in preview, and may have breaking changes in patch version releases. This API is not considered ready for production use.


        :::

    A pipes client for running workloads on GCP Dataproc in Job mode.

    Parameters:
      - <strong>client</strong> (<em>Optional</em><em>[</em><em>google.cloud.dataproc_v1.JobControllerClient</em><em>]</em>) – The GCP Dataproc client to use.
      - <strong>context_injector</strong> (<em>Optional</em><em>[</em>[*PipesContextInjector*](../dagster/pipes.mdx#dagster.PipesContextInjector)<em>]</em>) – A context injector to use to inject context into the GCP Dataproc job. Defaults to `PipesEnvContextInjector`.
      - <strong>message_reader</strong> ([*PipesMessageReader*](../dagster/pipes.mdx#dagster.PipesMessageReader)) – A message reader to use to read messages from the GCP Dataproc job. For example, [`PipesGCSMessageReader`](#dagster_gcp.pipes.PipesGCSMessageReader).
      - <strong>forward_termination</strong> (<em>bool</em>) – Whether to cancel the GCP Dataproc job if the Dagster process receives a termination signal.
      - <strong>poll_interval</strong> (<em>float</em>) – The interval in seconds to poll the GCP Dataproc job for status updates. Defaults to 5 seconds.


    <dl>
        <dt><Link class="anchor" id='dagster_gcp.pipes.PipesDataprocJobClient.run'>run <a href='https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-gcp/dagster_gcp/pipes/clients/dataproc_job.py#L116' className='source-link' target='_blank' rel='noopener noreferrer'>[source]</a><a href="#dagster_gcp.pipes.PipesDataprocJobClient.run" class="hash-link"></a></Link></dt>
        <dd>

        Run a job on GCP Dataproc, enriched with the pipes protocol.

        Parameters:
          - <strong>context</strong> (<em>Union</em><em>[</em>[*OpExecutionContext*](../dagster/execution.mdx#dagster.OpExecutionContext)<em>, </em>[*AssetExecutionContext*](../dagster/execution.mdx#dagster.AssetExecutionContext)<em>]</em>) – The context of the currently executing Dagster op or asset.
          - <strong>submit_job_params</strong> (<em>SubmitJobParams</em>) – Parameters for the `JobControllerClient.submit_job` call. See [Google Cloud SDK Documentation](https://cloud.google.com/python/docs/reference/dataproc/latest/google.cloud.dataproc_v1.services.job_controller.JobControllerClient#google_cloud_dataproc_v1_services_job_controller_JobControllerClient_submit_job)
          - <strong>extras</strong> (<em>Optional</em><em>[</em><em>Dict</em><em>[</em><em>str</em><em>, </em><em>Any</em><em>]</em><em>]</em>) – Additional information to pass to the Pipes session in the external process.


        Returns: Wrapper containing results reported by the external
        process.Return type: PipesClientCompletedInvocation

        </dd>

    </dl>

    </dd>

</dl>
</div>

<div class="section" id="context-injectors">

### Context Injectors

<dl>
    <dt><Link class="anchor" id='dagster_gcp.pipes.PipesGCSContextInjector'>`class` dagster_gcp.pipes.PipesGCSContextInjector <a href='https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-gcp/dagster_gcp/pipes/context_injectors.py#L17' className='source-link' target='_blank' rel='noopener noreferrer'>[source]</a><a href="#dagster_gcp.pipes.PipesGCSContextInjector" class="hash-link"></a></Link></dt>
    <dd>

    A context injector that injects context by writing to a temporary GCS location.

    Parameters:
      - <strong>bucket</strong> (<em>str</em>) – The GCS bucket to write to.
      - <strong>client</strong> (<em>google.cloud.storage.Client</em>) – A Google Cloud SDK client to use to write to GCS.
      - <strong>key_prefix</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) – An optional prefix to use for the GCS key. Will be concatenated with a random string.



    </dd>

</dl>
</div>

<div class="section" id="message-readers">

### Message Readers

<dl>
    <dt><Link class="anchor" id='dagster_gcp.pipes.PipesGCSMessageReader'>`class` dagster_gcp.pipes.PipesGCSMessageReader <a href='https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-gcp/dagster_gcp/pipes/message_readers.py#L85' className='source-link' target='_blank' rel='noopener noreferrer'>[source]</a><a href="#dagster_gcp.pipes.PipesGCSMessageReader" class="hash-link"></a></Link></dt>
    <dd>

    Message reader that reads messages by periodically reading message chunks from a specified GCS
    bucket.

    If <cite>log_readers</cite> is passed, this reader will also start the passed readers
    when the first message is received from the external process.

    Parameters:
      - <strong>interval</strong> (<em>float</em>) – interval in seconds between attempts to download a chunk
      - <strong>bucket</strong> (<em>str</em>) – The GCS bucket to read from.
      - <strong>client</strong> (<em>Optional</em><em>[</em><em>cloud.google.storage.Client</em><em>]</em>) – The GCS client to use.
      - <strong>log_readers</strong> (<em>Optional</em><em>[</em><em>Sequence</em><em>[</em><em>PipesLogReader</em><em>]</em><em>]</em>) – A set of log readers for logs on GCS.
      - <strong>include_stdio_in_messages</strong> (<em>bool</em>) – Whether to send stdout/stderr to Dagster via Pipes messages. Defaults to False.



    </dd>

</dl>
</div></div>

<div class="section" id="legacy">

## Legacy

<dl>
    <dt><Link class="anchor" id='dagster_gcp.ConfigurablePickledObjectGCSIOManager'>dagster_gcp.ConfigurablePickledObjectGCSIOManager IOManagerDefinition <a href='https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-gcp/dagster_gcp/gcs/io_manager.py#L169' className='source-link' target='_blank' rel='noopener noreferrer'>[source]</a><a href="#dagster_gcp.ConfigurablePickledObjectGCSIOManager" class="hash-link"></a></Link></dt>
    <dd>

        <div className='lineblock'> </div>

        :::warning[deprecated]
        This API will be removed in version 2.0.
         Please use GCSPickleIOManager instead..

        :::

    Renamed to GCSPickleIOManager. See GCSPickleIOManager for documentation.


    </dd>

</dl>
<dl>
    <dt><Link class="anchor" id='dagster_gcp.bigquery_resource'>dagster_gcp.bigquery_resource ResourceDefinition <a href='https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-gcp/dagster_gcp/bigquery/resources.py#L97' className='source-link' target='_blank' rel='noopener noreferrer'>[source]</a><a href="#dagster_gcp.bigquery_resource" class="hash-link"></a></Link></dt>
    <dd>

        <div className='lineblock'> </div>


    </dd>

</dl>
<dl>
    <dt><Link class="anchor" id='dagster_gcp.build_bigquery_io_manager'>dagster_gcp.build_bigquery_io_manager IOManagerDefinition <a href='https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-gcp/dagster_gcp/bigquery/io_manager.py#L26' className='source-link' target='_blank' rel='noopener noreferrer'>[source]</a><a href="#dagster_gcp.build_bigquery_io_manager" class="hash-link"></a></Link></dt>
    <dd>

        <div className='lineblock'> </div>

    Builds an I/O manager definition that reads inputs from and writes outputs to BigQuery.

    Parameters:
      - <strong>type_handlers</strong> (<em>Sequence</em><em>[</em><em>DbTypeHandler</em><em>]</em>) – Each handler defines how to translate between slices of BigQuery tables and an in-memory type - e.g. a Pandas DataFrame. If only one DbTypeHandler is provided, it will be used as the default_load_type.
      - <strong>default_load_type</strong> (<em>Type</em>) – When an input has no type annotation, load it as this type.


    Returns: IOManagerDefinition


    Examples:

        ```python
        from dagster_gcp import build_bigquery_io_manager
        from dagster_bigquery_pandas import BigQueryPandasTypeHandler
        from dagster import Definitions

        @asset(
            key_prefix=["my_prefix"],
            metadata={"schema": "my_dataset"} # will be used as the dataset in BigQuery
        )
        def my_table() -> pd.DataFrame:  # the name of the asset will be the table name
            ...

        @asset(
            key_prefix=["my_dataset"]  # my_dataset will be used as the dataset in BigQuery
        )
        def my_second_table() -> pd.DataFrame:  # the name of the asset will be the table name
            ...

        bigquery_io_manager = build_bigquery_io_manager([BigQueryPandasTypeHandler()])

        Definitions(
            assets=[my_table, my_second_table],
            resources={
                "io_manager": bigquery_io_manager.configured({
                    "project" : {"env": "GCP_PROJECT"}
                })
            }
        )
        ```
    You can set a default dataset to store the assets using the `dataset` configuration value of the BigQuery I/O
    Manager. This dataset will be used if no other dataset is specified directly on an asset or op.

        ```python
        Definitions(
            assets=[my_table],
            resources={
                    "io_manager": bigquery_io_manager.configured({
                        "project" : {"env": "GCP_PROJECT"}
                        "dataset": "my_dataset"
                    })
                }
        )
        ```
    On individual assets, you an also specify the dataset where they should be stored using metadata or
    by adding a `key_prefix` to the asset key. If both `key_prefix` and metadata are defined, the metadata will
    take precedence.

        ```python
        @asset(
            key_prefix=["my_dataset"]  # will be used as the dataset in BigQuery
        )
        def my_table() -> pd.DataFrame:
            ...

        @asset(
            # note that the key needs to be "schema"
            metadata={"schema": "my_dataset"}  # will be used as the dataset in BigQuery
        )
        def my_other_table() -> pd.DataFrame:
            ...
        ```
    For ops, the dataset can be specified by including a “schema” entry in output metadata.

        ```python
        @op(
            out={"my_table": Out(metadata={"schema": "my_schema"})}
        )
        def make_my_table() -> pd.DataFrame:
            ...
        ```
    If none of these is provided, the dataset will default to “public”.

    To only use specific columns of a table as input to a downstream op or asset, add the metadata `columns` to the
    [`In`](../dagster/ops.mdx#dagster.In) or [`AssetIn`](../dagster/assets.mdx#dagster.AssetIn).

        ```python
        @asset(
            ins={"my_table": AssetIn("my_table", metadata={"columns": ["a"]})}
        )
        def my_table_a(my_table: pd.DataFrame) -> pd.DataFrame:
            # my_table will just contain the data from column "a"
            ...
        ```
    If you cannot upload a file to your Dagster deployment, or otherwise cannot
    [authenticate with GCP](https://cloud.google.com/docs/authentication/provide-credentials-adc)
    via a standard method, you can provide a service account key as the `gcp_credentials` configuration.
    Dagster willstore this key in a temporary file and set `GOOGLE_APPLICATION_CREDENTIALS` to point to the file.
    After the run completes, the file will be deleted, and `GOOGLE_APPLICATION_CREDENTIALS` will be
    unset. The key must be base64 encoded to avoid issues with newlines in the keys. You can retrieve
    the base64 encoded with this shell command: `cat $GOOGLE_APPLICATION_CREDENTIALS | base64`


    </dd>

</dl>
<dl>
    <dt><Link class="anchor" id='dagster_gcp.gcs_resource'>dagster_gcp.gcs_resource ResourceDefinition <a href='https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-gcp/dagster_gcp/gcs/resources.py#L37' className='source-link' target='_blank' rel='noopener noreferrer'>[source]</a><a href="#dagster_gcp.gcs_resource" class="hash-link"></a></Link></dt>
    <dd>

        <div className='lineblock'> </div>


    </dd>

</dl>
<dl>
    <dt><Link class="anchor" id='dagster_gcp.gcs_pickle_io_manager'>dagster_gcp.gcs_pickle_io_manager IOManagerDefinition <a href='https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-gcp/dagster_gcp/gcs/io_manager.py#L179' className='source-link' target='_blank' rel='noopener noreferrer'>[source]</a><a href="#dagster_gcp.gcs_pickle_io_manager" class="hash-link"></a></Link></dt>
    <dd>

        <div className='lineblock'> </div>

    Persistent IO manager using GCS for storage.

    Serializes objects via pickling. Suitable for objects storage for distributed executors, so long
    as each execution node has network connectivity and credentials for GCS and the backing bucket.

    Assigns each op output to a unique filepath containing run ID, step key, and output name.
    Assigns each asset to a single filesystem path, at `\<base_dir>/\<asset_key>`. If the asset key
    has multiple components, the final component is used as the name of the file, and the preceding
    components as parent directories under the base_dir.

    Subsequent materializations of an asset will overwrite previous materializations of that asset.
    With a base directory of `/my/base/path`, an asset with key
    `AssetKey(["one", "two", "three"])` would be stored in a file called `three` in a directory
    with path `/my/base/path/one/two/`.

    Example usage:

    1. Attach this IO manager to a set of assets.
        ```python
        from dagster import Definitions, asset
        from dagster_gcp.gcs import gcs_pickle_io_manager, gcs_resource

        @asset
        def asset1():
            # create df ...
            return df

        @asset
        def asset2(asset1):
            return asset1[:5]

        Definitions(
            assets=[asset1, asset2],
            resources={
                    "io_manager": gcs_pickle_io_manager.configured(
                        {"gcs_bucket": "my-cool-bucket", "gcs_prefix": "my-cool-prefix"}
                    ),
                    "gcs": gcs_resource.configured({"project": "my-cool-project"}),
                },
        )
        ```
    2. Attach this IO manager to your job to make it available to your ops.
        ```python
        from dagster import job
        from dagster_gcp.gcs import gcs_pickle_io_manager, gcs_resource

        @job(
            resource_defs={
                "io_manager": gcs_pickle_io_manager.configured(
                    {"gcs_bucket": "my-cool-bucket", "gcs_prefix": "my-cool-prefix"}
                ),
                "gcs": gcs_resource.configured({"project": "my-cool-project"}),
            },
        )
        def my_job():
            ...
        ```

    </dd>

</dl>
<dl>
    <dt><Link class="anchor" id='dagster_gcp.gcs_file_manager'>dagster_gcp.gcs_file_manager ResourceDefinition <a href='https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-gcp/dagster_gcp/gcs/resources.py#L70' className='source-link' target='_blank' rel='noopener noreferrer'>[source]</a><a href="#dagster_gcp.gcs_file_manager" class="hash-link"></a></Link></dt>
    <dd>

    FileManager that provides abstract access to GCS.

    Implements the [`FileManager`](../dagster/internals.mdx#dagster._core.storage.file_manager.FileManager) API.


    </dd>

</dl>
<dl>
    <dt><Link class="anchor" id='dagster_gcp.dataproc_resource'>dagster_gcp.dataproc_resource ResourceDefinition <a href='https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-gcp/dagster_gcp/dataproc/resources.py#L277' className='source-link' target='_blank' rel='noopener noreferrer'>[source]</a><a href="#dagster_gcp.dataproc_resource" class="hash-link"></a></Link></dt>
    <dd>

        <div className='lineblock'> </div>

        :::info[beta]
        This API is currently in beta, and may have breaking changes in minor version releases, with behavior changes in patch releases.


        :::


    </dd>

</dl>
</div></div>
